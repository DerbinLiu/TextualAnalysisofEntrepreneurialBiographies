1	The dynamics of software packages
This chapter lays out the empirical context for our investigation into the development and evolution of packaged software. We are not attempting a formal literature review, as the body of published work is very uneven in coverage, approach and quality, but instead seek to show how this work bears upon the goals of our current research to develop an integrated understanding. Chapter 1 therefore describes the history and dynamics of the software package industry, and focuses on the emergence of Commercial Off-The-Shelf (COTS) packaged software solutions for complex organisational IT applications - a class of artefact that Smith and Wield (1988) describe as ‘organisational technologies’. We explore the historical evolution of what are today known as Enterprise Resource Planning (ERP) systems and how they are influenced by a set of historical roots that can be traced back to stock and production control systems in the aerospace and automobile sector from the 1960s.
ERP systems have been widely adopted, extending from their base in manufacturing to many other industrial sectors, including most recently public service organisations. The idea of supplying such complex applications as packaged solutions to such a wide range of different organisations raises a number of research questions. In particular:
1	Design: given the enormous diversity between organisations and sectors, how is it that software package providers can produce solutions which seemingly meet the needs of vastly different users?
2	Procurement: how is it that user organisations judge, rank and ultimately decide between the plethora of supplier offerings currently on offer?
3	Implementation: what kind of complex choices are facing organisations while they are attempting to implement a COTS solution - that is, between adapting the package to suit the particularities of organisational practice or vice versa?
4	Post-implementation: what is the overall career of the software package both within the adopting organisation and as a vendor offering?
We briefly review these issues which set the scene for our empirical investigation of packaged software, and underpin the theoretical concerns outlined in the next chapter.
The emergence and growth of the packaged software sector
The importance and distinctive features of software
Software is the sets of coded commands that tell a computer what tasks to perform and is thus crucial to the operation and application of computers. Computer programmes bridge the generic mathematical functions built into the (highly standardised) hardware technologies with the enormous range of specific applications for which we use ICTs at work and in everyday life -perhaps as word processor, calculator or electronic communication device (Pelaez 1988; Quintas 1993). The development of integrated circuits and fibre-optic technologies has delivered exponential increases year on year in computing and telecommunications power per unit price. This drew attention to the problem of the continued high cost and limited availability of software as a key ‘reverse salient’ (Hughes 1983) in the information economy. The 1968 identification of this ‘software crisis’ (Pelaez 1988; Quintas 1994a, 1994b) stimulated efforts to improve the quality and reduce the cost of production of software. This was initially by improved management of the craft process by which software was then produced (Friedman 1989) and, more recently, by finding ways to re-use and recycle existing code.
Software is labour intensive and highly expensive to produce, but can be reproduced - copied - quickly at near zero cost. If additional consumers could be found for a piece of software this would potentially allow the development costs to be shared over a much larger user base. Various firms (including major users of computing as well as specialist software producers) recognised opportunities to recover the monies invested in developing particular software systems by recycling them as packaged solutions (CampbellKelly 1995, 2003). If software could be standardised and supplied to a mass market, it seemingly would offer huge cost savings for consumers and profits for producers. Various methods for re-using code were thus developed, including for example, object-oriented programming that facilitated the task of recycling blocks of code to carry out the same function within different programmes, as well as the development of complete standard programmes, known as Commercial Off-The-Shelf (COTS) or packaged software.
The growth of packaged software
Packaged software production exhibits very different dynamics to the traditional approach to computing services based around supplying customised software solutions (Mowery 1996). The market for packaged software grew very rapidly throughout the 1990s until by the end of the twentieth century it constituted some 70 per cent of software sales (Sawyer 2000; OECD 1998). Within this, different kinds of software have demonstrated remarkably different trajectories (Quintas 1994a, 1994b). Development has been very uneven.1 We find big differences in the rate and extent of the uptake of
packaged solutions between the main classes of software: programming languages, operating systems, utilities and applications. Packaged software was most quickly and readily developed to deal with some of the core mathematical functions of computers, such as operating systems, programming languages and utilities (e.g. database systems) at the bottom of the ‘OSI hierarchy’.2 Such generic capabilities could be applied across a very wide range of settings of use, and standardised solutions were not only cheaper, because of their larger market, but had other advantages in terms of the availability of skills in programming and using such standard functions.3
While packaged solutions had emerged by the 1980s for various discrete tasks within the firm (e.g. Computer-Aided Design and payroll systems), particularly for ‘generic industrial applications’ that were widespread and carried out in similar ways across different organisations, progress was slower in relation to industry-specific and organisation-specific functions (Brady et al. 1992; Fincham et al. 1994; Nelson et al. 1996). It seemed difficult for external suppliers to understand the specific contexts and purposes of organisational users. This kind of knowledge appeared to be ‘sticky’ (von Hippel 1994) in that it was difficult to extract from the user organisation and its members.
Moreover, the discrete applications that had emerged merely constituted ‘islands of automation’.4 Nevertheless, by this stage, the focus of corporate technology strategy was beginning to shift away from the automation of particular activities and specialist functions within the firm towards sharing of information across the firm, thereby improving its effectiveness and adaptability (Fleck 1988a; Lopes 1992; Fincham et al. 1994). However, integrated solutions, capturing the full range of a user-organisation’s activities, remained the territory of custom solutions. These were initially built inhouse by larger computer users, though their supply was increasingly outsourced. It seemed difficult to supply standardised software solutions that would match the requirements and operating methods of large organisations, with their diverse array of activities and their own particular structure and history (Brady et al. 1992). In consequence, firm- and industry-specific applications continued to be developed largely in-house.
In the 1990s, things were in a state of flux. In the financial services sector, for instance, some specialised system suppliers succeeded in selling on custom solutions as niche solutions to other organisations conducting similar operations (such as credit-card-processing operations) (Fincham et al. 1994). In addition, as this book will analyse, a number of firms with their roots in stock control and manufacturing solutions set out to develop packaged integrated solutions that became the basis of today’s widely adopted Enterprise Resource Planning systems. Foremost among these, there is the now legendary story of the German firm SAP (Systems, Applications, Products in Data Processing) launched in 1972, by programmers who had left IBM, around a vision of developing standardised solutions, and which has now become the second-largest software company in the world (Meissner 2000).
Integrated enterprise systems
Emergence of enterprise resource planning systems
The call for commercial organisations to be more flexible and customer-centred in their operations found a technological correlate in the espousal of ideas of integrated organisation-wide computer systems. Initially this goal was conceived in terms of improving task efficiency (reducing error and efficiency by eliminating re-keying of data between functions). However, this was subsumed behind recognition of the scope for improved overall organisational effectiveness by the more timely and strategic use of information to increase responsiveness to the market and its customer orientation. For example, it was envisaged that by building integrated customer-centred databases, the banks and other retail financial organisations would be able to utilise information about their various kinds of relationship with existing clients (who may hold personal cheque accounts, business accounts, mortgages, etc.) - hitherto held on separate databases - as a key marketing resource for selling them other kinds of financial services (Fincham et al. 1994).
In the manufacturing industry, integrated systems have a long history that can be traced back to 1960s stock and inventory control systems, developed primarily in the car and aerospace industries to manage the complex logistics of building large complex assemblages (Webster and Williams 1993). In the 1970s, these were extended into Materials Requirements Planning (MRP), which tracked the logistics of all the components needed to make an assemblage such as a car or jet engine. MRP systems were extended to include production scheduling (e.g. through a Master Production Schedule, expanded through the Bill of Materials required to make a product), then capacity scheduling, calculating the time needed to produce orders on available equipment. The further development, in particular of ‘closed loop’ systems, where plans were modified according to progress in manufacturing on the shop floor, led to their redesignation in the 1980s as Manufacturing Resource Planning (MRP II) systems.
The emergence of new concepts of industrial best practice, inspired by the perceived success of Japanese manufacturing and related ideas of the lean and flexible firm, Just-In-Time, etc., saw these systems being reconfigured, re-presented and further developed, not for the careful management of stock but for organisational flexibility (Webster and Williams 1993; Clausen and Williams 1997). They were offered as company-wide systems, which were suites of software built from modules that would capture specific activities and that would speak to each other, and which were also presented as a stepping stone to Computer-Integrated Manufacture (CIM).
Additional functions were added on to the core materials planning functions, including sales and order management, purchasing, financial management and linkages to computer-aided design (Webster and Williams 1993). In the UK these systems gained the ephemeral title of Computer-Aided
Production Management (CAPM) in the context of public funding and promotion for technology development (ibid.). However, this terminology refused to stick, and was overtaken by the concept, articulated by the industry analyst Gartner, of Enterprise Resource Planning (Wylie 1990). Gartner characterised ERP systems in terms of their multifunctional and modular design, and above all that they were integrated, so that when data was updated on one module this would result in all systems being updated. They contrasted these with existing solutions where firms had linked together multiple often-incompatible systems. The claim was advanced that ERP would allow a company to integrate the data used throughout its entire organisation (Wylie 1990; Lopes 1992; Kumar and van Hillegersberg 2000; Mabert et al. 2001). An ERP system like SAP’s R/3 package, for instance, potentially supports an enormous range of functions (as described in Table 1.1).
Today ERP systems have extended their coverage substantially. We can compare Davenport’s list with that on SAP’s website in 2007 (see Table 1.2). While some of the functional areas are still included, the current characterisation of SAP’s ERP offerings shows not only a significant increase in the range of functions covered, but also an evident shift in presentation away from specific functions towards generic areas of management strategic concern.
Table 1.1 Davenport’s list of functions contained in SAP’s R/3 package
Accounts receivable and payable Asset accounting Cash management and forecasting Cost-element and cost-centre accounting Executive information system Financial consolidation	General ledger Product-cost accounting Profitability analysis Profit-centre accounting Standard and period-related costing
Human-resources time accounting Payroll Personnel planning Travel expenses	
Inventory management Material requirements planning Materials management Plant maintenance Production planning Vendor evaluation	Project management Purchasing Quality management Routing management Shipping
Order management Pricing Sales management Sales planning	
Source: Adapted from Davenport 1998.
Table 1.2 Current list of functions contained in SAP’s R/3 package
Analytics	Strategic enterprise management Financial analytics	Operations analytics Workforce analytics
Financials	Financial supply Chain management Financial accounting	Management accounting Corporate governance
Human capital management	Talent management Workforce process management	Workforce deployment
Procurement and	Procurement	Inbound and outbound
logistics execution	Inventory and warehouse management	logistics Transportation management
Product development	Production planning	Product development
and manufacturing	Manufacturing execution	Lifecycle data management
Sales and service	Sales order management Aftermarket sales and service	Professional-service delivery
Corporate services	Real estate management Enterprise asset management Project and portfolio management Travel management Environment	Health and safety Compliance management Quality management Global trade services
Source: Compiled from SAP website, sampled 23 May 2007.
In the early 1990s, these systems came in turn to be presented as a new approach revolving around ‘automating the enterprise’ (Lopes 1992). As Lopes notes, the term Enterprise Resource Planning was ‘designed’ by the technology industry analyst organisation, the Garter Group, which proclaimed ERP as the ‘new information systems paradigm’ (Klaus et al. 2000). An April 1990 Gartner publication, Computer Integrated Manufacturing, described their view of ERP ‘which we consider the software architecture for the next generation of MRP II’; a checklist of technical features anticipated ‘the continuing evolution of computing systems’ including graphical user interfaces, relational databases and client/server models (cited in Wang and Ramiller 2004). As described by Mabert et al. (2001: 69-70):
The Gartner Group coined the term ‘enterprise resource planning’ in the early 1990s to describe the business software systems that evolved as an extension of MRP II-type systems. They stipulated that such software should include integrated modules for accounting, finance, sales and distribution, HRM, material management, and other business functions based on a common architecture that linked the enterprise to both
customers and suppliers. This description implies three key properties. First, ERP systems are multifunctional in scope, tracking a range of activities such as financial results, procurement, sales, manufacturing and human resources. Second, they are integrated in nature, meaning that when data are entered into one of the functions, information in all related functions is also changed immediately. Third, they are modular in structure and usable in any combination of modules.
However, this description also makes apparent many elements of continuity with the earlier CAPM and MRP/MRP II systems. This includes for example the modular architecture (and one that broadly coincides with the conventional structure of large enterprises [Kallinikos 2004a]) and the transfer of data and integration of operations between modules. The vision of ERP incorporated the philosophy that had underpinned CIM ‘the enterprise connected and the systems integrated and interoperable’ (Xue et al. 2005: 280). Furthermore, it was not just the vision that was transferred across from CAPM and its predecessors but also in some respects the technology; the ERP offerings of the top five ERP vendors still use the kernel of MRP II functionality for the manufacturing planning portion of their systems (Chung and Snyder 2000). MRP II thus evolved into ERP systems today by a process described as bottom-up evolution (Kumar and van Hillegersberg 2000). In other words, both their past and their future were shaped by this history (Jakovljevic 2000).5
Alongside this picture of continuity in development and incremental growth in package functionality, there were also episodes of more radical change. We can identify two types of discontinuity:
1	Technical discontinuities occasioned by shifts in system architectures. We can illustrate this by examining the history of the offerings of industry leader SAP. SAP’s first product, a financial accounting system named R/1, was replaced at the end of the 1970s by R/2, their widely adopted MRP system, which evolved into what subsequently became known as ERP. R/ 1 and R/2 were both mainframe-based. In 1992 SAP responded to the widespread shift towards client-server architectures by launching a clientserver version of their software, called R/3, built around relational database technologies and an adaptable three-layer architecture, which segmented the presentation layer (controlling the interfaces with the user), the application layer (which held the business-specific logic) and the database layer (which stores all information about the system, including transactional and configuration data) (Meissner 2000; SAP no date).
2	Changing business prescriptions which motivate and pattern innovative activity by vendors, and which are used to convince customers to acquire their packages. ERP was motivated by a vision of enterprise integration which, as we discuss below, has lately been criticised for its failure to support linkages to external bodies (e.g. through customer relationship management and supply-chain integration).
There is a complex interplay between these technical and business ‘drivers’ of change, which may be more or less tightly coupled together. Sometimes ‘old’ technologies may simply be represented as meeting new social and economic goals (as when CAPM was reoriented towards Just-In-Time [Webster and Williams 1993]). Significant technical advances and new concepts of the organisational outcomes tend to become more tightly woven together. This is perhaps necessarily so, as vendors target their development activity around their vision of the product’s future - and in so doing provide a compelling business reason for existing customers to upgrade. Thus SAP has launched MySAP, an internet-based version incorporating tools to facilitate user customisation as well as CRM extensions (Stutz and Klaber 2006; Blankenship 2003). The marketing strategies of suppliers are thus important factors in the ways these changes are presented, bundling together technical capabilities and expectations of social and economic outcomes.
As well as exploring the activities of individual suppliers, we need to pay attention to the ways in which overall conceptions of product markets for packaged software are being shaped by the aggregate activities of suppliers and their customers and also by influential external commentators: consultants, policymakers, academics and industry analysts. The latter have been particularly influential as mobilisers of community opinion in the case of ERP. Thus Gartner, which as already noted had coined the term ERP in the early 1990s, declared in 2000 that ERP is ‘dead’ and mapped out a transition to the next phase, which they described as extended ERP or ERP II (Bond et al. 2000). And as we shall see later, although Gartner’s recent declaration signalled a reorientation of ERP, the concept remained (perhaps having already achieved sufficient momentum to sustain itself). Not all visions manage to sustain themselves as technological fields, however, as the case of CAPM illustrates.
CAPM was posited as a new technological field and the next big step from MRP and MRP II (Cork 1985). In the UK particularly, the field of CAPM achieved considerable support when it was identified as a key technology by the UK Science and Engineering Research Council’s ‘Application of Computers in Manufacturing and Engineering’ (ACME) initiative. A review of the state of the art in CAPM identified a number of challenges (Waterlow and Monniot 1987). Public research, geared to overcome difficulties experienced with CAPM, encouraged suppliers from various markets (especially MRP II) to develop their offerings under the name CAPM. CAPM technology was promoted by the efforts of suppliers and further reinforced by promotion within professional associations (Newell and Clark 1990; Clark et al. 1992). However, suppliers were not particularly committed to this specific terminology and their attention migrated away (and was to be quickly absorbed by the ERP agenda) while CAPM remained as a term used primarily by the academic, and especially the British social science, community.
Like CAPM, ERP remains a somewhat amorphous concept. Reviewing the ERP literature, Klaus et al. (2000) note the diversity of views as to what
constitutes ERP, partly rooted in the fact that views of what constitutes ERP differ between economic, business and technical specialist views:
First, and most obviously, ERP is a commodity, a product in the form of computer software. Second, and fundamentally, ERP can be seen as a development objective of mapping all processes and data of an enterprise into a comprehensive integrative structure. Third, ERP can be seen as the key element of an infrastructure that delivers a solution to business
(ibid.: 143)
Clausen and Koch (1999) similarly characterise ERP as revolving around a triple vision of the enterprise: an economic view (the enterprise as a financial entity); the logistical view (the enterprise as a system of material flows); and the Informational view (the enterprise as information flows). In short, it can be seen that ERP has a hybrid character that is viewed differently from different perspectives. Thus, a commonly agreed definition is unlikely.
ERP is a concept that seemed to emerge from industry rather than academia. We can contrast its rise to the broadly contemporary articulation of the extremely influential change management strategy of Business Process Redesign (BPR). This was a concept that emerged and became disseminated through academic as well as practitioner fora, and that was promulgated by leading figures at Harvard Business School (Hammer and Champy 1993). There are also interesting parallels between BPR and ERP in terms of their rapid and wide uptake, with reinforcement between provider, customers and commentators that each particular strategy represented ‘the way forwards’ (Westrup and Knight 2000).
Gartner’s coining of the ERP concept coincided with the release by the largest global supplier, SAP, of its R/3 suite. Indeed Holsapple and Sena (1999: 2) cite the conclusions of a panel of experts that the definition of ERP tends to be defined by vendor offerings themselves: ‘Thus, as SAP expands the scope of its software, the notion of ERP software expands as well.’
ERP timeline
Klaus et al. (2000), charting the introduction to the academic Information Systems community of the ERP concept, trace it back to a 1996 conference paper by the highly influential writer on information technology and business process redesign Thomas Davenport (1996). This was published one year after SAP R/3 was released in the USA, and it discusses the challenges of ‘megapackages’, which is the prelude to widespread discussion of ERP in academic and trade circles in the period 1997-2000.6 The ERP timeline is summarised in Figure 1.1.
Esteves and Pastor (2001), reviewing the information systems literature and journals, chart the uptake of discussion of ERP over this period (see Table 1.3). Conference papers kick off in 1997 and peak by 1999, which is
Figure 1.1 Adoption of the ERP concept in IS academe. Source: Klaus et al. 2000: 153.
when nearly all IS conferences had a panel dedicated to ERP. By contrast, journal articles start from 1998 onwards, which reflects the greater lag in their publication processes.
Wang (2007b) identified 2,270 articles on ERP appearing in the ‘trade press’ between October 1991 and December 2002 and recorded in the ABI/ Inform Global database. Analysis of these records the dramatic growth in ERP articles from below twenty in 1996 to 150 in 1999, followed by a gradual tail off year on year until 2003.
The growth of generic ERP supply
A number of factors seem to have driven and shaped the explosive growth in adoption of ERP. Klaus et al. (2000) consulted an expert panel that flagged the coincidence of three sets of factors:
1	Technical: the availability of faster, cheaper computing capacity and in particular the advent of client-server architectures, at the same time that user organisations realised the difficulties of pursuing integration of information systems as an in-house project, particularly in the context of the skills shortage for IT professionals;
2	Managerial: a means to fulfil the promise of ‘best practice’ management concepts; Business Process Redesign and ideas of the lean and flexible firm;
Table 1.3 Information systems publications and presentations on ERP 1997-2000
	1997	1998	1999	2000
IS journals	0	2	3	16
IS conferences	4	15	82	57
Source: Adapted from Esteves and Pastor 2001.
3	Market: the success of vendor marketing, coupled with a particular drive to replace legacy computer systems in the late 1990s which might suffer from anticipated Y2K problems with new systems that could be expected to be Y2K compliant.
Grabot and Botta-Genoulaz (2005) identified a similar range of factors underpinning what they describe as the ‘ERP movement’, stressing in addition to the above the desire to share information across an entire company and exercise more financial control by means of an integrated system rather than a collection of non-homogeneous packages.
Light and Papazafeiropoulou (2004) surveyed user organisations’ motives for ERP adoption; this included for example the direct benefits of organisational changes, the desire to attain ‘best practices’ and to implement (organisational) change. Though cost was a factor, it was notable that many of the benefits identified relate to the quality and availability of technical expertise. These included attempts to ‘free up’ the information systems function, the availability of a broader knowledge and skills base, the perception that the system was a ‘tried and tested’ product, that it could deal with an applications backlog and ‘overcome’ IS legacy problems. Other motives included the desire for standardisation across industry and national boundaries and presentational issues, the role of selling (by vendors) and, finally, improving the public standing of the adopting organisation. Here ERP was undoubtedly a beneficiary of the parallel programme of business improvement through Business Process Redesign. ERP offerings with their libraries of ‘best practice’ industry business processes could be presented as a vehicle for BPR. In this sense, the two programmes can be seen to be mutually reinforcing, even though their proximity led to a certain amount of ‘boundary work’ and conflicts about the respective contributions of software and organisational change management (Champy 1997; Earl 1997; Davenport 1998; Al-Mashari 2001).
ERP in the West had its roots in manufacturing systems integration (typically in big firms with a high functional division of labour). The saturation of these core markets led vendors to target new sectors. Thus, today, ERP systems and the concept of integrated enterprise systems have been applied to other productive sectors (e.g. the process industries) and then to the services sector (e.g. financial services) and, more recently, as the vendors sought to further expand their markets, to the public sector. In order to do this, SAP, in common with the other major vendors, produces specialised industry-specific modules geared towards a particular market segment. SAP currently offer 28 such modules - see Table 1.4.
Finally, ERP has spread internationally - through the developed countries and into developing countries (Mabert et al. 2001; Kumar and van Hillegersberg 2000). While SAP R/2 had been widely adopted by European multinationals, which valued its ability to handle multiple currencies and languages, the shift to R/3 occasioned SAP’s move to leadership in the
Table 1.4 SAP industry-specific business maps
Financial services	Banking Insurance	
Manufacturing industries	Aerospace and defence Automotive Chemicals Consumer products Engineering, construction and operations High tech	Industrial machinery and components Life sciences Mill products Mining Oil and gas
Public services	Defence and security Healthcare	Higher education and research Public sector
Service industries Trading industries	Hospitality Logistics service providers Media Postal services Professional services Retail Wholesale distribution	Railways Telecommunications Utilities Waste and recycling
Source: Compiled from SAP website, sampled 23 May 2007.
global market. It has been estimated that, by 2001, ERP systems had been implemented in 30,000 firms worldwide, including many global corporations (Mabert et al. 2001). However, by 2005, ERP market leader SAP alone had more than 100,600 installations worldwide, 38,000 customers and 12 million users (SAP no date). On top of this, it had sold its generic package to all the FTSE100 companies.
The ERP supply sector is highly concentrated. In 2000, the top five vendors (at that time SAP, the Oracle Corporation, PeopleSoft, J.D. Edwards and Baan) accounted for close to 60 per cent of total ERP revenue (Jakovljevic 2001). The market in 2001 was estimated as being worth over $20 billion, up from $11 billion in 1997 (Jakovljevic 2001). Gartner Dataquest’s preliminary estimate for the size of the worldwide ERP market is approximately $17.8 billion in total software revenue in 2006. Through 2011, they expect the market to continue growing at a compound annual growth rate of 7 per cent to a market size of approximately $25 billion. The fall-off in growth of the ERP market in the late 1990s, down from the astonishing growth rates that had been achieved in the mid-1990s, promoted announcements, which proved premature, of the ‘death’ of ERP (Kumar and van Hillegersberg 2000; Jakovljevic 2001). Jakovljevic (2001) provides the figures for the ERP market (see Table 1.5).
This market downturn was associated with the continued consolidation of the market, with many smaller local software houses that specialise in bespoke solutions going out of business (Clausen and Koch 1999; Koch 2005), leaving only a few large players to dominate. Takeovers and mergers
Table 1.5 ERP market
	1997	1998	1999	2000	2001 (est.)
Total revenue	11.0	16.6	18.6	19.9	20.5-22.5
($ billion) Revenue	43%	40%	12%	7%	3%-13%
growth
Source: Jakovljevic (2001).
also affected the largest players: J.D. Edwards was taken over by PeopleSoft, which in turn was taken over by Oracle (CIO 2004). In addition, the Dutch firm Baan, after running into financial difficulties in 2000, eventually became a subsidiary of the American SSA Global Technologies.
The extension of ERP
Today the future of ERP is under discussion. We find the large vendors seeking to promote their solutions in medium-sized enterprises where penetration has been hitherto limited (Mabert et al. 2001; Kumar and van Hillegersberg 2000). At the same time the extension of ERP has come under discussion in two important respects.
First, the emphasis of current system development efforts has shifted from integration of activities within the enterprise towards rationalising the interface between the organisation and its customers. This is evidenced, for instance, by many suppliers including Customer Relationship Management (CRM) systems within their offerings. Second, and at the same time, prescriptions of the sources of improved competitiveness have shifted away from the individual organisation towards the performance of a range of organisations trading and linked in the value chain. However, current ERP systems, which emphasise integration of all different types of information within the organisation, are not well suited for supporting such supply-chain integration, in which the emphasis is on standardising information exchanges across the supply chain. Very different technical architectures are also being proposed for information systems based upon web-service models. The industry analyst Gartner predicted that ERP suppliers would need to reorient their offerings around new Extended Enterprise Resource Planning systems but pointed out that many existing vendors would find it difficult to achieve this new vision (Judd 2006).
In short, it seems that a new configuration of technical, managerial and market elements is being worked out. This process seems already to exhibit similarities with previous waves of technical change. There is, for instance, the initial attempt by vendors to represent their offerings as a solution to emerging business practices. In this respect, we note how experiences with ERP II echo the way in which MRP II and CAPM were reoriented as a vehicle for ‘Japanisation’ and flexible production a decade earlier. We will
come back to this short history of ERP and its predecessors in Chapter 3. There we will use it to illustrate how our biographies framework can analyse the emergence and evolution of technological fields.
Research questions arising with the adoption of ERP
The development of generic COTS solutions for complex, integrated organisational information systems and their supply to a wide range of organisational users raises three key sets of research questions:
1	Design of generic systems: how can enterprise systems vendors produce generic software solutions, given the diversity of their intended market of organisational users? In particular, how do COTS developers build the representations of organisations and their practices needed to develop systems that can capture the enormous breadth of activities within particular organisations? How, furthermore, can they develop generic representations to underpin commodified packages that can be implemented across diverse ranges of organisations and across many different industrial sectors and nations?
2	Procurement: organisations face a plethora of supplier offerings. Software is a ‘non-material artefact’ and therefore its properties are hard to ascertain. They may only ultimately become known when a package is installed and used. This then prompts the question of how user organisations make effective choices in a context of uncertainty about the effectiveness of vendor offerings and their appropriateness for their own organisation.
3	Implementation: what are the choices facing organisations that have decided to implement a COTS solution? In particular, how do user organisations handle the trade-off between customising the package to suit the particularities of organisational practice and, at the other extreme, adopting the system wholesale and adjusting the organisation to meet its exigencies?
4	Post-implementation: what is the subsequent career of the software solution within the user organisation (its ‘project’ lifecycle) and as a vendor offering (its ‘product’ lifecycle)? And what is the tension between these two views on the generic system?
The design and development of ‘universal’ solutions
In developing industrial IT applications there have been ongoing problems in capturing user requirements and embedding them in software solutions (Friedman 1989). One of the reasons for this is that knowledge about organisational ‘users’ and the particular practices and purposes of organisation members includes what von Hippel (1994) has described as ‘sticky data’. This is, knowledge that is hard to capture, extract, formalise and apply from a distance. When integrated applications (like ERP) are being created,
development must take into account not only the diversity of organisational functions but also the variety of cultures and practices across the organisation. Reconciling these disparate views may not be straightforward, not least because there may be differing views within user organisations about what constitutes best practice (Wagner and Newell 2004).
In the development of packaged systems, the problems of user requirements capture is very different to that encountered with the conventional model of supplying software as a bespoke product (Regnell et al. 2001; Sawyer 2001). However, despite the predominance of packaged solutions, there is remarkably little research literature in this area (a fact we return to in the next chapter). With packaged solutions, different sets of considerations come into play, deriving from the need for a product-centred rather than a service-centred view.
Packaged application suppliers must simultaneously cater for new customers, where they have to emphasise the suitability of their offerings as well as ease of implementation, and for existing customers, where they may need to add new functionality and ensure ease of upgrade to retain their custom and continued income from upgrades (Kremer and van Dissel 2000). Thus, it has been observed that suppliers may need to include at least one ‘wow’ feature in every new release that might motivate existing customers to buy the new version (Deifel 1999). The development process for packages becomes exceedingly complex in the face of challenges regarding how to prioritise and plan the release of enhancements, and how to deliver these in a competitive environment which demands high reliability, at reasonable cost and under time pressure (Sprott 2000; Regnell et al. 2001). Many suppliers have developed elaborate policies for the upgrading of their products, geared to keep existing customers on board (and the income streams they represent) and attract new customers and at the same time reducing the number of different software versions they need to support (Kremer and van Dissel 2000).
In the service model of software supply, a solution is custom-built around a particular organisational user and its expressed requirements; and here close direct links between the supplier and user are seen as crucial (Regnell et al. 2001; Sawyer 2001). With packaged solutions, by contrast, there is no longer a single primary customer which is paying for the project. Instead, it is the supplier which bears the risk and must decide which features to include and how. Among the existing research on software package design, some accounts emphasise the autonomy of suppliers to ‘invent’, or at least decide about, requirements (Regnell et al. 2001: 51). However, although users may no longer be of necessity integral to development, they are still central to packaged development. This draws attention to the question of how users’ needs are reflected in packaged software products and what mechanisms are developed by packaged software developers to gather and represent user needs (Sawyer 2000).
In practice, however, and one of the things that we suggest needs to be more fully researched and conceptualised, packages often emerge from
customised solutions and evolve through further implementations in different organisational settings. In this respect, packages and enhancements to packages emerge through particular implementations in user organisations. Suppliers are likely to build organisational models around the firms to which they have access and with which they have established relationships and access (Soh and Sia 2004; and see Chapter 4). Packaged organisational solutions thus tend to reflect the features of their previous market of installed applications - as we saw in the case of ERP and its predecessors (Webster and Williams 1993).
Even though user organisations continue to be important in package development, the relationship between the developer and user organisation has changed significantly (see particularly Chapter 8). Empirical research reveals that vendors use a wide variety of sources to identify requirements, including: user groups; indirect links with their existing customers through analysis of calls to help-desks and support lines; through various kinds of intermediary such as consultants and systems integrators; and through third-party information providers (such as market research and product reviews) (Keil and Carmel 1995; Sawyer 2000, 2001). Most of these avenues are also used by custom software providers. However, with package development, the balance is very different. In particular, direct customer contact is reported as one of the least likely means. Keil and Carmel (1995) point to the increasing role of surrogates and intermediaries, which may include internal and external consultants, as well as selected customers, and suggest that this may be problematic since intermediaries can intentionally or unintentionally filter and distort messages and may not have a complete understanding of customer needs. Keil and Carmel attribute this limited use of direct customer links to the separation by vendors of support for existing products from new product development. Sawyer (2001: 99) notes that selection of features for inclusion in COTS ‘is often driven by expectations of the technical trajectories of potential future vendor products as much as by specific internal needs’.
As ERP vendors have extended their market base, the problem of representing user requirements has become more challenging and more pressing. Initially we see the vendors seeking to increase their share of their original markets, and then, as they have saturated their historical markets, they have sought to move into other segments and nations. The growth in the user base, which necessarily encompasses a greater variety of user organisational practices, coupled with the extension of functionality offered (what Carmel and Becker [1995] describe as ‘function creep’) poses acute problems for vendors. As well as questions of how to manage their customer interface with an ever-larger user base and associated knowledge management issues, there was a pressing design question - of how to prioritise between competing requirements. We return to these issues in more detail in Chapters 4 and 5.
The move that SAP and other ERP vendors made, away from their base in large firms in the manufacturing industry into wholly new markets (for
example, medium-sized firms) and industries (for example, public and private services), forced vendors to acquire new knowledge and competencies (Kumar and van Hillegersberg 2000). This move into diverse industrial segments presented a geometrical leap in terms of the ‘complexity’ the vendors needed to address.
In particular, it posed an important challenge for them of how to combine information on requirements from different market segments and make a trade-off between their priorities (Regnell et al. 2001). It is clear that package suppliers have given priority to certain markets over others. Automobile manufacture, for instance, has from the outset been a key target market for the ERP vendors and thus the major ERP suppliers all offer automotive specific functionality (Gould 2002). The corollary is that, by implication, other sectors with smaller markets command less attention. ERP vendors therefore need to decide how much effort to invest in providing solutions for particular segments, since the requirements of some players are liable to be more thoroughly recognised.
In this context, simple models of customer/user engagement are not applicable. Instead, a complex division of labour has emerged within the software vendors between those developing new products and markets and those supporting existing product lines (Keil and Carmel 1995). The strategies developed by some vendors to deal with this challenge have been extremely sophisticated, as they seek to maintain a stable and dependable architecture and keep some kind of strategic direction for the development of their products, underpinned by an economic and technical view of where the technology is heading in the face of a huge array of often contradictory user requirements (Deifel 1999). As we shall explore later, vendors seek to achieve this by selective and managed responses to bottom-up requirements arising from current and potential users.
Another way in which packaged solution suppliers have sought to protect themselves from becoming overwhelmed by the diversity of their user base is by seeking to minimise their involvement in packaged implementation (Sawyer 2001). Instead, much implementation effort is outsourced to third parties - typically to consultants who are used in the vast majority of implementations. Most often, these are independent consultants - though sometimes provided by hardware or software suppliers (Kumar et al. 2003).
Another more recent way in which major suppliers, in particular SAP, sought to offset the burden of needing to cater for each and every organisation-specific requirement has been by setting up their offerings as a platform - enabling independent houses to supply solutions specifically adapted to their purposes that will interface with the ERP platform (Sprott 2000; Jakovljevic 2001). This has provided an opening for providers of interoperable niche applications and partnerships between the major ERP vendors and local specialist suppliers (Sprott 2000; Koch 2005). It has also resulted in the creation of a substantial market for what have been described as enterprise application integration (EAI) services. Some have argued that the
failure of other software package suppliers to get into this market for integration services has been risky and threatens to displace them in future from a lucrative and growing market (Sprott 2000) (though this risk has been reduced by the subsequent inclusion of development tools within ERP packages to facilitate system implementation and customisation by the user).
The cost of consultancy and related services typically exceeds the software cost, perhaps by as much as threefold or more (Westrup and Knight 2000; Mabert et al. 2001). The willingness of the big ERP vendors to forego the larger sums associated with business consultancy and package implementation indicates their commitment to their business model of retaining the part of the information systems market that had the highest mark-ups and thus scope for profits. Rebstock and Selig (2000: 932) note that the globalisation of demand has forced ERP vendors like SAP to develop ‘local versions and sales presences, local support and, often also, development organizations outside their home market and language’. It was in this way that they acquired the capability to ‘construct multi-language, multi-currency, multicountry and multi-system IT landscapes’ (ibid.: 932). The ERP suppliers have thus surrounded themselves with a swathe of complementary firms. Koch (2005) points to the existence of some 100 value-added resellers (implementers and integrators) even in a small economy like Denmark (a number that could be as high as 500 if training firms were included). This array of local actors provides local knowledge and services that could not readily be supplied by the big global vendors. Moreover, even if they had wanted to provide these services, the enormous growth of the ERP market in the 1990s outstripped the available expertise within the vendor organisations (Westrup and Knight 2000; and see Chapter 8, where one global supplier has attempted to build a novel organisational form to resolve this problem). It was more advantageous for the vendors to build local partnerships (Koch 2005). By maintaining a network of local partners, they could exercise some control. SAP in particular is noted for its efforts to train and certify SAP consultants. Its ‘TeamSAP’ Partner Programme links consultants, imple-menters and suppliers of complementary products (Westrup and Knight 2000). This wider array of players thus underpins the plausibility of the supply of standardised generic COTS solutions.
Selection and procurement
The problem of ‘fit’: misalignments between package and user organisation
The key challenge in implementing standardised packaged enterprise solutions has been seen in terms of a misalignment between the various ‘standard’ organisational presumptions embedded in the generic software and the particular ‘non-standard’ structure and practices of the adopting organisation (Soh et al. 2000; Soh and Sia 2004; Wei et al. 2005). Enterprise packages are built around ‘reference models’ for various industries and sectors
which, in turn, are constructed around a traditional view of organisations and, for example, presume a hierarchical, functional division of labour. As well as incorporating views about the specialised functions needing to be carried out, the package involves libraries of business processes for particular industrial segments. These are often presented as embodying ‘best industrial practice’ based, it is claimed, upon the most efficient ways in which these functions can be fulfilled. This claim requires critical examination. Even if it were possible to identify best practice, what is incorporated into ERP design inevitably lags behind latest best practice. Moreover, the emphasis on ERP as a driver towards best practice may be in tension with another claim, which is that these processes are diverse and flexible enough to be able to cater for the full range of possible organisational practices and thus able to be adapted to any particular organisations (Kumar and van Hillegersberg 2000; Westrup and Knight 2000).
More fundamentally, there may be different views about what constitutes best practice (Wagner and Newall 2004). Ultimately it is the view of the vendor, not the user organisations, concerning what constitutes best practice that is incorporated in the package (Davenport 1998). A vendor’s products will tend to converge around the firms and sectors in which they emerged and were first applied. Firms within the same sector vary, of course, because of differences in their size, particular markets, history and strategy. More obvious differences arise between different industrial sectors as well as between nations and cultures - as became apparent when ERP vendors sought to sell their products into emerging economies such as China (Soh and Sia 2004; Xue et al. 2005; Liang et al. 2004; Liang and Xue 2004).
The offerings of different vendors vary in the range of functions supported and the way in which they are carried out. While in its earliest stages ERP was presented as offering universal solutions for all firms, with greater application experience it became apparent that misalignment between the functions supported by the software and the goals of the user organisation was common (Wei et al. 2005; Soh et al. 2000). This in turn provoked the diagnosis that one of the critical success factors for ERP implementation was the ‘fit’ between the software and the organisation (Alves and Finkelstein 2002; Hong and Kim 2002). Careful selection from supplier offerings was therefore vital. Attention thus turned to package procurement, and specifically the ERP selection process.
The ERP selectionlprocurement process
The procurement of packaged software is, however, an extremely complex and difficult task (Kumar et al. 2003; Alves and Finkelstein 2002). Complex organisational packaged solutions, like ERP systems, come with a huge array of software features and options. It is extremely difficult for the user organisation to assess these offerings and identify how well they match their own requirements in order to select the most appropriate option for their
own organisation, with its particular historical structure, methods of operation, goals and context. The vendors promote the capabilities of their offerings using different terminologies; it can be difficult for potential adopters to translate between these, and, no less important, between supplier descriptions and their own understandings of their requirements (Alves and Finkelstein 2002; Franch and Carvallo 2003). To offset this problem, particularly for such high-value strategic acquisitions, would-be adopters increasingly turn to various kinds of structured assessment procedures. For example, the use of formalised Requests-for-Proposals (RfP) and Requests-for-Information (RfI) have become widespread.
Here the customer issues a Request-for-Proposal, perhaps using templates that are widely available from industry advisory bodies, which requires vendors to describe the performance of their packages in a uniform manner (Finkelstein et al. 1996). This may help reduce, but by no means eliminates, uncertainty since some requirements, such as usability or software quality, can be difficult to assess (Franch and Carvallo 2003). Indeed, the properties of software technologies are hard to assess, not simply because non-material artefacts cannot be inspected in the way a physical object can be inspected, but also because their performance depends upon their operation within an organisation and its information systems and business and information practices. Many of the affordances of the complex software systems, in particular their ability to support particular work processes and goals, cannot readily be resolved on the basis of vendor statements about system capabilities, since the organisational effectiveness of the system will only become clear following a substantial implementation effort (and then can only be demonstrated when the system is in use).
Moreover, as we explore in Chapter 6, the choice facing the procurer goes beyond functional issues and includes some assessment about the performance of the vendor, now (what kind of post-sales support will it offer?) and in the future (for a long-lived infrastructural investment like ERP the potential adopter needs to ask whether the vendor will still exist in a dozen or so years; moreover, if so, will it still support this product or market?). Organisations have encountered problems in getting support for products that have been withdrawn or from firms that have gone out of business (Kumar et al. 2003). Here the potential purchaser must make decisions about an unknown future on the grounds of indirect evidence such as reputation and an assessment of the vendor’s commercial prospects (Alves and Finkelstein 2002; Kumar et al. 2003). Difficulties in assessing the future behaviour of unknown suppliers was one of the factors encouraging the frequently observed resort by user organisations, contemplating the acquisition of new technologies, to their existing providers of technology and expertise (Clausen and Koch 1999).
Finally, the choice between packages necessarily involves multiple tradeoffs, for example in terms of which functions are and are not supported and their relative importance to the user organisation (Alves and Finkelstein
2002). Formalised decision-support tools which have been proposed for software selection have been criticised as unhelpful as they revolve around unrealistic presumptions that the user organisation’s requirements and priorities are ‘knowable’ and ‘specifiable’ at the outset. Though these models attempt to apply decision theory, with the idea of converting selection into a rational technocratic process, as Finkelstein et al. observe:
they are weak in supporting multi-valued features, features valued in partially ordered sets and inexact-matching of features with requirements. The weighting techniques they employ generally require a total prioritisation ordering of requirements, an unjustified and misleading simplification, since commonly customers are only able to partially order requirements according to their significance.
(Finkelstein et al. 1996: 2)
The difficulties user organisations face in spelling out their requirements is well-known (Friedman 1989). More important, in practice, user priorities may evolve in the light of what functionality is available from COTS systems. Finkelstein et al. describe the iterative process through which organisations acquire requirements for packaged software solutions:
Customers start from an initial perception of their requirements and the priority they attach to them, normally based on a detailed understanding of their domains...their existing manual systems and other aspects of their operational environment.Initial requirements are revised on the basis of advertisements, package descriptions provided by suppliers, demonstrations, use of the packages and comparative studies provided by third parties (trade papers and the like).
(1996: 1)
Sawyer (2001) has suggested that expectations of where the technology is heading may influence selection as much as existing identified organisational needs. As a result, a more iterative matching process may instead be called for in software selection, balancing a number of non-commensurate priorities (Finkelstein et al. 1996; McKinney 1999; Alves and Finkelstein 2002).
Would-be adopters, seeking to choose between different vendor options, need to balance complex sets of considerations on the basis of incomplete information about both their requirements and the packages specifications (and as we shall see later, information that is often contradictory and contested) (Finkelstein et al. 1996; Kumar et al. 2003). Alves and Finkelstein (2002: 793) point out that organisations must come to a decision on the basis of ‘subjective judgements’ and suggest that the matching of supplier offerings against user priorities and goals requires ‘a sort of qualitative reasoning strategy’. This observation and the associated critique of rational decision models is a matter we explore in Chapter 6.
Decision-making
Incompleteness of information, and associated uncertainty about the features of different software packages and about the current and future behaviour of vendors, constitutes one reason why package selection could not be reduced to a simple technical rationality. The other reason, closely intertwined with the first, is that the decision to adopt and the selection process are, like all kinds of organisational decision-making, part of and subject to what is widely described as ‘organisational politics’ (Burns and Stalker 1961; Pettigrew 1973). However, the politics of ERP selection and adoption decisions has some distinctive features. Such acquisitions are expensive - usually $1 million or more (Mabert et al. 2001). This scale of investment, its organisation-wide scope, as well as the strategic goals around which ERP has been promoted, means that decisions will tend to be pushed up the organisation to senior managers and also promotes the resort to external consultants (Sawyer 2001). The increasing role of consultants and other external knowledge providers in IT acquisition and implementation must be seen as part of a broader picture of outsourcing IT provision and change management, but also reflects particular features of the supply of packaged solutions. As already noted, various kinds of intermediaries are needed to support the market-based supply of standardised organisational software products, to handle the complex transactions and knowledge flows between vendors and consumers (ibid.).
Potential adopters turn to consultants to assist them with the package selection process. This does not resolve uncertainties, however, but opens up new issues. These include, for example, the difficult issue about how to choose consultants. Would-be adopters, resorting to consultants to help them choose a software solution, then find themselves needing to consider what methodology they should use to select these consultants (Kumar et al. 2003).
The outsourcing of software supply changes the role of the adopting organisation’s internal IT function from being a provider of technologies to managing knowledge networks and negotiation processes across and beyond the user organisation (Procter et al. 1996; Howcroft and Light 2006). Technology acquisition is becoming a multi-player game. This sets up for complex interplays of power between senior management, information technology managers, other groups within the user organisation and, from outside, IT vendors and consultants (ibid.). There are asymmetries between these classes of player in terms of what is at stake and thus the incentives experienced. The groups within the adopting organisations are engaged in competition for political power and perceived centrality to the organisation (and some groups may be better placed to articulate their perspectives than others [Soh and Sia 2004]). External actors are engaged in a competition to win lucrative contracts, and build a reputation that may help them maintain income streams from that user organisation, and win other customers. These interactions are further characterised by an asymmetrical knowledge
distribution: with different groups contributing specific knowledge, respectively, of organisational requirements; of the existing IT infrastructure; and of package functionality; and coupled with differences in culture and orientation (Soh et al. 2000).
The selection and implementation of ERP involves very intense interactions as groups try to bridge these different areas of technical and organisational knowledge (Soh et al. 2000; Skok and Legge 2002). The wide range of involved players - and especially the asymmetry of knowledge and incentives - creates a very difficult terrain for negotiation and decision-making. Some user organisations, for example, experience problems in managing the relationship with consultants. This is because the multiplicity of players creates difficulties in allocating responsibility for failure, especially, as is frequently the case, where there were links between vendor and consultant, which compounded the problems arising from the lack of clarity about roles and motives (ibid.). Hislop’s (2002) case studies of ERP adoption show that the role of consultants varied, shaped by a pre-existing web of social relations, which sometimes gave opportunities for them to exercise greater autonomy and influence. This had important consequences where the consultants had greater influence, and meant organisations tended to favour radical implementation of ERP packages, in comparison to the cases where the user organisation maintained greater influence, which tended to favour extensive customisation.
Implementation choices - how much customisation?
ERP vendors stress the capacity of their systems to provide best practice solutions for all firms and the advantages of ‘vanilla implementation’ of ERP packages (utilising the functionality it incorporates without adaptation). They also place considerable attention on the costs and risks that may be associated with attempts to customise packaged solutions (SAP INSIGHT 2006) and which are invoked as underpinning instances where ERP failed or did not deliver expected benefits. The widely publicised cases in the late 1990s where a significant proportion of package implementation projects undertaken massively overran time and cost budgets were attributed to efforts to customise packages and integrate them with other application environments (Sprott 2000).
Customisation was portrayed by vendors and ERP consultants as liable to prejudice the success of ERP, bringing high costs when the system is implemented. These include substantial expenditure on consultancy and customisation activities and delayed implementation, but also problems that occur over the lifecycle of the package: these include issues with the reliability of systems, their maintenance and difficulties installing upgrades, and a reduced longevity of the system (Davenport 1998; Liang et al. 2004; Soh et al. 2000).
It is striking, therefore, that the practices of user organisations have diverged sharply from these recommendations. Surveys of ERP implementations
in the late 1990s revealed some instances (representing a minority of implementations) in which organisations, having identified gaps between the package and their organisation’s ways of doing things, decided to just live with these gaps or re-engineer their business processes to meet the requirements of the package. More frequently (and in the majority of implementations), there was an extensive process of customisation and adaptation of the software systems. These adaptations could take various forms:
1	Configuring the package: adjusting the packages according to built-in parameters. This is a necessary part of installing any packaged solution insofar as the package must be configured to meet the technical context of adoption. The variety of options catered for in the package design, including the libraries of standardised business processes for particular industries, allows a certain level of variation in user organisation practices to be catered for. However, such configuration can be challenging to implement and it is often easier to use default values (a phenomenon which Clausen and Koch [1999] and Pollock and Cornford [2004] have described as the ‘power of default’).
2	Customising the package: this refers to more extensive changes involving modification - rewriting code - in the package. Some degree of ERP customisation is possible; however, the complexity of these systems makes major modifications impracticable (Davenport 1998) and liable to generate unintended clashes with other parts of the system, and thus undermine its reliability and robustness. Customisation requires a considerable effort in programming and testing (Soh et al. 2000). Suppliers therefore seek to restrict and regulate which parts of their systems the users can adjust, to prevent any changes that could prejudice reliable operation of core processes, maintainability and future compatibility (e.g. for upgrades). In SAP’s case, for instance, they clearly annotate within the code which parts of the system can be modified and which cannot. Problems may be most acute where there is a misfit between the user organisation’s requirements and the ERP package in terms of the relationships among entities represented in the underlying data model. This generates an incentive to customise the package. However, this requires changing the structure and relationship of the table objects, which are viewed by vendors as prohibitive core changes to the ERP packages. Customisation may result in loss of vendor support (Brehm et al. 2001), may impede post-adoption support (e.g. maintainability and the ability to implement vendor upgrades), and may shorten the longevity of the system (Soh et al. 2000; Richmond et al. 2006).
3	Partial, selective implementation of package: the modular design of ERP packages facilitates their partial implementation. Not all the functions may be needed by a firm (Davenport 1998). A selective approach to customisation may be most acutely needed where sharp differences exist
between the ERP system presumptions and the particular context and goals of the user organisation. Particular challenges arise, for example, where ERP is being installed in other countries and cultures (Liang et al. 2004). Linked to this are decisions about how much integration is needed, particularly in multi-site organisations, where strategic decisions may be taken to centralise some, and not centralise other, functions. For example, multinational corporations may decide not to install a monolithic central Human Resources system as employment laws and taxation vary between nations and change frequently and at different dates (Clemmons and Simon 2001).
4	Add-ons, bolt-ons or ‘extension software’: where the package functionality was lacking or not well-matched to organisation requirements, additional modules, described variously as add-ons, bolt-ons or ‘extension software’, were developed or acquired from other providers to provide missing functionality or links were established with existing legacy systems (Kumar et al. 2003). Integration of legacy or add-on components presented significant challenges, not so much because of technical difficulties in getting different systems to talk to each other but because of ‘differences in semantics and business rules between different applications that were never intended to collaborate’ (Sprott 2000: 66). This reflected the fact that the first ERP packages were mainly designed with an inward focus.
5	‘Best of breed’ - multi-vendor systems: despite these integration difficulties some organisations chose a range of modules to secure the particular functionalities they required from several vendors. This became known as the ‘best of breed’ (Sprott 2000). This extension of the add-on strategy became more feasible where packages had in effect become established platforms (as happened, for example with SAP and other major ERP suppliers), which enabled other suppliers to develop bolt-on or ‘extension software’ designed to interoperate with the core functionality of the ERP system. Add-on and best of breed approaches offer benefits to the user organisation in terms of meeting exceptional requirements not supported by generic packages or of securing a better fit to their specific requirements; there are some costs in terms of greater interface and implementation problems (Mabert et al. 2001). However to the extent that they have been generated by third-party providers in collaboration with ERP vendors, there may be less configuration effort than with customisation, and no loss of vendor support (Brehm et al. 2001).
The picture this brief review reveals stands in contrast to the widespread portrayal of ERP adoption as imposing a stark choice between acquiring enterprise solutions as a custom solution versus a packaged solution, and in the latter instance a choice between adapting the package or alignment with its requirements. Instead, we find a wide spectrum of choice (Brehm et al. 2001; Light et al. 2001). This range of choices seems to undermine the portrayal of ERP as a rigid and monolithic approach - interpreted by its
proponents as strong but bitter medicine and by its opponents as the reason that ERP is risky and not appropriate.
When an ERP package is adopted, some tailoring is always required. The ‘portfolio’ of tailoring options (Brehm et al. 2001) encompasses different strategies that offer different trade-offs in terms of cost and the extent to which the organisation’s information systems and business processes were adapted to its perceived requirements and methods of operation or vice versa (Mabert et al. 2001). Hislop has produced a more detailed typology of ERP package customisation options - summarised in Table 1.6 below - which distinguishes some specific types of customisation that may arise, for example in relation to creation of particular kinds of user interface (screen masks) and reports. The latter are relatively easy to achieve without threatening the core of the system (Soh et al. 2000).
Brehm et al. (2001) examine how these different customisation efforts differ in terms of the amount of effort required to develop and also the effort required for system maintenance and post-implementation (e.g. upgrades) and in the distribution of that effort, for example, between the user organisation and third party suppliers of bolt-ons.
The package paradox revisited
These choices underpin what has been described as the package paradox (Brady et al. 1992; Fincham et al. 1994). To summarise this argument, the user organisation faces a dilemma regarding how much to adapt and enhance the package they have acquired, as this may prejudice maintainability and ability to take on board future enhancements and upgrades. If they undertake too much customisation they will find themselves with a system that is no longer a package, and thus lose the benefits of having a standardised system, in terms of being cheap, robust and supported (particularly in terms of an ability to migrate to subsequent releases). However, the development of inter-operable add-ons to cater for specialised requirements not supported by the ERP vendors changes the parameters for this choice. In particular, where a third party vendor supplies the add-ons, it can share the cost of developing the module across multiple customers and, crucially, the user organisation transfers to that third party the responsibility for developing the interfaces, and testing the module’s compatibility with the ERP system and its upgrades.
While Clausen and Koch (1999) had characterised the monolithic ERP systems as inflexible (‘bricks’) in the hands of local organisational actors, in contrast to the more malleable products of smaller independent suppliers (‘clay’), the resort to add-ons and multi-vendor solutions opens up a wider range of choices to would-be user organisations. It potentially changes the role of the ERP core to that of a platform or infrastructure, around which the user organisation can select an appropriate range of vendor- or independently-produced modules. This resembles the ‘pick-and-mix’ strategy
Table 1.6 Typology of ERP customisation options
No	Type of customisation	Description
1	Configuration	Setting of parameters (or tables) in order to choose between different executions of processes and functions in the software package
2	Bolt-ons	Implementation of third party software designed to work with the ERP system
3	Screen masks	Creation of new screen masks for input and output data
4	Extended reporting	Programming of non-standard workflows
5	Workflow programming	Creation of non-standard workflows
6	User exists	Programming of additional software code in an open interface
7	ERP programming	Programming of additional applications without changing the source code
8	Interface development	Programming of interfaces to legacy systems of third party products
9	Package code modification	Changing the source code ranging from small changes to changing whole modules
10	Develop new modules	Development and production of completely new modules with different functionality not available in existing modules
Source: adapted from Hislop 2002: 661.
by which users could build ‘configurational technology’ solutions to match their purposes from a selection of custom and commodified components (Fleck 1988b; Fleck et al. 1990; Brady et al. 1992).
ERP artefacts continue to evolve. More recently SAP and other ERP suppliers, notably including Microsoft, have sought to make their offerings easier to implement, for example by including tools to make it simpler for users to configure their own implementations. This was geared in particular to making ERP more attractive to the large market of small and mediumsized enterprises which had hitherto tended to see ERP as designed and suitable only for larger firms. Such developments may undermine the trend to see a package as a platform for best of breed configurations.
Post implementation
The connections already described between implementation choices and issues of post-implementation maintainability underline the need to address the whole package implementation lifecycle with technologies such as ERP. The career of the package within the implementation arena of its organisational users (the ‘project’ lifecycle) also represents one moment in a bigger picture in the evolution of the technology itself (the ‘product’ lifecycle), alongside the further development of the software by vendors.
The organisational appropriation of the package
For the user organisation, the adoption of ERP constitutes a major and long-term investment - one that shapes the user organisation’s information systems and practices for extended periods. The benefits of ERP in terms of organisational performance typically take a long time to accrue. Mabert et al. (2001) observe that it takes 12 months before any productivity growth will be observed. This can be attributed to an extended process of experimentation as the organisation grapples with the affordances of these enormously complex systems for its own purposes; identifies misfits and gaps between the system and ways in which the organisation has carried out its business and sorts out ways of overcoming these gaps. This may involve re-aligning organisation practices to system requirements; developing workarounds to bridge between system functionality (and gaps in functionality) and organisational process; finding new ways of utilising existing functionality; as well as configuring and customising the system (Soh et al. 2000). Managerial guidelines for how to achieve successful outcomes from ERP implementation place increasing weight on the post-implementation phase (Somers and Nelson 2004), as evinced by the suggestion by Wei et al. (2005) that it should be subdivided into what they call a ‘shakedown phase’ and an ‘onward and upward phase’. These observations point to important processes of social learning as an initially generic software suite is appropriated or domesticated within a particular organisational setting. Work has to be done to embed the technology and incorporate it within the organisation’s information practices and processes (Williams et al. 2005).
Maintenance, upgrades
With the service model of software delivery, maintenance would be linked to the initial software provision, and the user organisation could choose to use that supplier for further development of its solution. With the shift to packaged software, and systems sold as a product, however, the arrangements for maintenance and enhancement change radically. They become more formalised and governed by the exigencies of commodification (Sawyer 2001). Maintenance and upgrades, for instance, represent an important source of
revenue for ERP vendors. Though corrective maintenance (patches and bug fixes) are typically provided as part of the licence fee, software enhancements become the basis for new releases that can be sold profitably to existing customers. These users are already effectively ‘locked in’ to the product by the substantial sunk investments of money, time and energy to implement the system and get it to work in their organisational setting (Kremer and van Dissell 2000; Sawyer 2001). Vendors have to balance the number of new releases (and new sales and additional licence fees expected) against the need to minimise the number of different software versions they need to support and service, in a context in which many users are slow to take on board new supplier upgrades.
Post-implementation support
The supply of standard packaged organisational technologies poses challenges for vendors in providing post-implementation support for differently implemented products, particularly as these become components, and often the backbone, of a customer’s broader information infrastructure. Organisational information infrastructures are configurational technologies (Fleck 1988b), built from a combination of standard and custom technology components from different suppliers, selected and adapted to the user’s context and purposes - even in the case of ERP implementations in which some of the components are very large. System failures and bugs may arise through unpredicted interactions between these components in various different states. The idea of modular design to limit unintended interactions and interference is a central aspect of computer systems philosophy (evinced through principles such as ‘componentisation’ and ‘composability’7). However, the proliferation of technology components that may be combined into inter-operating systems, and the fact that these come in multiple versions that themselves are being constantly updated and revised, and that they may be configured in different ways, all contributes to incalculably large numbers of variations in any system’s operation. One important result of this is that it may be very difficult to diagnose the reasons for failure when things go wrong. For instance, as we will show in Chapter 8, there may be difficulties in reproducing problems that arise - and this poses enormous challenges for support services.
These discussions provide an occasion to remark upon the changing character of technical knowledge in system development, particularly in relation to customer support and maintenance. Computer-support knowledge becomes not so much a question of depth of technical expertise but more one of identifying interactions between a product and a wide variety of other components in different configurations and different states. In resolving these calls for extensive, and by implication shallower, knowledge of the enormously wide array of potential interactions between their product and other elements of the broader socio-technical infrastructure, it creates issues
about how such knowledge is acquired and communicated. Where such expertise emerges spontaneously, it has a strong tacit element: learnt through practice and communicated through informal networks.
The further evolution of systems
The release of upgrades, new modules and new versions of the software package geared towards particular market segments constitutes a process of incremental innovation in ERP packages - with new functionality added to the core transaction process and database infrastructure. What is supplied is not a single product, but a bundle of capabilities, which may be evolving in parallel in a continuing wave front of innovations, built around a relatively stable core.
In between these periods of stability, and as we have already seen, there have also been more radical transitions. These have been occasioned most notably by changes in underlying system architectures, and also by changes in the wider business prescriptions which have reoriented the focus of technological activity in different directions, and which have prompted suppliers to promote ERP in radically different ways.
From 2000, commentators had been discussing the need for another radical shift in the conception and architecture of ERP. As we have already mentioned, the influential industry analyst Gartner in late 2000 declared ERP ‘dead’, predicting that by 2005, ERP would be replaced by what they described as ‘extended ERP’ or ‘ERP II’ (Holsapple and Sena 1999; Gillman et al. 2002; Gould 2002). This involved a coming together of two contradictory sets of ideas.
First were calls for a modular, component-based architecture to facilitate integration. Existing ERP systems had been driven by an emphasis on increasing functionality (through an accumulation of package parameters, options and configurable functionality) at the expense of ease of upgrading. It was proposed that this could be redressed by shifting to alternative information system architectures comprising flexible and customised federations of smaller components, perhaps relying on component-based technologies (Holsapple and Sena 1999; Kumar and van Hillegersberg 2000; Sprott 2000). Commentators were sceptical that vendors would cooperate in such an endeavour and in particular, that ERP vendors would embrace the idea of opening up their market for other suppliers and just selling the information backbone onto which other components linked (Davenport 2000a). Despite the theoretical benefits of a component-based architecture, the transition would require considerable investment. In the absence of customer demand, most ERP vendors have selected a safer approach, and instead have sought to upgrade their systems within existing architectures, using componentbased Application Program Interfaces (API) to construct a facade’ for their existing application (Jakovljevic 2001; Judd 2006). Some companies, notably market leaders SAP and Baan, opened up their offerings as platforms with
service-based architectures that enabled business transactions and information requests from outside the core application, which facilitated the emergence of a thriving market for add-on components (Sprott 2000; Markus et al. 2000). Though it appeared that some ERP vendors saw advantages in allowing third party supply in catering for specialist requirements too small for them to address profitably, they did not want to see their position in the enterprise solution market eroded. Instead, we see them seeking to expand their markets by developing new functionality (notably in the area of customer relationship management) and by taking over established suppliers, arguing that this was more beneficial than the ‘best of breed’ strategy for customers who wanted seamless systems rather than having to bear the integration costs of fragmented component supply (Jakovljevic 2001; Stutz and Klaber 2006).
The proposed changes in technical infrastructure were coupled with changes in the business goals to which vendor efforts were directed. Thus, it was suggested, component-based architectures could help ERP II take on board new functions and overcome weaknesses in decision support and knowledge management, and in particular provide better support for inter-organisational linkages (Kumar and van Hillegersberg 2000; Davenport 2000a, 2000b). This reflects the second line of criticism of ERP which focuses upon its imputed ‘introverted’ enterprise perspective, as emerging business discourses began to focus the search for competitive advantage on the performance of ‘value-driven networks’ rather than individual enterprises (Davenport 2000a, 2000b). This was to be achieved through customer relationship management and supply-chain management modules, conceived as add-on modules to ERP. These were areas where significant markets had sprung up, in which other suppliers had a lead over ERP vendors. However, ERP systems, with their proprietary interfaces and complex database structures, were designed for enterprise-level integration, not information sharing across different firms in the supply chain (Jakovljevic 2001).
One pathway towards supply-chain integration with ERP had been mapped out based upon ‘componentisation’. This is using component interface protocols (such as CORBA) and integration standards (such as XML) as well as e-commerce semantic agreements (such as CommerceOne) about the meaning of business data exchanges (Markus et al. 2000; Sprott 2000; Gillman et al. 2002). However, it remains far from clear that this model of networks of inter-operating ERP systems will prevail in comparison, for example, to models based around third party facilitation (e.g. by portals) or by a hub in the supply-chain maintaining supply-chain management features linked to its own ERP system with its supply-chain partners linking to these (Markus et al. 2000).
The industry analyst Gartner in 2000 predicted that by 2005, ERP II would displace ERP as the means to support internal and inter-enterprise process efficiency, while warning that many existing ERP vendors would not be able to achieve this paradigm shift because of the huge investments and
technical challenges involved in redeveloping systems. Gartner’s predictions (and warnings) were only partly borne out by subsequent experience. Judd (2006) notes that traditional ERP vendors were quick to rebadge their existing packages as meeting the ERP II requirements but to date have delivered only what he terms ‘ERP-1.5’ - that is, industry-customised versions of their existing products. ERP vendors have offered extensions for supply-chain management (SCM), customer relationship management (CRM), warehouse management and other functions such as call-centre management (Gould 2002). However, this brings them into competition with established suppliers in these fields (Sprott 2000), which has provoked some convergence in these markets, accompanied by takeovers.8
At the same time, radically different service models are also in the offing. The success of CRM vendor, salesforce.com, which does not require customers to install and operate its software but provides the service for them for a monthly fee, catalysed a dramatic change in the model for delivering/licen-sing CRM applications. This idea underpinned what has come to be known as Software as a Service (SaaS) and has been extended from CRM to ERP web-hosting in which user organisations outsource their information systems to third parties who will maintain and operate ‘SAP by the hour’ (Garretson 2005; Sweeney and Shepherd 2006; Stutz and Klaber 2006). The lower entry costs have been seen as enabling ERP vendors to break into the applications market for small and medium-sized firms that have hitherto avoided a technology that seemed exclusively geared towards large organisations.
In summary, we see that organisational software packages follow a complex trajectory, shaped by their history and the strategies of a range of players - suppliers, organisational users, consultants and, increasingly, industry analysts. What we want to do in the next two chapters is to begin to revisit some of these themes and conceptualise them in more theoretical terms.
2	Critique of existing knowledge
Introduction
What we know about technologies, their innovation processes and outcomes is closely bound up with how we know them. Though the last chapter was primarily descriptive, this chapter and the one that follows map out the differing ways in which technologies, and in particular workplace technologies, have been interpreted and analysed. The analytical frameworks we adopt in an enquiry are important, embodying, as they do, assumptions about the world and about how we may investigate it. They thus pattern the tools and methods of enquiry and shape the kinds of understanding we are likely to reach.
This chapter therefore advances a critique, from the viewpoint of our own discipline, Science and Technology Studies (STS), of the dominant frameworks through which technologies and their implications have been understood. It identifies some of the key issues and debates around which we seek to carry forwards STS analysis. Chapter 3 continues this intellectual journey, though with a different goal of mapping out our framework for analysis.
Following this short introduction to this chapter, we first explore the discourses articulated by the proponents of new technologies, which we describe under the rubric of the ‘rhetorics of technology supply’. STS writings have engaged critically with these rhetorics for their ‘supply-side bias’: the claims advanced about the capabilities of technology were portrayed as coercive - a misleading means of ‘selling’ technology inappropriately to unwary potential users. They held out the promise that new technologies could be rationally and reliably deployed to deliver radical organisational improvement. Social scientific research has engaged critically with the claim that technology can be so rationally and instrumentally deployed. More recently, STS analysis has begun to move beyond critique and see how these discourses are a medium for shaping expectations, requiring serious (as well as critical) analysis.
One of the key goals of STS has been to reject the traditional portrayal of technologies (and the means by which we assess them) as being in a ‘technical realm’, somehow separated from social and political process. From its earliest days, STS has criticised this boundary, insisting that there is a
seamless web between technical and social (Hughes 1983). Much STS research has been concerned to reveal the social, economic and political agendas inscribed in what may be presented as narrowly ‘technical’ choices. An alternative account has emerged (broadly inspired by a ‘social constructivist’ agenda) which emphasises the contexted and contested character of artefacts (and also the criteria and tools by which we assess technologies). Much of this work highlights the role of organisational politics: the struggles between groups with differing organisational settings, expertise and commitments. Some have taken this point further, arguing that, as assessment criteria and assessment are contestable and contested, we can never really know the material properties of artefacts, but only beliefs about these properties. We thus find a marked polarisation between accounts of innovation in terms of the operation of ‘technical’ rationality versus its portrayal as a ‘political process’. Technology decision-making is always subject to potentially conflicting criteria under conditions of uncertainty and incomplete information. It can never be a narrowly ‘technical’ rational process. Though some have concluded from this that political processes are in command and that technical assessments are subordinate to this, our analysis seeks to go beyond such dichotomised understandings. Instead, we seek to analyse and take seriously the work of assessment of technology. We explore how various assessment criteria are produced and refined in the course of decisionmaking over technology.
Having outlined the perspective that we will be developing in this book, we then explore some of the main forms of specialised and professional knowledge that bear upon the industrial application of technologies, focusing in particular upon Computer Science, and work emerging from Business Studies and Economics, encompassing both their general views about technology and more specific accounts of the development, adoption and use of packaged enterprise software systems such as ERP. We discuss the consequences of the fragmentation of the knowledge base between different specialist disciplines and the uneven development and fragmentation of research between different kinds of study, addressing the development, implementation and use of new technologies.
Finally, we return to the domain of STS and explore the debates within this field about how to theorise the relationship between technology and society -and, associated with this, about what explanatory models and methodologies are needed for an effective understanding. There has been a debate between various currents within the area of STS, in particular between those that emphasised the influence of broader social structures versus the efforts of particular actors. This has been an area of creative tension that has led, for example, to a reworking of our own analytic framework based upon MacKenzie and Wajcman’s (1985) Social Shaping of Technology (SST) perspective. We show how the Social Learning framework (Williams et al. 2005) emerged as an extension of SST, which seeks an integrated view encompassing, for example, the moments of technology design and implementation.
Chapter 3 continues this critical examination with a view to producing an analytical framework adequate to the goals of the book and the research project it was based on. It will explore the various approaches that have been adopted for the study of technology and organisations and their explanatory weaknesses that have led to the biography of artefacts perspectives.
Interrogating the promises: addressing the rhetorics of technology supply
The promise of new technologies - as a driver of organisational change
Current popular discourses about the organisational ‘impacts’ of the introduction of new Information and Communications Technologies (ICTs) into the workplace frequently portray technology as a driver of organisational change. Such views have prevailed with little modification since the 1970s when the growing availability and falling price of microelectronics paved the way for the widespread implementation of ICT applications in the workplace. Such accounts correspond closely to the rhetorical claims advanced by technology vendors and promoters but can also be seen to embody a more general supply-side perspective, for example in their tacit view of technology as the causal factor (or independent variable) and organisational change as the outcome (dependent variable). Such accounts typically portray technology as an exogenous factor - emerging from a separate technical realm. Moreover, technology design is presented as capable of anticipating requirements for more efficient or more effective organisational arrangements -indeed, embodying ‘best practice’ - that will, when implemented, deliver organisational improvement. As we have already seen in Chapter 1, the history of ERP would provide a sharp challenge to the exogenous view of technology. There has also been protracted criticism, particularly from STS, of the technological determinism inherent in the portrayal of technology as a driver of organisational change - as if specific artefacts required particular sets of social or organisational arrangements. Instead, an array of social scientific studies of technology adoption point to the diversity of potential work organisation arrangements with particular artefacts (Wilkinson 1983; Williams and Edge 1996).
The promise of new technology as offering rational control - subject to technical, managerial or economic rationality
Closely related to these discourses of transformation is the promise that new technology offered greater certainty and rational control; that it would provide a solution to the problems faced by the organisation. We see this in various accounts, which offered an ‘instrumental’ view: that technology could readily be subject to rational choice and control and could be shaped to managerial/organisational goals. We find this kind of view expressed
across a number of professional discourses of technology, which conceive technological change as subject to rationality, conceived differently, depending on the disciplinary commitments of the commentator, in more or less straightforward technical, managerial or economic terms.1 This claim to rationality and control can be seen as a claim about both the character and performance of technology and the capacity of a particular group to deploy it for particular ends (Bloomfield and Best 1992; Bloomfield and Danieli 1995). Technology decision-making within such accounts is similarly portrayed as straightforward - subject, in other words, to a calculative logic.
In the early stages of a new and promising technology or technique, the business and specialist press is often full of success stories. These can come in the form of case studies of the successful adoption and organisational impacts of a new technology or technique. Sometimes these are promulgated by those promoting the new technology as evidence of its power and relevance for other organisations. Even where there is no such link with technology promoters, in the early stages of adoption when general enchantment with a promising new technology is high and there is little evidence to challenge suppliers’ claims, we can find many uncritical accounts of the benefits of an exciting new technology. More sceptical and empirically informed accounts often do not appear until later, causing some to talk of cycles of enthusiasm and scepticism about new innovations as fads or fashions (Abrahamson 1991).
This kind of thinking is evident in much engineering and managerial literature uncritically extolling the benefits of new IT applications. It is often closely associated with technology supply and the promotion of a new class of technology (for example from consultants, vendors or academics working with them). Interestingly, many of the social scientific accounts, even those more critical perspectives such as, for example, early writings on ICT by Industrial or Organisational Sociology, can be seen to have uncritically accepted supplier claims about their consequences (Fleck et al. 1990).
One can readily find such accounts, for example visibly displayed in airport bookshops, about the introduction of ERP today (for example O’Leary 2000). Although contemporary accounts, needing to address widely advertised failures of new technologies, are less naive than the first generation of promotional literature about their ready benefits, and are couched with caveats regarding the need to manage the introduction of technology well, the claims regarding the capability of new supplier offerings are just as clearly stated. The rhetorics of technology supply are remarkably persistent: thus, we see how the transformatory promises of enterprise-wide systems continue undiminished, even though, over time, enterprise systems technologies have been reworked and the goals to which these technologies are primarily directed have changed, from improving productivity to improving efficiency and effectiveness to adding value.
These kinds of discourse have effects. Indeed, their primary motive is to promote a particular vision of how the world could or perhaps should be and to induce others to buy into and align themselves with such a vision.
The perspective we have described as the rhetorics of technology supply, in promoting the promise of technology, also conveys a strong message about the desirability and indeed, perhaps, the inevitability of following particular technological pathways.
We need to pay attention to the shift in knowledge relevancies engendered by the rhetorics of technology supply. The vision of the transformative capacity of new technologies such as ERP typically privileges sites and practices of technology design over those of implementation, consumption and use. Conversely, it also invites a reading of potential user organisations as amenable to technically driven restructuring. In relation to enterprise solutions it conveyed a sense that it was possible to generate abstract generic software models that could capture and replicate the key activities of the huge variety of potential user organisations, with their array of particular methods and practices. The proposition that organisations could be reduced to such finite sets of algorithms and relationships on databases conveyed a sense that organisations were in essence fairly simple or, at least, that their information and business functions were rather similar, and amenable to capture in this way (Webster and Williams 1993; Robertson et al. 1996). However, a body of writing from Technologies Studies and other social science research has criticised the idea that technology supply can generate solutions that will deliver this kind of improvement. For example, it has raised questions about the extent to which technology supply can adequately understand, anticipate and cater for the needs of a particular user organisation and its members.
Finally, we draw attention to the way in which the rhetorics of technology supply serve to delete certain experiences. The discourses of technology supply are concerned with ‘the new’ - with novel artefacts that have no history - except to reassure us that the new solution overcomes the shortcomings that may have been encountered with its predecessors. According to these claims, the previous (often painfully acquired) experience with information systems, like the knowledge base from which they were built and used, is a past that needs to be deleted, along with the artefacts themselves, which are now termed legacy systems,2 and replaced with the next generation; the modern.3 And here we note a rather distinctive feature of information technology business solution and change management theories, which have proceeded through a series of proclaimed technological revolutions (though often on examination revealed as involving the relaunch of enhanced core technologies as a radically new and different product) - each with its own ephemeral acronym - with the claim that the new offering is the next obligatory solution which the successful organisation must urgently acquire (Fleck et al. 1990; Newell et al. 2000; Heusinkveld et al. 2005). The technological determinism they deploy is thus a device for ‘discursive closure’ (Leonardi and Jackson 2004).
These considerations focus our attention on the need to analyse the rhetorics accompanying new technologies. Technology and Organisation
Studies have, perhaps understandably, focused on refuting their deterministic messages and supply-side bias. Rather than merely limit ourselves to this (undoubtedly important) critique, we need to conduct detailed analysis of how the rhetorics of technology supply operate as a medium of influence; a space where the promises and futures of new technologies are contested, shaping perceptions of technology markets, innovation trajectories and the benefits a new technology will bring.
Understanding technological innovation
The debate between technical rationality and political process
Elements of the STS critique of these supply-side discourses can be seen to apply equally to some academic analyses of the deployment of new technologies. Writings emerging from Computer Sciences and from Business Schools (work from the Management of Technology and Strategic Management perspectives, for example, adopting a decision theoretic approach) have inherited and carried forward the idea that technology deployment can be subject to particular forms of technical rationality. That is, the properties and performance of technologies can be assessed in technical or financial terms, and their selection and implementation can therefore be guided to optimise economic and business outcomes.
In contrast to this, an alternative tradition in studies of technology and organisation that can be traced back to Burns and Stalker (1961) has explored the political processes that surround the deployment of new technological knowledge within organisational settings. This tradition points to the range of individuals and groups involved in innovation, with their particular roles and locales within and outside the adopting organisation, which in turn underpin their differing expertise bases, perspectives and commitments. These groups deploy more or less competing views about particular innovations, which shape their visions of good practice and the criteria needed to assess these. Analyses from Technology or Organisational Studies of the political processes surrounding innovation thus emphasise the conflicts encircling the design and implementation of new technologies, and also the contestable and contested nature of the criteria used to assess technology, which emerge through the activities of individuals and groups seeking to advance their positions within particular settings (Koch 2000). Some accounts portray the political process as thoroughly in command and emphasise, for instance, the attendant conflicts, myth and drama in which the particular rationales for change are portrayed as secondary to political processes within the organisation and between the firms and its external sources of expertise (Pettigrew 1985; Abrahamson 1996; Knights and Morgan 1991).
We wish to examine further this debate between these two contrasting broad analytical approaches - which have counterposed technical rationality
versus political process. The dichotomy is mirrored by other, related analytical schisms that bisect research into the application of ICTs. Thus, Orlikowski and Baroudi (1991) have distinguished between their own interpretive social science tradition and a more traditional positivist research perspective, which assumes that the phenomena under investigation exhibit broadly fixed relationships, and are therefore amenable to similar methods of enquiry to the natural sciences. The bulk of research into information systems, for example, follows this positivist tradition and assumes that decisions can be subject to rational decision-making (Dibbern et al. 2004; Howcroft and Light 2006).
In a similar connection, Kling (1980) highlighted the gulf in theoretical analysis of computing between work that adopts a System Rationalist perspective, in which assumed common values across the organisation mean that the social commitments of different stakeholders are overlooked,4 and a Segmented Institutionalist perspective, where stakeholders are seen to pursue their private interests over and above organisation goals. These lines of debate can be seen as linked to a broader discussion within the social sciences between what might be characterised as the social constructivist and the positivist research traditions. While sympathetic to the general insights from social constructivism, and in particular with its critical project to explore how technological artefacts and practices are shaped by the contexts and actors in which they emerge, we are concerned about its potentially relativist outcomes (Williams and Edge 1996; McLoughlin 1999). In this book, we seek to go beyond the counter-position of rational and political accounts. Such a dichotomised understanding either wholly accepts the proponents’ claims about the effectiveness of decision criteria (and, for that matter, of technologies) or portrays them as epiphenomena of some underlying process of conflict between competing interests. Rather than reproduce this polarisation, we seek to sidestep it. We are instead interested to explore how technologies and the criteria for assessing them are constructed, tested and contested, and indeed performed in the process of technological innovation.
Before we do this, it is necessary to clarify our understanding of technology and our relationship to the various currents of constructivist analysis of technology.
Breaking down the boundary between ‘the technical’ and ‘the social’
The portrayal of technology as subject to technical rationality typically involves engineers drawing a boundary between the technical and the social - in which choices in the technical realm may be subject to formal, perhaps mathematically expressed criteria, such that a technically rational decision can be achieved. In reaction to this, one of the key concerns of STS throughout its short life as a domain of academic enquiry has been to critically address the boundary that is thereby drawn between ‘the technical’ and ‘the social’.
Writers like Hughes (1983) noted the lack of a clear distinction between technical and social; instead, he argued that there was a ‘seamless web’ between them such that it was necessary to address the profoundly ‘socio-technical’ characteristics of innovation (Law 1988). This reflected the intellectual project of STS that social scientific enquiry should no longer be excluded (as other social sciences had largely accepted) from the technical realm. Instead it was important to ‘open the black box’ of science and technology and subject them to analysis (MacKenzie and Wajcman 1985; Latour 1987; Williams and Edge 1996). When STS writers came to examine the minutiae of even such apparently narrowly technical features as the circuitry of a computer chip or the design of a missile guidance system, diverse social and political influences could readily be detected (MacKenzie 1990).
Much STS writing, both within the early critical traditions (for example Dickson 1974) and subsequent constructivist approaches (for example Bijker and Law 1992), sees in the constitution of the boundary between society and technology a political manoeuvre in which certain facts and artefacts (and the technical specialists who deploy them) are presented as neutral and somehow separate from the conflicts of values and interests that beset relations in social and economic life. Technical specialists seek to maintain a boundary between what is ‘technical’ and what is ‘political’: ‘seeing politics as intruding on technology either when there is no single best technical solution, or where the best solution is not adopted’ (MacKenzie 1990: 413). Among information technology specialists, for example, ‘‘politics” have been equated with evil, corruption and, worst of all, blasphemy in the presence of the Rational Ideal’ (Keen 1981: 31). However, typically, technical specialists describe as ‘political’ decisions those that went against their preferred option. The definition of something as ‘technical’ provides discursive resources for groups to reinforce their own views, and undermine their opponents. It closes off from inspection the negotiations and exercises of power that underpinned the definition of a matter as technical (Bloomfield and Vurdubakis 1994). These considerations underpin the critical attention of STS analyses to how matters are constructed, as technical. STS insists that we question who is drawing these boundaries, and for what purposes.5
Addressing the material properties of artefacts
Woolgar and Grint (1991) have taken this argument further - arguing that we can never know the capacities of machines, only the beliefs circulated by various parties about these capacities. For them, ‘the nature and capacity of the technology remain essentially indeterminate’ (ibid.: 370). Grint and Woolgar thus reject, on the grounds of the multiplicity of possible accounts about these technical properties, any attempt to engage with the material properties of artefacts (Woolgar and Grint 1991; Grint and Woolgar 1997). Though there is value in their critique of naive realist accounts which take at
face value the claimed material properties of machines, Grint and Woolgar’s ontologically relativist stance has been contested (Kling 1992; Williams and Edge 1996; McLoughlin 1999; Koch 2000). For most within STS, such a view remains unsatisfactory for the analysis of material artefacts and practices, and particularly if research is concerned with intervention and, for example, exploring the implications of research findings for policy and practice (Radder 1992; Sorensen and Williams 2002).
In analysing industrial IT applications, we likewise argue that it is necessary to address the material dimensions of artefacts and their properties. In understanding the design, application and significance of packaged software such as enterprise-wide solutions, we need to attend to the uneven affordances that may be built into the particular structure and content of these complex software systems. This is not to resort to mechanistic materialism or technology determinism, nor to deny the importance of discursive processes. Indeed, as outlined above, there are certainly important processes of rhetorical shaping accompanying the development of particular ‘technologies’, including the rhetorics of suppliers and other involved actors in promoting not only their offerings but also the visions of best practice associated with the technology (Clausen and Williams 1997).
ERP systems would on the face of it seem to be a good candidate for a discourse theoretic account of technology. These and other integrated IT applications are extremely complex socio-technical ensembles (Bijker 1995) encompassing a huge variety of elements (Koch 2000). This complexity is elevated by the fact that they are ‘organisational technologies’ which support and are bound up with a range of practices of organisation members (Smith and Wield 1988). In the case of enterprise systems, this complexity has been doubly augmented, first by the need to address many functions within the organisation and second by the many different types of organisation which the package is designed to cater for.6 They can be described as organisational technologies in two further ways not identified by Smith and Wield.
First, many of their elements comprise codified and encoded formalised representations of organisational practices (from generic models of organisation structure and functions to very detailed models of business processes; coupled when implemented with databases of products, transactions, people and organisations). In addition, in the case of packaged generic software systems, they are supplied as only partly finished products; they only ‘come alive’ as information systems following their implementation and extensive configuration to operate within particular technical and organisational circumstances. As a result of these and related factors, the boundaries of these kinds of integrated organisational technologies remain diffuse and their properties elusive. These are then technologies far removed from the mechanical devices of the first industrial revolution; they are ‘heterogeneous assemblages of human and material elements’ (Koch 2005: 43), thoroughly penetrated by ‘soft’ elements. Koch (2005: 43-4) conceives ERP as ‘communities of software companies, customers, professional associations, different
kinds of hardware and software, implementation procedures, practices and rhetoric spanning time and space’.
Despite these factors, our insistence on addressing the materiality remains, impelled by our general understanding and by a number of reasons more particular to this study, including (inter alia):
1	because user organisations must engage in detail with the technical operation of these artefacts to get them to work in their technical and social setting;
2	because organisational actors find there are uneven levels of malleability of the overall technological configuration and of the particular elements of packaged software solutions. Prior choices in the earlier development of these packages largely fix certain elements, for example regarding their overall architecture and specific features, reflecting the design and development choices bundled into the offerings of various suppliers (as evinced by Koch’s [1997] concept that these may be ‘bricks’ or ‘clay’ in the hands of local actors). Some elements are designed to be readily reconfigured to meet user requirements; other elements may be potentially reconfigurable, but the effort of so doing tends to favour the use of default variables (the ‘power of default’); core component of the software suites would be very difficult for individual user organisations to alter (and the vendors will typically limit customer interventions in those parts of the programme that might inhibit its reliability and maintainability). As Koch (2005: 44) states: ‘The systems are not solely shapable clay, rather they are heterogeneous materiality composed with abstract discourse-elements with a certain hardness’;7
3	because different supply and procurement strategies are accompanied by different and very uneven levels of local malleability. This reflects features of the institutional supply structure and the character of technological configurations and varies, for example, between user organisations resorting to simpler locally supplied products, building best of breed solutions, or adopting standardised packaged solutions (Clausen and Koch 1999).
Choices in package design and development thus impinge significantly upon the implementation process and the organisational outcomes of new organisational technologies - and user organisations correspondingly approach the choice of software with great care. This brings us on to a central concern of the chapter - regarding how to theorise the processes of decision-making over technology adoption.
Theorising the processes of decision-making over technology adoption
The widespread recent shift from ‘make’ to ‘buy’ for new workplace technologies, with technology design and procurement/implementation
organisationally separated, has resulted in a shift in focus, with the key adoption decisions being increasingly centred on procurement: that is, the selection and implementation of externally developed (often packaged) solutions. In the case of complex non-material artefacts such as ERP and other packaged organisational technologies, the selection and comparative assessment of supplier offerings presents particular challenges. It remains extremely hard to assess their properties as these, as already noted, cannot be readily disclosed by inspection but are only finally verified in their organisational implementation and use. Indeed, as MacKenzie (1990) shows, even in the retrospective assessment of a technology through empirical testing, there is the possibility of claimed performance being challenged. No test is incontestable - though some assessments may be accepted without question where there is no constituency minded or in a position to contest that claim. The challenges are much greater in the case of prior assessment of novel technological products, where the system is not yet in existence and selectors are forced to rely on other indirect evidence. The resort to packaged software reduces uncertainties in some respects, compared to bespoke solutions, since it does allow retrospective assessment of ‘the artefact’ to the extent that it has been implemented and used elsewhere. In addition, as we will see in Chapter 6, many adopters place particular weight on seeing systems in use. However, even in this case, uncertainties remain about this kind of argument, notably regarding:
1	whether the software being used in these ‘reference sites’ is identical (or rather sufficiently similar) to what is being offered; and
2	whether the reference site or demonstrator organisation is sufficiently similar to the prospective adopter in its structure and working methods. This point is of particular importance given the idiosyncrasy of user organisations and the consequent importance of the ‘fit’ between generic software solutions and the unique features of the adopting organisation.
Software engineering and management texts propose formalised assessment methods that hold out the promise of selection being made on technical-rational lines. However, as we saw in Chapter 1, there is a gulf between actual procurement practice and the typical broad presumption of these methodologies that user organisations already have a complete list of their requirements that can be simply matched against the verifiable properties of software packages. Instead, user organisations need to reach some kind of trade-off between the strengths and weaknesses of vendor offerings in meeting different, imperfectly expressed and incommensurable user requirements. The advance assessment of supplier offerings typically takes place under circumstances of incomplete knowledge and, though a variety of proposed selection methodologies have been proposed, the absence of established and verified models. Assessments are always, under such circumstances, potentially contestable, given the likelihood that the different
groups involved are advancing different priorities and differing in their commitments, and the expertise, methodologies and criteria that they deploy.
Importantly, however, in advancing this argument we are not suggesting that ‘anything goes’. In this book, we intend to move away from a reading of the constructivist metaphor that these criteria are simply arbitrary ‘fabrications’, the content of which does not matter. Our aim, instead, is to produce a serious analysis of procurement, which addresses the means by which assessment criteria are constructed in the process. By contrast, our preference is to develop a model that pays attention to the understandings that the actors involved develop of the situation. Our informants would not relate to a model which portrayed technology decision-making as a simple political model, where there is the naked exercise of power of certain key players in the adopting organisation, or that the selection results simply from the success of vendors and consultants in ‘selling’ their solutions. At worst such analyses tend to depict the adopters as ‘pawns’, ‘dupes’ or ‘empty vessels’. Moreover, at best, they overlook the often-strenuous efforts made by organisation members and others involved in carrying out assessment, and thus fail to acknowledge that procurement does involve a process of critical examination. Decisions are subject to particular forms of verification and accountability, which are likely to be particularly elaborate in the case of major high profile investments (such as enterprise system procurement).
This is not to deny that there might be cases where simple interests prevailed, and where verification was negligible. In our earlier research into CAPM adoption, we encountered a case in which the Managing Director made a (flawed) procurement decision in a car park, allegedly the worse for wear after an extended ‘liquid lunch’ with the vendors. Conversely, in paying attention to the hard work that may be involved in procurement decisions, we do not suggest that it somehow manages to become a neutral technical process. The process must also meet other tests, including key tests of organisational politics and do-ability, which may be more or less in tension. As a result, procurement and other decision-making processes include important symbolic elements of ritual, ceremony (Tingling and Parent 2004) and, as we shall see in Chapter 6, of drama. By addressing both aspects of the decisionmaking process we aim for a richer (or more ‘holistic’ [ibid.]) understanding. Crucially our argument is that we need to be able to distinguish different settings and processes for technological decision-making: between those in which ad hoc political contingencies are more evident and those where formalised criteria for assessment are in place.
Our enquiry seeks to address the ‘grey spaces’ in which technology decision-making occurs, between the technical rationality and organisational politics accounts. We see decision-making as subject to diverse incentives and forms of verification (and differing in these respects from one case to the next). Formal assessment methods are increasingly being applied. However, these have not yet been sufficiently technified, routinised or institutionalised for them to appear to work in an apparently objective technically rational
manner. There are obvious opportunities here to draw from the work of Callon (1998, 1999, 2007) and MacKenzie (2003, 2006a, 2006b) in the field of Economic Sociology. Here they have been exploring the work needed for economic and financial markets to operate. In doing so, they highlight the processes of ‘framing and disentanglement’, whereby objects and their properties are stabilised and transactions may be separated from their particular unique and local social settings. The settings that we are examining, of packaged software procurement, are at the other end of the spectrum from the international commodity markets that Callon and MacKenzie have addressed. And in Chapters 6 and 7 we use these concepts to help us explore the work done in packaged software procurement in making a comparison between various vendor offerings.
Contributions from other disciplines
In the remainder of this chapter we critically explore some of the main forms of specialised and professional knowledge concerning the industrial application of technologies, focusing in particular upon Computer Science and Economics, as well as Management where the same set of criticisms can also be applied.
We review the social scientific critiques that can be made of this work, drawing upon the Sociology of Organisations and in particular our own specialist domain of STS. A central concern of the latter has been to interrogate the claims of mainstream approaches from the specialised engineering, economic and managerial writings to be able to subject new technologies to a calculative logic. The sociological writings emphasise the contestable and contested nature of the criteria used to assess technology, which emerge through the activities of individuals and groups seeking to advance their positions within particular settings. While we may be critical of the ways in which these other disciplinary approaches have approached the topic of technology, they do offer analytical tools which have distinctive strengths and which may assist us in our enquiry. Thus MacKenzie (1992) has noted the contrasting respective strengths of Economics, in modelling the aggregate outcomes of highly regularised actions (for example, in the operation of a market), and Sociology, in analysing specific contexts of individual action, particularly in innovative and changing settings where the parameters of action are not well established and actions and outcomes are consequently highly contingent. The focus of (Ethnographic) Sociology on process and detail is at a cost in terms of its ability to make generalisation; whereas Economics makes assumptions that Sociology finds unrealistic.
The economic account
Mainstream Economics has been very successful in projecting the scientific basis of its knowledge claims. The widespread shift to outsourcing
technology supply and the extraordinary success of packaged solutions for enterprise-wide applications would seem to reinforce its presumptions about the efficiency of market mechanisms. Fleck (1988b), however, has disputed the implied effectiveness of the market mechanism for advanced industrial technologies (like enterprise systems) specifically questioning the neo-classical economists’ view that the market can be relied upon to deliver effective solutions. He points out that the impersonal ‘pure’ market posited by economists constitutes a rather poor mechanism for information exchange between vendors and would-be adopters. User organisations buy packaged organisational software systems infrequently, giving them little opportunity to assess the products and their suppliers. They lack the expertise and knowledge needed to make effective choices of such complex products (Fleck et al. 1990).
Difficulties in assessing technology: transaction cost-based explanations
Market provision thus presents particular problems for the procurement of complex informational goods such as complex organisational technologies. Williamson’s (1975) seminal analysis of make-buy choices would suggest that transaction costs are liable to be high with such software acquisition -because of the high levels of uncertainty experienced by the would-be user about the performance of the vendor and its products. This asymmetrical distribution of knowledge between supplier and purchaser is seen as liable to encourage ‘opportunistic behaviour’ by suppliers, for example, by overcharging for their products. With bespoke software products, which do not exist at the time of the procurement decision, the purchaser is forced to depend upon indirect measures of the capacity and behaviour of suppliers and their wares (such as the supplier’s reputation). Even with packaged software, as we have said, though the generic package already exists, uncertainty remains not only about the product features but also more particularly about their fit with particular user organisations. The Transaction Cost Economic (TCE) account has gained considerable import in the field and merits more detailed examination. In particular, its explanatory framework appears to provide a basis for reasoning about different procurement settings. It is moreover, in principle, amenable to empirical examination.
To briefly summarise: TCE theory builds upon Coase’s (1937) theory of the firm, which seeks to explain why capitalist economies include ‘islands of conscious control’ (firms or hierarchies) as well as self-directing markets. Williamson (1975, 1985a) addresses the choice facing a firm between buying in a product and making it itself, starting from the assumption that this ‘make/buy’ choice is driven by cost reduction. He further assumes that economies of scale in production normally favour market-based provision and seeks to explain deviations from this.8 He then analyses the circumstances in which the costs and difficulties associated with market transactions favour in-house production (hierarchy) rather than markets as an economic
governance structure, whereby the elevated costs of coordinating (transaction costs) market provision outweigh the saving in production costs.
Transaction costs comprise:
1	the search and information costs of finding out what goods are available within the market and identifying the lowest price;
2	the bargaining costs required to agree a transaction and set up and contract; and,
3	the policing and enforcement costs of monitoring the other side’s behaviour and making sure it complies with the contract. Transactions are seen to vary, depending upon the frequency of transactions (firms will not normally set up in-house facilities for infrequent transactions), uncertainty about the behaviour of other players in a context of bounded rationality and ‘asset specificity (for example, where technological knowledge and/or equipment are dedicated to particular processes or customers and cannot therefore be utilised elsewhere, impeding production economies).9
Williamson (1975) applies these variables to map out the circumstances which are liable to increase market transaction costs and favour integration of the activity within the firm (hierarchy) including, crucially for the analysis of the procurement of complex technologies and informational products, the ‘asymmetry of information’ distribution between parties to the transaction. Williamson thus draws attention to a condition of ‘information impacted-ness’ between players in procurement, where the inability of one player to scrutinise the process may encourage opportunistic behaviour (defined as ‘self-interest seeking with guile’, for example by charging too much for a product). Information asymmetries can lead to adverse selection (possible misrepresentation leading to selection of a partner not able to provide the requisite capabilities) or moral hazard (whether or not the partner is willing to provide the requisite resources and capabilities). Informational products present a particular problem since the buyer does not know the value of the good before she has purchased it (Williamson 1985a, 1985b) (a condition which, for reasons described above, also applies in relation to the procurement of organisational software packages). These elements of TCE make it of particular interest for the study of technological innovation and knowledge.
Attempts have been made to apply Williamson’s TCE analysis to the procurement of ERP and other organisational software solutions. Various sympathetic explorations report difficulties in interpreting TCE (Lacity and Willcocks 1996), and highlight anomalies that TCE models do not address (ibid.; Dibbern et al. 2004; Heiskanen et al. 2000). Thus, they question whether the single transaction is the appropriate unit of analysis since, for example, the contractor still needs to consider the effects of its actions on losing future business from either this client or other clients (Lacity and Willcocks 1996; Wang 2002). Their empirical research suggests that TCE offers too simplistic a theory of human behaviour (Wang 2002), highlighting
the multiplicity of motives at play (drawing attention, for example to differences between the goals of individuals and groups within the organisation [Lacity and Willcocks 1996]).
A further issue surrounding the efficient market provision of information systems is the matter that has been described as ‘incomplete contracting’. To reduce policing costs, effective monitoring of the contract calls for strict specification of the customer’s requirements at the outset, which can be embedded in the contract and its fulfilment policed. In reality, however, as we have already seen, in organisational system procurement, whether as a custom or packaged solution, the user organisation only has an imperfect initial understanding of its own requirements, which invariably change as the would-be user develops its understanding (partly in reaction to the capabilities of the package). Conversely arm’s-length supply often breaks down as implementation problems frequently force supplier and user to collaborate, whether such collaboration was planned or not, to get the solution to work (Fleck 1988a, 1988b; Brady et al. 1992). This opens the door for opportunistic behaviour by suppliers. Such ‘incomplete contracting issues’ in outsourcing IT impede market provision (Richmond et al. 1992; Wang 2002). On the other hand, as Wang (2002) points out, if we look beyond the individual transaction, we find that whether suppliers will behave in such a way is moderated by the potential impact upon their reputations. We return to this shortly.
Economic modelling of software procurement from a TCE perspective notes a strong predisposition towards internal development (Nelson et al. 1996; Richmond et al. 1992), suggesting that packaged solutions will prevail only in the case of more basic, common technologies (Nelson et al. 1996), with outsourcing on a customised basis for advanced and specialised applications. Interestingly this work also failed to anticipate the ERP explosion that was already under way!
Fincham et al. (1994) and Scarbrough (1995) offer a more critical assessment of TCE, calling into question Williamson’s counter-position of markets and hierarchies as alternative forms of governance, stressing:
1	the continuum between economic (exchange) relations and social (control) relations and, more important,
2	the complementarity between market relations and social networks. Fincham et al. (1994) highlighted the role of linkages between occupational specialists with their peers (for instance, in the Scottish financial services sector) which provided informal channels for the exchange of information about the provenance of new technological offering and their vendors, their experience when implemented within similar organisations - all of which underpinned the market-based procurement of packaged solutions. As Scarbrough (1995: 1015) concludes, ‘Transactions do not allocate themselves to institutional forms on the basis of efficient fit...the presumed economic closure around TC efficiency is frustrated by
dependency on more or less diffuse forms of social control.’ TCE theorists have taken the latter criticism on board and have suggested that a third, hybrid form of relationship exists (Williamson 1991; Larson 1992), though Williamson (1991) insists that these models constitute mutually exclusive alternatives.
As we see, TCE has led to increasing attention to the role of trust and reputation as factors inhibiting opportunistic behaviour by suppliers, providing an incentive against moral hazard by suppliers and in overcoming adverse selection by providing an indirect indicator of supplier capabilities and performance. It is of particular importance in relation to the provision of informational products whose utility is very hard for the adopter to assess (Fincham et al. 1994; Wang 2002). An early paper discussing the economics of generic packaged software (Milne and Weber 1983) highlighted the possibility of consultant service providers investing in and developing a longterm reputation for expertise and reliable behaviour in assessment (and thereby able to charge more for their services).
Mowery (1996) observes that the need to establish a reputation through long-term relationships with customers constitutes an important barrier to entry in the custom application software market, supplemented in the case of packaged software by the need also to create marketing and distribution networks. Reputation can be considered as a form of irreversible investment, which is built up over time, often at great cost (Wang 2002). Clausen and Koch (1999) observed that user organisations adopting ERP (especially SMEs) tended to stick with existing suppliers, and attributed this to their hope that the high-trust long-term relationship so generated would reduce the risk of supplier opportunism, providing both a source of internal reputational measures for the user firm and an incentive for the supplier to behave well. Fincham et al. (1994) pointed to the importance of professional networks in exchanging experience between potential ‘user’ organisations about the availability and suitability of new technologies and their providers.
A more recent paper of interest is Gluckler and Armbruster (2003) who explore three different kinds of reputational knowledge. First, they highlight the role of experience-based trust in moderating behaviour of external providers (in this case management consultants) and in providing information about their capabilities. They note that it is difficult to acquire and maintain this kind of experience/knowledge, and is therefore limited in its coverage, drawn from a few highly trusted sources, and thus uncompetitively limiting the organisation’s range of potential partners. Second, they point to public reputation that is at the other end of the spectrum, but since it depends upon multiple sources of mixed and indeterminate reliability and emerges only slowly, it has limitations. Finally, they highlight an intermediate form, networked reputation, which balances the respective drawbacks of these previous information sources, allowing access to a wider range of sources, though at a trade-off in terms of the confidence that can be invested in those
sources. Networked reputation provides a way of assessing suppliers of informational products and underpins the importance of social networks in sustaining market-based transactions. Gluckler and Armbruster (2003: 291) call for more research into the operation of the mechanisms of networked reputation and the informal social institutions that support economic exchange. This is an area we return to in Chapter 7, where we explore the role of industry analysts such as Gartner in making user community experience available on a commodified basis.
The most profound criticism of TCE concerns its exclusive attention to reducing transaction costs and its consequent failure to address processes of inter-organisational learning. In particular, it overlooks the role of learning between players in the supply chain (Lundvall 1985). Scarbrough (1995) notes that Williamson sees knowledge and information flows between supply-chain players only in terms of their impact on transaction costs and ignores its contribution to innovation. Lundvall (1985) offers a more extensive economic critique of TCE. He examined the emergence of what he described as organised capitalism, characterised not by the short-term market relations that would be predicted by TCE and mainstream neo-classical Economics, but by long-term stable relationships between supply-chain players. These linkages need to be explained in terms of the benefits from interactive learning between users and producers rather than reducing transaction costs (Lundvall 1985, 2005). This argument underpins the subsequent articulation of an alternative ‘Evolutionary Economic’ account of innovation which highlights the contribution of ‘supplier-user coupling’ in what is described as a ‘learning economy’ (Freeman 1974; Dosi et al. 1988; Lundvall 1992).
The Evolutionary Economic account is one that we have much sympathy with, and it has been one of the factors influencing the emergence of our own social learning perspective (Williams et al. 2005). Though there is not space here to provide a detailed account or critique of Evolutionary Economics, it is useful to stress our main point of difference, which is that, as an economic theory, what it seeks to explain is the aggregate efficiency of particular innovation systems (e.g. depending on the strength of supplieruser coupling). It therefore fails to pay sufficient attention to the detailed operation of the learning economy. By this, we mean the specific processes and mechanisms of knowledge transfer and the consequences these may have for technology choices and their societal outcomes.
We have used the example of TCE to examine the explanatory claims of economic theory.10 We conclude that TCE has provided tools to think more formally about the systemic interaction of different kinds of factor variables. The excursion has been of interest to us - not least because this kind of formalisation has become weak in Sociology and sociologically inspired STS, with its recent widespread shift towards processual explanation and qualitative analysis. TCE provides a simplifying frame that brings with it some prospect of generalisability. However, it does this at a cost - most notably of
a loss in terms of the richness and realism that qualitative research approaches can yield. In particular, we note the unhelpful boundaries many economists have drawn between markets and firms (whereas our work sees these as part of a continuum and further stresses the necessity for network relationships for markets to operate effectively) and their neglect of the broader context within and through which markets are constituted.
Comments on the uneven state of knowledge on ERP
When we review the development of systematic knowledge about the development and adoption of packaged software and organisational technologies, we notice different kinds of barriers to developing an overall understanding. As well as the unhelpful segregation of knowledge between different academic disciplines, we also note the fragmented and uneven development of knowledge, particularly the fragmentation of research between different kinds of study; of technology development and of implementation, and the uneven development and different level of attention devoted to each of these.
The first element of unevenness, however, is the delayed appearance of research literature relative to technological practice. Mabert et al. (2001) note that the research literature appears slowly, in contrast to the dynamism of ERP vendors and adopters in development and uptake (Mabert et al. 2001; Grabot and Botta-Genoulaz 2005). However, the identification of complex enterprise systems as a focus of industrial and academic interest heralds an explosion in studies of ERP implementation. Huge numbers of implementation studies have now been amassed. Though the first generation of consultants’ reports gave largely uncritical coverage about the business outcomes these systems would deliver (e.g. through case studies of successful adoption), this work, as already noted, largely lacks credibility. Following widely publicised cases in which ERP had been judged to have failed, or failed to deliver expected organisational improvements, a large part of this kind of work subsequently comes to bear upon identifying what are described as ‘critical success factors’ behind successful implementation. Much of this work lacks the theoretical framework and tools that might be needed for rigorous analysis (Motwani et al. 2005). This work is notable also for its almost complete lack of attention to technology supply - despite the importance of the design and configuration of the artefact for implementation outcomes. Even such crucial implementation issues as package selection and procurement are largely glossed over.
We find a marked lack of research into vendors and into the relationship between the vendors and the customer (Dibbern et al. 2004), and almost no research which addresses technology development and implementation in tandem (the most important exception here being the work of Clausen and Koch and from current and past Edinburgh scholars - D’Adderio, Fleck, Pollock, Stewart, Webster and Williams - working within a common Technology Studies perspective).
This uneven development and fragmentation of research between studies addressing the development of packaged software and its implementation also reflects disciplinary divisions. Thus, the huge literature on ERP implementation derives especially from Business and Organisational Studies and from the related discipline of Information Systems (Dibbern et al. 2004). There exist at the same time a small number of studies of software package development from the Computer Sciences, Economics and Technology Studies. The particular concerns and framings of issues articulated by these disciplines set boundaries on the way in which a topic like packaged software is addressed. Scarbrough (1995) highlights that these disciplinary demarcations pose a major barrier to the analysis of innovation processes. It is noteworthy that the work which seeks a broader and more integrated understanding (e.g. spanning development and implementation and use as well as broader contexts and longer timeframes of analyses) emerges primarily from Technology Studies, with its strongly interdisciplinary emphasis.
Disciplinary specialisation may be effective in producing more detailed analyses of particular dimensions of innovation. However, at the same time the elaboration of ever more differentiated fields of specialist knowledge generates new problems of cross-disciplinary integration and communication. These difficulties are reflected in the widespread and growing calls for interdisciplinarity in research. Nevertheless, questions remain about how this integration is to be achieved (Weingart and Stehr 2000). Knowledge domains are not like building blocks, to be simply plugged together. The various disciplines have advanced their own, more or less distinctive methodologies, concepts and tools. These are built around (tacit) founding assumptions and narrative structures (Jeffrey 2003), which are often incompatible one with another, and which even shape the view of the empirical object of study. Thus Klaus et al. (2000) draw attention to differences in view as to what is comprised by ERP between Computer Sciences, Organisational Theory and Economics.
The relatively new fields of Information Systems and Technology Studies both emerge with a cross-disciplinary endeavour. There are also important connections between schools of thought across cognate disciplines. Indeed, there is a rather clear web of connections between constructivist analyses across STS, Organisation Studies and critical perspectives in Business Studies, Economic Sociology and Information Systems, as evinced by the observation that Bruno Latour is the most cited social theorist in a survey of Information Systems literature (Ho and Tan 2004). Moreover, engagement with a particular setting or issue can encourage scholars to interact with others from different disciplines and schools in the struggle to reach a more adequate understanding. Dibbern et al. (2004) point to the evolution of the ERP research literature from single disciplinary to multi-disciplinary approaches (with the exception, they note, of mathematical models which, they complain, do not refer to other literatures and make assumptions that are far removed from reality).
The Computer Sciences
Examination of the ERP research literature points to a surprising and disturbing gulf between academic Computer Science research and Computer Science as practised. The former, to the extent that it addresses the development of IT applications, is almost exclusively concerned with custom-built solutions. This is in stark contrast to the industrial application of computers, of course, which has become increasingly, and today overwhelmingly, concerned with the selection and implementation of packaged solutions. There is remarkably little academic Computer Science research literature on ERP and packaged software more generally. The neglect of packaged software is all the more surprising since this method of deploying IT represents a radical change in the character of technical expertise and technical, occupational and organisational division of labour in relation to both technology development and its implementation. One reason is that fundamental Computer Science research has been understandably preoccupied with creating the tools and knowledge needed to build the next generation of infrastructures and component technologies, rather than install and maintain the current generation systems. We can also understand this gulf in part as a reflection of a broader divide within Computer Science between the formalists and theorists (who conceived the idea of computing machines) and those who pursued a more practical, engineering approach, and the subsequent development of a range of disciplines within the Computer Sciences (Pelaez 1988). However, this disciplinary ordering has unhelpful consequences; it creates a tacit knowledge hierarchy within computing - with the more formal mathematically based disciplines at the top and the more practically oriented topics at the bottom. In addition, the new disciplines that have emerged, such as that of Information Systems, which can be seen in part to have emerged in an attempt to overcome this gap in Computer Sciences, would be seen by the theorists to reside at the bottom of this hierarchy. This disciplinary division and its assumed knowledge hierarchy are contested by other groups that propose a more interdisciplinary understanding of the domain of computer sciences (notably Information Systems and the emerging field of Social Informatics).
As a consequence of the relative neglect of COTS within Computer Science, when we examine the techniques of computer system development we find that these are tacitly conceptualised around custom build which has, of course, been the mode of technology provision for most of its short history. Thus Sawyer (2001: 99) suggests that the waterfall model, which represents perhaps the central historical model for computer system development, can be seen to embody the implicit assumption of a make-to-build process rather than packaged software procurement in two ways: that information systems development ‘takes place within one organization (or, at least, is controlled by that organization when hiring contractors and consultants to build a custom product), thus reflecting vertical integration (a hierarchy); and that it is focused on building, not buying, software’. Ahituv
et al. (2002) note that this observation also applies to other more recent development methodologies (such as iterative ones that pursue a closer relationship with organisation members).
Similarly, various authors (Finkelstein et al. 1996; Deifel 1999; Carmel and Becker 1995) note that the discipline of Requirements Engineering (RE) is largely devoted to helping organisational users develop purpose-built software rather than elicit requirements for the design or procurement of packages. There are only a very few articles on requirements engineering for COTS, and these emphasise the sharp differences between packages and custom build. They further highlight the challenges requirements engineering needs to address: on the one hand, for software package vendors to move to a market-driven requirements engineering process (Deifel 1999; Xu and Brinkkemper 2005) and, on the other, for adopting organisations to move from a development perspective to a procurement perspective (Alves and Finkelstein 2002).
The idea of supplying software as standard commodified packages would appear to be challenging because it tears apart the previously ‘integrated’ bespoke system development process. This process was integrated in two ways: initially insofar as requirements engineering (whatever techniques for involving user organisation members were adopted) took place in more or less close relationship to various user organisation members; and, ultimately, in that the system would eventually be implemented within that organisation and used by specific organisational users. In contrast, developing packaged software involves the increased separation of users from developers by both organisational and market boundaries (Sawyer 2001). Indeed, there are two distinct RE processes, one underpinning the design of the generic solution, and the other around the selection and implementation of the package by the user organisation. We consider these in turn.
Requirements engineering for software package producers
Some requirements engineering writings portray the uncoupling between supplier and individual customers in the shift from bespoke to COTS software production in terms of the freedom to manoeuvre of the COTS supplier. Thus Regnell et al. (2001: 51) describe market-driven requirements engineering:
where requirements are invented for packaged software offered ‘off-the-shelf ’ to a mass market. This situation is in many respects different from the contract situation, where a developer interacts with a specific customer to elicit requirements for a bespoke system...[Unlike the situation with bespoke software] the developer decides about the requirements and selects the stakeholder representatives. The developer bears all financial risks and there is no single customer who is the principal stakeholder as with bespoke software development.
The idea that the market is driving development and that requirements are invented conveys a sense of the autonomous developer, and that generic software development is much like other forms of product innovation. Requirements engineering here is portrayed as something like market research about essentially anonymous users. An alternative account emphasises some elements of continuity between COTS and bespoke software development as well as differences (Keil and Carmel 1995). Given the importance placed in conventional systems development on early and close links between developers and organisational users, they examine instead what mechanisms are used by COTS developers to capture and represent user requirements and how this affects the development process (Sawyer 2000, 2001). Studies of software package developers reveal that they utilise a wide range of sources of information from third parties and intermediaries such as consultants, as well as direct and indirect links with their customers, and that direct customer links are no longer the most important links. Some early analysts complain that requirements engineering for package production is still a craft process in contrast to its ‘industrial’ development for custom software (Carmel and Becker 1995). In sharp contrast, Deifel (1999) presents a model of package systems development derived from industrial practice that is extraordinarily elaborate and sophisticated. Deifel proposes a processual model that distinguishes different functions surrounding the development - divided between sales-oriented roles (utilising links with users and other sources to develop a market view about the functionality required) and development-oriented roles which look to maintaining a robust architecture and a road-map and strategic view of where it is developing. It would seem that academic models (as articulated, for example, by Carmel and Becker [1995] or by Regnell et al. [2001]) lag far behind industrial practice evolved by vendors in the face of the exigencies of building packaged solutions (as captured by Deifel [1999]).
Requirements engineering (RE) for software package users
The ideal presumed by RE for bespoke solutions, that requirements engineering should be a bottom-up process conducted at the start of a development and driven by users, is not compatible with the procurement of complex organisational technologies such as COTS. Though the literature on requirements engineering for package adoption is minute compared to that for bespoke (Finkelstein et al. 1996), it does address the challenges involved. First, it argues that the range of potential requirements and capabilities is so large that it is impossible to specify them completely in advance and there is never an exact match between package features and user needs. Instead, a complex process of matching is needed, in which the adopter must consider trade-offs between the ways that contending vendor offerings meet their particular priorities, and the likelihood that user requirements may evolve in the course of implementation. This favours an iterative process (Finkelstein
et al. 1996; McKinney 1999). Finkelstein et al. (1996) note that many of the RE methodologies proposed for COTS selection are clearly too narrow and simplistic for the task (for example, Franch and Carvallo [2003]). A number of contributions offer a richer, interdisciplinary and above all experiencebased understanding that acknowledges that COTS selection involves choices between many incommensurable elements, and between features where different players will have differing perceptions and priorities and will include many criteria based on complex judgements, for example about future vendor behaviour (Finkelstein et al. 1996; McKinney 1999; Alves and Finkelstein 2002).
The Computer Science encompass a range of disciplines and approaches. We have only attempted a very limited examination of this field - by exploring only how it addresses (or, rather, largely fails to address) the explosion of packaged software for industrial IT applications. Though we have only been able to investigate the domain through this narrow keyhole, it does reveal a rather uneven set of approaches encompassing both narrow technical academic understandings (exemplified through the espousal of tools and methodologies that seem simplistic, with little scope for application), as well as more robust experience-based approaches. The latter are notable for their richer, interdisciplinary and socio-technical understanding. From our viewpoint, of trying to build a comprehensive understanding of the processes of technological change, Computer Science contributions thus offer an uneven resource.
Processes of disciplinary formation and ordering within the Computer Sciences seem to have unhelpfully divided formal theoretical approaches and practical knowledge. However, there are competing tendencies within this broad field, including attempts by some to broaden out the scope of Computer Science into a more encompassing understanding of the collection and organisation of information (perhaps under the heading of Informatics), as well as efforts to apply interdisciplinary approaches to some of the key problems in computing (for example, design for dependability and usability).11
STS search for analytical models and methodologies
Having completed this critical examination of how other analytical traditions have addressed the relationship between technology and society, it is time to examine, if only briefly, some of the main critical debates that have arisen within our own field of STS and their implications for our current enquiry. STS has from the outset been concerned to develop adequate analytical models. These models serve both to identify the key relationships between technology and society and to provide methodological guidelines as to the effective conduct of research. They have been one of the major foci of debate within STS.
The establishment of STS as a field of systematic study in the 1980s was accompanied by the articulation of a variety of models for analysis of
innovation derived from different traditions and involving differing templates for research (Williams and Edge 1996). Perhaps the most distinctive and influential of these, Actor Network Theory (ANT), offered a critique of existing social science theory which tended to explain social outcomes in terms of the operation of power or interests rooted in existing social structures (for example, of power or markets) (Callon and Law 1982). ANT, by contrast, insisted on explaining these outcomes solely in terms of the success of innovation actors in enrolling others to support their project through various strategies. These strategies included problematisation, the definition of problems in a way that the primary actors establish themselves as the source of solutions; and interessement, where they convince others that their interests are met by aligning with their project. In this view, power is thus not a cause but an outcome of the success of actors in enrolling and mobilising others.
This actor-centred explanation espoused a simple methodological rule: in place of examining the world on the grounds of a priori theoretical categories and concepts, researchers should ‘follow the actor’ to use Latour’s (1987) now much-repeated term. We would argue that this nostrum has proved inadequate, however. For a start, every research design involves choices about where to address research effort, and tacitly also therefore choices about which black boxes to open for detailed examination and which to leave unexplored. Another obvious question ensues - which actors to follow. As Sorensen and Levold (1992) noted, a plurality of accounts would be possible depending upon which actor(s) were placed at centre-stage, leaving the researcher lost in a morass of possible accounts. ANT’s scepticism towards existing social science theory, on the grounds that it imported unwarranted assumptions and generalisations, though justified in the interests of naturalism and intellectual openness, paradoxically leaves the research processes and choices inherent therein underspecified and unaccountable. ANT and Pinch and Bijker’s (1984) closely associated Social Construction of Technology (SCOT) model have been criticised for their focus on local actors directly involved in innovation and for leaving unaddressed the influence of some broader structures (for example, of class and gender which underpin sharp inequalities in access to resources) (Russell 1986; Russell and Williams 1988).
This theoretical debate has, in turn, underpinned much of the subsequent conceptual development within STS. Later SCOT and ANT work appears to respond to some of these criticisms (see, for example, Law [1991]; Bijker and Law [1992]; and more recently Latour [2005]). Bijker (1995) for example introduces the concepts of socio-technological ensemble to describe how the local arrays of involved actors (the Relevant Social Groups flagged in Pinch and Bijker’s [1984] seminal account of their SCOT perspective) are bound up in complex configurations of artefacts, technological frames and relations of power.12 This is an area of ‘creative tension’ and there are signs of theoretical convergence with the competing Social Shaping of Technology perspective
and related accounts which, giving greater attention to broader social relations, seek also to explain the fine structure of innovation processes (Williams and Edge 1996; Williams 2002).
Social Shaping of Technology Mark II
The subsequent two decades have seen a flowering of conceptual frameworks in Technology Studies - inspired by both a growing body of empirical analysis and by these and related theoretical debates (Russell and Williams 2002a). These have led to a progressive reworking of the Social Shaping of Technology perspective from MacKenzie and Wajcman’s original (1985) account. This drew on what was perhaps the classic instance of social shaping, David Noble’s (1984) analysis of the promotion of numerical control machine tools and the suppression of an alternative technology of ‘record playback’, inspired by an explicit goal to reduce managerial dependence on the skills of craft machinists, shared across a consortium of aerospace component manufacturers, machine-tool builders and MIT engineers. This case, with its simple array of actors aligned around a technology choice, does not provide a good template for the complex arrays of actors and settings that we find to be important when, for example, we analyse how the societal context impinges upon the development and implementation of industrial IT applications today. Technologies are more complex - and comprise multiple elements developing in parallel with their particular histories and dynamics. In addition, we see innovation processes as extending beyond the development laboratory to the wider social settings of technology implementation and use. We recognise that innovation is increasingly distributed along a chain of players in the technology supply chain, and analysis has focused upon the role of diverse intermediaries coupling technology supply and use, which may span chains of intermediate, as well as final, producers and consumers.
This is not just a feature of the character of complex modern technologies: it also reflects a recognition that social shaping processes are occurring across multiple locales and timeframes.
Innovation does not stop with initial research and development but extends over the cycle through implementation to use and continues over multiple product cycles. This means the future of a technology is being tested, contested and worked out at a number of different levels and, crucially, at different historical tempi. This flags the need to theorise and examine empirically the various broader contexts as well as immediate contexts of action (Russell and Williams 2002a). The new conceptual schema emerging around the Social Shaping of Technology today, constituting what we might call SST Mark II, offers a variety of approaches to this challenge (see, for example, Sorensen and Williams 2002).
Here, ANT has made important critiques of mechanistic structuralist explanation and attempts to account for the world by applying conventional social science readings of society as operating at micro-, meso- and macro
levels, insisting instead that a single analytical frame be adopted rather than that inferring that micro-meso-macro are different kinds of ‘social stuff’ (Callon and Latour 1981; Knorr-Cetina 1981a). However, addressing the range of timeframes and levels of generalisation that surround social action may require different methodologies and modes of analysis (Russell 1986; Russell and Williams 1988). This has obvious important consequences for how we conduct our research. Instead of applying a single methodological guideline, the research design will necessarily depend upon the phenomenon being investigated and the goals of the study. Pragmatically it will be necessary to ‘foreground’ some elements for detailed examination and ‘background’ others. No single methodology will guarantee correct research design. Instead, we emphasise the need to develop complex methodologies and argue for their adequacy in addressing the problem at hand (Russell and Williams 2002b).
The social learning model
One way in which we have sought to address these issues is through the ‘social learning’ perspective. Social learning has been proposed as an extension to the Social Shaping of Technology perspective but one that takes into account the complex and dispersed processes of learning and struggling as new technological capabilities are adapted to and incorporated within the detailed fabric of social life (Sorensen 1996). Social learning highlights the extended range of actors and locales in which innovation takes place (Williams et al. 2005; Russell and Williams 2002a). Within this social learning process, we highlight two linked cycles - of domestication and appropriation, which are particularly relevant to analysing the design, implementation and use of IT applications.
Fleck’s ‘innofusion’ concept (Fleck 1988b; Fleck et al. 1990) draws attention to the processes of experimentation that take place in the contexts in which supplier offerings are implemented, through the (often unplanned) struggle to get the artefact to work and be productive in a particular (organisational and technological) socio-technical setting. In this process, complex artefacts such as organisational technologies may need to be taken apart, broken down, adapted and reconfigured. Sometimes the technology may be re-invented in its implementation and use. Innofusion thus constitutes a potentially important innovation resource.
Various writers have looked at the processes by which the affordances of artefacts are discovered by users and incorporated into the everyday processes of work and social life. Arrow (1962) highlighted the extended trial-and-error processes of ‘learning by doing’ as organisation members discovered how to utilise machinery more productively. Clark and Staunton (1989) described how industrial technologies needed to be ‘appropriated’ by organisations and their members. Similarly, Sorensen (1994, 1996) draws upon the Silverstone et al. (1992) concept of ‘domestication’ to describe the
work done by users of technology to incorporate artefacts into their practices. In relation to industrial IT applications, the domestication and appropriation concepts have been used to explore how IT artefacts (software, hardware, classification systems) become part of functioning information systems within the organisation, through the efforts of organisation members to incorporate them into their information and coordination practices. Social learning studies reveal how work must be done to get supply-side offerings to work and be useful. Supplier offerings are inevitably generic and unfinished in relation to specificities of practice of particular organisations and groups, and thus require a creative effort by users by developing new routines and practices to get the system to be useful and, through what are known as workarounds (Pollock 2005), make up for its deficiencies.
Sorensen (1996: 6) defines social learning as ‘a combined act of discovery and analysis, of understanding and meaning, and of tinkering and the development of routines’. Though we have made the concept of social learning our own it is with some trepidation that we deploy the terminology of learning, lest this be seen narrowly as a cognitive process, of simply knowledge acquisition and exchange.13 Though these are important elements, we conceive social learning as involving processes of conflict and struggle (following Fleck’s [1988b] analysis of learning by struggling). A central metaphor is that of negotiation with (often obdurate) technical and social elements.
It has become commonplace within Technology Studies to talk of the ‘mutual shaping’ of technology and society. There is, however, a danger that this metaphor could, even unintentionally, convey an impression that this was a smooth and harmonious process (and this criticism could equally be applied to our social learning concept or the Evolutionary Economic concepts of the learning economy and of innovation systems). Social learning research draws attention to the gulfs and schisms that beset the innovation system, to socio-technical distance and misalignments, and to innovation processes marked by frequent setbacks and failures (Williams et al. 2005).
In the case of organisational technologies, the outsourcing of technological development capability, the shift to market supply and the commodification of offerings in the form of packaged solutions radically change the terrain in which social learning occurs. It opens up an organisational, indeed industrial, division of expert labour between supplier and vendor - facilitating specialisation but at the same time fostering a differentiation of perspectives (knowledge, priorities, culture) between developer and user. The social learning research tradition emphasises the importance of tacit, experiencebased and local knowledge and the consequent difficulties of capturing, formalising and applying elsewhere such ‘sticky knowledge’ (von Hippel 1994). It is in this context that social learning theory places special stress upon the role of various kinds of intermediary (some linked to technology development, others to its appropriation), able to bridge between innovation locales - to collate local knowledge and sort it and transform it so that it can be
applied elsewhere. As well as examining formal intermediaries, our studies show that intermediaries often arise at the interstices between organisations as an informal role (raising questions about how this role may be resourced and motivated) (Williams et al. 2005).
When we examine the supply of enterprise-wide systems as packaged solutions, we are struck by the proliferation of different forms of intermediation around ERP design, procurement and implementation:
1	in relation to development intermediaries, we are therefore particularly interested to examine how suppliers develop and exploit diverse knowledge sources and customer linkages to understand their current and potential user base and build their offerings; and
2	in relation to implementation intermediaries, we note the emergence of a complex structure of consultants, implementers, and value-added resellers around the procurement of packaged ERP. Outsourcing innovation and change management has been widely adopted as part of a wider business strategy of the ‘lean organisation’. Offsetting responsibility to commercial sub-contractors in turn throws up new issues for organisations about how to choose and manage this inter-organisational array of players.14
We have already noted the difficulties faced by organisations in the procurement of artefacts and knowledge services, in selecting between multiple supplier offerings in the context of incomplete information about the performance of vendor offerings and their fit to specific organisations. Organisational users acquire experiences of technology procurement and implementation slowly and painfully. Who then will be the carriers for this kind of knowledge? Organisational users seem surprisingly willing, as we shall see later, to act as reference and demonstrator sites and thus to share information with other potential users. Yet it is not in the interests of user organisations to apply this knowledge more broadly. In contrast, the technology-supply side are keen to use this knowledge but may not necessarily do this in an impartial manner. User organisations are aware that suppliers and consultants wish to sell you their products, raising issues of trust and conflicts of interest. Earlier work highlights the role of peer networks in these processes (Newell and Clark 1990; Fincham et al. 1994; Swan and Newell 1995). These issues are the focus for Chapter 7, where we point to the emergence of new kinds of intermediaries in the selection of products.
3	The Biography of Artefacts Framework
Here we describe the approach that we will adopt to study these various technologies. Building on earlier studies that have examined the mutual adaptation of technology and organisation, we develop a framework for investigating the ‘biography’ of software systems. Drawing on work from Science and Technology Studies, Material Culture and Cultural History, among others, we suggest an approach that follows the actual packages themselves as they evolve and mature, progress along their lifecycle, and move across sectoral and organisational boundaries. In this endeavour we address multiple timeframes and locales.
How modes of research frame the analysis
STS has from the earliest days been concerned to resolve the question of adequate models for the analysis of technological innovation and associated societal change - as these frame the analysis and guide the methodology adopted and thereby what it is we can and cannot find out. This project in particular seeks to apply and further develop the biography of technology perspective, which emerged from our earlier work on organisational technologies (Brady et al. 1992; Clausen and Williams 1997; Pollock et al. 2003; Pollock and Cornford 2004). Our aim is to build a comprehensive understanding of the evolution of a technology - encompassing both technology design and implementation/use - and how it is shaped by its specific historical context across multiple social locales.
This research project was designed to exploit the opportunity to achieve insights from a longitudinal contemporaneous study, building upon our earlier research, including studies of Computer-Aided Production Management (CAPM) and other integrated automation systems in the late 1980s and early 1990s that were the progenitors of ERP and other ‘enterprise’ software packages of today (Fleck et al. 1990; Webster and Williams 1993; Fleck 1993; Clausen and Williams 1997). Underpinning this endeavour was an attempt to develop modes of enquiry that might be adequate to explore these complex technologies in terms of both the design of empirical research and the conceptual tools advanced to explore them.
The project was inspired by our unhappiness with the way that most existing research into packaged software and ERP in particular was framed. As we noted in the previous chapters, there is a huge literature addressing ERP and the development and adoption of workplace technologies more generally. This research is often weaker in theoretical and conceptual terms than STS (which only constitutes a small share of this literature), particularly in its understanding of innovation, and is less concerned to consider issues of methodology and epistemology. The bulk of these studies are framed, somewhat unreflexively, within particular well-established modes of research, constrained within particular loci, timeframes, disciplinary perspectives and concerns. Our contention is that the framing of these studies can produce unhelpful readings about the character and implications of these technologies. Moreover, as Grabot and Botta-Genoulaz (2005) observe, particular types of research (e.g. quantitative survey, qualitative case studies) give salience to certain kinds of issues. We need to be in a position to reflect upon the implications of research choices for the outcomes of an investigation.
In this chapter, we first advance a critique of the dominant modes of enquiry into ERP and similar workplace technologies. We then go on to develop a research framework - based on our Biography of Artefacts Framework - that will be more adequate to the task. Here we draw upon research within our own tradition of STS, and also on related work from Organisational Studies and Information Systems perspectives which shares some of our presumptions and concerns (and we note that there is not always a clear dividing line between these disciplines, particularly in relation to work informed by constructivist insights, much of which has been influenced by STS perspectives).
We want to explore how some of the general debates within STS, outlined at the end of the previous chapter, relate to the concept of the biography of artefacts, about what would constitute an adequate analytical framework, equipped to address the multiple interfaces between technological artefacts and society. Studies of particular socially/temporally bounded locales, for example, the typical ‘ERP implementation’ case study or survey are, we contend, ill equipped to get to grips with these complex technologies which are instantiated at multiple sites (Clausen and Williams 1997; Kallinikos 2004b). Koch (2007) suggests that we need better spatial metaphors for addressing such objects. He draws attention to the evolution of perspectives, moving away from single-site studies to multi-locale studies, and has further advanced the suggestion that we should analyse ERP as a ‘community’ (Koch 2005, 2007).1 These suggestions are thoroughly congruent with the broader social learning perspective we outlined in the previous chapter (Sorensen 1996; Williams et al. 2005). The task of this chapter is to chart out an analytical framework for such an endeavour.
Existing studies of technology and work organisation have in general paid inadequate attention to these kinds of debates (which is not to overlook outstanding exceptions to this generalisation and the increasing strength of
social science research in business schools, for example). There is a large amount of unreflexive research, including the many ‘impact studies’, addressing the organisational consequences of particular technologies, which pay little attention to questions of research design and theory. Among academic research, disciplinary divides have served to separate those studying technology supply from its adoption/use and technical from organisational issues.
The success of the organisational case study as a research model within Business Studies (as well as Information Systems and Technology Studies research) valuably focuses attention on local negotiations and choices around the design or the implementation and use of new technologies. Studies of technology and work have benefited greatly from the growing influence of interactionist perspectives (inspired, for example, by the exciting work of authors such as Lucy Suchman [1987]) and an associated enthusiasm for local ethnographic studies, which has been very effective as a research methodology in producing a rich local picture. However, this emphasis on local processes and actors may be at the expense of paying less attention to more generalised and long-term shaping processes.
Our work, however, seeks to explore how these local struggles are taking place within broader circuits of knowledge and influence, including economic and social structures and material structures (and we suggest that a study of technology needs to engage with technology as a materialised institutional form) which mobilise beliefs and visions and provide various incentives, resources and penalties and which thus set the parameters in which local actions are played out. We are also keen to explore how local outcomes may react back on and transform the broader setting, through diffuse and gradual processes of influence, which may not readily be detectable within short-term local studies (Williams 1997).
Within the study of technology, the SST perspective has been marked by its insistence on the need to pay attention:
1	to the specific material characteristics of technological artefacts and systems; and,
2	to the influence of social structure and history which pattern innovation, and which explain the patterns of uneven access to resources and sites of influence.
These considerations inform our search for a research strategy that addresses the multiple locations and different timeframes in which technologies operate. To this end, we examine the utility of various conceptualisations of ‘arena’ (Fleck 1988b; Jorgensen and Sorensen 1999), to explore the hybrid spaces in which different actor-worlds interact, and of an ‘agora’ of technological and organisational change (Kaniadakis 2006), which provides a framework for looking at the relationship between different arenas and levels, and how local actions are set within broader settings. The agora is
conceived in a relational sense; it is a complex space captured through the viewpoints that different actors (and analysts) make of this. Within this framework, we may wish to focus upon local, immediate settings of action or more widely dispersed institutionalised patterns. And this endeavour also suggests that we need to provide a register of the multiple different historical timeframes at play: from the immediate moment of action to the long term in which institutions emerge and evolve.
Before we turn to these questions, we shall first review the different kinds of research that have been carried into technology and work organisation and in particular ERP. We wish to explore how these characteristic modes of empirical study impinge upon the framing of the research and on their findings.
Existing studies of technology and work organisation and their shortcomings
Snapshot studies
When a new technology comes to the fore, many of the first papers appearing are from the trade press and practitioner journals. The concern is to demonstrate the benefits of the technology and how these benefits may be successfully achieved. As a result the focus is typically on what Botta-Genoulaz et al. (2005: 574) describe as ‘impact factors’. Moreover, impact studies are also often what might be called ‘snapshot studies’. This is research that typically describes the organisation before and after the implementation of the particular innovation in question.2 There is a tacit technological (or other) determinism embedded in the research design and temporal/organisational framing of such enquiries, in which observed changes are attributed to the effects (‘impacts’) of the technology, apparently disconnected, for example, from the prior organisational setting which motivated and shaped the innovation as well as the subsequent organisational processes through which the outcomes were achieved.
Importantly, snapshot studies are often conducted a relatively short time after the introduction of a new technology, arguably before the complete consequences of an innovation can be reasonably assessed. This is problematic because as we have already seen, it can take years before the full benefits and costs of major changes such as ERP or Business Process Redesign materialise. As a result, researchers may be forced to anticipate projected benefits/costs, and therefore run the risk of being unduly influenced by the programmatic goals and visions of the promoters and suppliers of a technology, as well as the hopes of the managers pushing these changes through within their own organisations.
As discussed in Chapter 2, these kinds of studies are important in developing and circulating ideas about an emerging technology. They thus contribute to the rhetorics of technology supply and often also suffer from its shortcomings. They are, however, influential in framing understandings - and
many of these studies are geared towards influence rather than analytical accuracy and distance. Even though they might provide interesting information and insights, snapshot studies are typically only of limited analytic value and the data and claims they contain need to be interpreted with caution.
Implementation studies
We include under the heading of ‘implementation studies’ the large body of writings concerned with the introduction of new technologies into organisational settings (whether from sub-disciplines within Business Studies, such as the Management of Technology and Innovation, or from Technology Studies and the Computer Sciences). This involves work with a stronger analytical grounding than the snapshot ‘impact studies’ characterised above, which addresses how the technical and business outcomes are closely related to (indeed generated in) the process of implementation (Clark and Staunton 1989).
Implementation studies account for the bulk of research into new organisational technologies such as ERP. There is a huge literature on ERP systems (for reviews see, for example, Esteves and Pastor [2001]; Al-Mashari [2003]). The ERP Research Group (2006), for instance, has 600 articles in its online bibliography and the overwhelming majority, over 95 per cent of these, correspond to what may broadly be described as ERP implementation studies (including also closely related topics such as the management of ERP adoption, organisational outcomes and ‘critical success factors’).3 Though most of the earlier papers were of the sort we have described as impact studies, scholarly research tends to appear later (Grabot and Botta-Genoulaz 2005). This recent work has a stronger social scientific grounding and has been more rigorous and offered more critical insights. We note the growth across a range of disciplines (including, for example, Information Systems, Organisation Studies, Management of Change, etc.) of more sociological work, informed by a processual understanding of technical and organisational change and deploying qualitative, often ethnographic, research methods. This work yields a richer knowledge base, going beyond the standard unitary managerial view of the organisation, and addressing different perspectives within the organisation and the particular processes which underlie these outcomes.4
Problems of the temporal and societal framing of implementation studies
While welcoming these studies we draw attention to potential forms of bias that arise from the temporal and societal framing of implementation studies (Williams 1997).5 The severe limitations of impact studies and methodological shortcomings of snapshot studies have already been outlined. Though many implementation studies may be more grounded, they are still typically retrospective accounts and of short duration compared to the extended
timeframes involved in the complete adoption cycle (involving the initiation, procurement, implementation, use and subsequent review) for such kinds of radical technological and organisational change.
Retrospective studies suffer the risk that respondents will modify their responses with hindsight and often align with a managerially sanctioned view. For example, as McLaughlin et al. (1999: 104) observe, the selection of an information system will typically be described by informants as having occurred in a ‘carefully managed’ and ‘rational’ manner. There are likely to be biases in empirical access, since user organisations, let alone supplier firms, are not necessarily keen to promulgate accounts of failure or allow researchers access to projects that are the subject of overt conflict and controversy.
It is moreover necessarily difficult for external researchers to gain access to the earliest stages of a project to address its inception, what Gerst (2006) calls the ‘pre-project stage’. One obvious reason for this is that, in developing studies of the adoption of a particular technology, the selection of firms for inclusion in the case-study or survey is on the grounds of their having already taken at least the ‘in principle’ decision to adopt. As Pozzebon and Pinsonneault (2005) point out, choices by organisational actors at this stage configure the initial context of an implementation project and can have important consequences for its conduct and outcomes.
Researchers, limited by the practicalities of research funding and agreeing access, are also liable to leave too soon as well as arriving too late. Implementation studies are still often based on short- or medium-term access, with fieldwork covering a few months or at most a year or two, and are therefore weak in terms of assessing longer-term outcomes of innovation episodes for organisational users. This may be important given the large body of research, dating back to Arrow’s (1962) analysis of ‘learning by doing’, that stresses the significant post-implementation improvements in productivity as organisation members discover and refine ways of using artefacts more effectively. This process of trial-and-error learning and struggling is key to our social learning perspective, which highlights crucial innofusion and domestication processes. Recent studies of ERP implementation, and especially research concerned to assess its outcomes, increasingly stress the need to look at this extended ‘post-implementation’ phase (Berchet and Habchi 2005). Various analysts have further divided this into the ‘shakedown phase’, and the ‘onward and upward phase’ as these complex systems are coupled with organisational practices and as their further utility for the organisation is discovered and exploited (Markus and Tanis 2000; Somers and Nelson 2004; Robey et al. 2002).
Implementation studies that end too soon may thus underestimate the eventual organisational consequences of an innovation. Indeed, many critical researchers, keen to highlight the gulf between promised benefits of an innovation and its outcomes, may have unintentionally replaced the ‘can do’ rhetoric of technology supply with a misleading ‘no can do’ scepticism about its ability to reshape organisational contexts, emphasising the barriers to
fulfilling their promise of delivering rapid organisational transformation -barriers that are rooted in particular in the diversity of local working practices (Williams 1997). In the short term, organisational structures and practices appear to be more robust than the organisational templates embedded in the machine (Webster 1990). However, there is a danger, and this is a very real danger when extrapolating from individual implementation studies, of overlooking the gradual alignment and harmonisation of organisational practices that may occur around the organisational templates embedded in the technology (Williams 1997). For example, Webster and Williams (1993) describe a case in which a second implementation of CAPM succeeded (after the first ‘failed’), aided by the informational and change management practices that had been put in place in the course of the original implementation.
Various writers pursuing a deeper understanding of the organisational consequences of technologies have sought a more intricate, dialectical understanding of the interplay between organisational structures and artefacts (Orlikowski 2006). Thus Kallinikos (2004a) sees ERP systems as embodying organisation templates and taken-for-granted views of the firm and at the same time reinforcing the routinised view of organisational activities, thus conditioning the behaviour of organisation members. Benders et al. (2006) similarly suggest that standardised organisational technologies like ERP, with their general models of organisational centralisation and standardised business processes, encourage diverse organisations to align with their embedded organisational models. They argue that this may constitute a new form - which they term ‘technical isomorphism’ - of the isomorphic pressures asserted by neo-institutional theory as causing organisations to become increasingly similar. We are sceptical, however, of accounts such as the neo-institutionalist isomorphism thesis, which simply emphasise stability or change. Our analysis seeks to explore the intricate interplay between stabilising and dynamising factors, which often lead to more uneven outcomes. Thus, changing managerial alliances and circumstances may promote ‘drift’ over time as a result of misalignment between an ERP programme and evolving organisational exigencies (Lee and Myers 2004; Nandhakumar et al. 2005).
Studies conducted from a technology-supply perspective tended to see the user organisation as a blank slate, amenable to technology-induced transformation. However, organisations are robust socio-technical systems, with their established technical and managerial divisions of labour and assemblages of routines and working practices, formed through their particular history and earlier phases of technical and organisational change. We therefore need better tools to characterise organisations. Clausen and Koch (1999), for instance, have analysed these under the heading of the Company Social Constitution (CSC), pointing out that particular episodes of technological change are just small and localised moments in the evolution of the CSC.
Many other implementation studies demonstrate the reverse of this problem and have tended to treat the software and its supplier as a something of
a ‘black box’. This is partly due to the framing of their research. Lacking access to sites of technology development, studies of implementation had little opportunities to scrutinise the development processes and history that had given rise to it.6 Any inference about supplier behaviour made in these studies is thus primarily derived from observations and perceptions within the user organisation. Implementation studies have been mainly reticent about the world of technology design. Rather paradoxically, perhaps, the software package vendor appears to have been made ‘other’, and, where discussed, one-sided accounts and on occasion negative stereotypes have often been deployed to characterise the behaviour of vendors and of consultants. Drawing, perhaps, on critical perceptions of supplier offerings and behaviour within the user organisation, these accounts often convey a negative sense of the role and contribution of external technology and knowledge providers that seems hard to reconcile with the fact that it is the user organisation that hires the vendor.
Technology designldevelopment studies
This brings us on to considering how to address technology supply, which is linked to our concern to understand the material properties of organisational technologies. This aspect has not received sufficient attention overall. Rather few studies have been undertaken of the contexts in which organisational technologies have been developed and have evolved. We can identify some practical reasons for this, which include the difficulties in obtaining access to commercially sensitive sites of technology development, and the fact that there are far fewer developer firms than users. No less important may be disciplinary divisions: Organisation and Management Studies are concerned with organisational process and outcomes and have therefore tended to focus on the organisational user and black-box the supplier and their technology.
However, a number of writers have shared our concern about the need to look at the historical development of these artefacts prior to their organisational implementation. This is in relation to earlier systems like MRP II (Clausen and Koch 1999) and current ERP systems (Kallinikos 2004a; Koch 2005, 2007). There has also been some research into technology development processes, notably within a Technology Studies perspective. These latter studies have, however, often been carried out in isolation from technology implementation and use (MacKay et al. 2000). This fragmentation and framing of enquiry has consequences. In particular, those analysing design may succumb to the temptation of seeking to infer the implications of particular design choices for those using the artefact, as exemplified by Woolgar’s (1991) much cited idea that the designer ‘configures the user’. This kind of approach to analysing technology design, with its rather simplistic presumptions about development capabilities and how design choices will affect the user, yields a bizarrely over-politicised account of design (Stewart and Williams 2005). This temptation is understandable, perhaps. Social
scientists want to develop an understanding of design choices and their attendant social implications and outcomes but run into the problem, when studying sites of design, that most of the work is ‘mundane’. Design choices have a ‘taken-for-granted’ quality and not much seems to be happening in relation to the user. In this context, every comment made by designers about the user and their context is liable to be seized upon. However, an understanding of the everyday reality of design and development work and the internal exigencies that exist within supplier organisations is important - and it is one of the things our book attempts to investigate, especially in Chapter 8.
In terms of exploring the implications of design choices for organisational users, one approach might be to look for internal technology controversies that have emerged, to find sites where competing options are being contested and where choices and their implications become highlighted. This, however, is perhaps more easily done in historical research (Hard 1993). Another approach, and one that we have followed in this study, is to focus upon the various interfaces between suppliers and users which constitute key nexuses in which competing requirements are presented and worked out. In our study, for instance, we focused on the ways in which an ERP supplier utilised groups of users (Chapter 5) to help guide the evolution of its packaged solution. Design - and the coupling between artefact design and its imple-mentation/use - is thus being worked out through a range of different networks and intermediaries linking supplier and user. It is also being worked out over multiple settings of organisational implementation (implementation cycles) and in aggregated form over multiple product cycles. It is important to address these implementation-design-implementation cycles. For example, as we have previously mentioned, numerous ERP implementation studies have highlighted the problem of ‘fit’ between the package and the various settings in which it is applied. However, upon examination, this problem can be seen to be rooted not simply in the lack of fit between ‘the technology’ and ‘the organisation’, but between the implementation sites for which the technology was initially developed and the sites in which it is currently being applied (Brady et al. 1992; Webster and Williams 1993; Soh and Sia 2004).
Need to attend to technology design and implementation in tandem
These observations underline our argument about the need to attend to technology design and implementation in tandem. Yet there are almost no studies addressing design and implementation together. Why is this? One reason, as well as the above-mentioned disciplinary divisions between those studying organisational users and software developers, is the very practical one that technology development is in most instances not only socially but also temporally separated from implementation. Moreover, the lag between design of a technology and its implementation typically exceeds the duration of most social science research projects (Williams et al. 2005).7 Researchers
contemplating the trade-off between depth of fieldwork and the number and range of fieldwork sites of technological innovation have tended to opt for one or other setting. This trade-off is arguably made more difficult by the emphasis within contemporary social science on ethnographic approaches. Though strong in capturing the richness of local processes in real time, ethnographic methods are labour intensive. Ethnographic researchers have therefore often opted for simple research designs - mostly involving singlesite studies or studies of a number of closely related settings.
In this study we sought to overcome these problems, going beyond the current fashion in qualitative social science with deliberately naive methodologies and utilising our theoretical and substantive knowledge - in particular our theory of the biography of packaged software - to sample a selected array of locales of technology design and implementation. Our multi-site analysis thus addressed settings of technology development and implementation/use and focused in particular on nexuses between design and use and the interactions between them. It integrated the historical and contemporary; and it addressed different locales in the design implementation cycle. We further argue that researchers need to look at artefacts at different stages in their biography. We thus observed packaged developments at early and late stages of this cycle, studying both the birth and the evolution of new artefacts, as these differ significantly in terms of the relations between actors and the institutional structure - in short, the mechanisms for mutual shaping of technology and society. Let us look at these points in turn.
Addressing the social fabric
Let us recap our point of departure from mainstream constructivist approaches to the analysis of technology and work. Early contributors to the selfstyled ‘New Sociology of Technology’ (Pinch and Bijker 1984) primarily addressed innovations and innovators that established a new field of techno-scientific practice. As these were often at the interstices of existing institutional structures, ANT and SCOT theorists were able in their analyses to ‘foreground’ the actors directly involved. These often focused particularly on ‘heroic’ technical specialists, who were conceived as ‘Sartrean engineers’ (Latour 1987), apparently outside or able to operate free from constraint from the social structure. They thereby relegated to the background, or ignored entirely, the historical and institutional factors which underpinned these developments. However, such actor-centred accounts yield unbalanced explanations. Their shortcomings are particularly problematic when we try to use these frameworks to analyse the development of workplace technologies and other instances of incremental innovation within well-established institutional settings. Local actions are sustained and constrained by an extensive network of technical, organisational and social arrangements whereby some (material, institutional) elements are difficult for local actors to change (Kallinikos 2004a; Koch 2005).
Clearly, we need a ‘contexted view’ (Morrison 2002) that can address the complex social fabric and its history which pattern the activities of those involved locally. Moreover, our explanatory frame needs to be one which avoids the simplifying logics of particular disciplinary approaches or schools, and which can match the intricacy of the settings and processes we are studying. We start with an observation that the character of these applications is being fought for, and shaped, at a number of levels ranging from local contestation around features of design or its organisational implementation to the broad macro-level concept. Moreover, the complex web of relationships involved changes over time; it is, as Koch (2007) observes, a ‘moving target’.
Thus far, we have established the importance of sites of technology development and its implementation and use in understanding innovation. This alerts us to the need to address the way individual actors (e.g. suppliers, potential users, intermediaries) and the relationships between them are conditioned by their broader setting. How then shall we conceptualise the broader setting?
Koch (ibid.) has criticised the existing explanatory frameworks of ERP studies as being too simple. In an important series of articles, he maps out a broad framework: local studies (e.g. implementation studies) will no longer suffice, he argues; ERP is both ‘local’ and ‘an institution’. However, he also expresses dissatisfaction with dualistic analyses that counterpose local and institutional developments (Koch 2005). He proposes, instead, that we should examine ERP as ‘a community’ (Koch 2003) constituted by joint involvement in a technology. Koch (2007: 440) observes that in future ERP studies need to ‘go beyond the single space enterprise, as well as moving away from implicit assumptions of stable states of the system’ and to adopt instead a ‘multi-local’ analysis of technology. To this end Koch (ibid.) proposes, as a conceptual frame, a six-field matrix, encompassing short- and long-term, and micro, meso and macro elements, of which implementation is but one out of six aspects. We find ourselves in complete agreement with Koch’s analytical project (and particularly his call for more effective analytical templates addressing multiple locales and histories). We hope in this work to contribute to this common goal in some way.
However, we have some reservations about the macro-meso-micro distinction. Though these are very convenient and communicative labels (and we will use them as such) they run the risk of being mechanistically misconstrued as fixed and separate levels when in fact they are interpenetrating (a point which Koch of course recognises in his critique of the local-institutional dualism). Moreover, these need to be seen as relational categories. In other words, what is seen as macro and meso is relative to the sets of (‘local’) actors and issues under examination (Kaniadakis 2006). It is perhaps more useful to develop a generic model of the social shaping of complex, commodified organisational technologies such as ERP, rooted in empirical analyses, that can provide a complex template for our analysis.
Developers, users and the developer-user nexus
The need to attend to the developer-user nexus is flagged by various approaches. It is, for example, axiomatic to the Social Shaping of Technology (SST) perspective, which insists that the development of industrial IT applications is shaped not just by strategies of designers/developers in the domain of technology supply but also, as previously noted, by the settings of implementation and use. Fleck’s innofusion concept (Fleck 1988b, 1993; Fleck et al. 1990) flags the innovation processes taking place in the so-called diffusion stage in the process and arenas of technology implementation, and the possibility that these experiences may feed back into future technology supply. It also highlights instances where this experience was not effectively utilised. The Social Learning Framework builds upon this important observation and proposes an integrated picture of the development and implementation of technologies (Williams et al. 2005). Like Evolutionary Economics, the social learning perspective draws attention to the importance of the coupling between supply and use. However, unlike the main tradition in Evolutionary Economics with its concern to assess the differential overall efficiency of such coupling in particular national or sectoral innovation systems, the social learning perspective seeks to explore the detailed processes and mechanisms for this coupling and its consequences for innovation pathways.
The importance of paying attention to these coupling mechanisms comes immediately to the fore when we examine COTS solutions. Here we find an extraordinarily intricate web of formal and informal linkages between package vendor and organisational users. Here our analysis takes us beyond the immediate inter-organisational level of direct interaction between supplier and user. For example, our concern to analyse procurement stimulates us to address the broader terrain of suppliers of classes of products. New industrial technologies develop in parallel in a multitude of user sites and through the activities of many vendors and associated players (e.g. consultants). It is to this meso level, beyond the direct user-supplier linkage, that we now turn.
The fine structure of external experts, intermediaries and knowledge networks
We can track an array of relationships out from the organisations contemplating COTS adoption. Here we note first the various avenues of information the user organisation draws upon in deciding to resort to a packaged solution like ERP and in selecting a particular vendor and providers of complementary products and services. Finkelstein et al. (1996: 1) have drawn attention to the influence of advertisements, supplier literature and demonstrations. They also point to the importance of observed use of the packages in other settings (for example, demonstration sites where COTS have been installed) and comparative studies provided by third parties (trade papers, etc.). As well as drawing information from knowledge and technology
suppliers, potential adopters seek more impartial information through informal links with similar organisations. In addition, our study (Chapters 6 and 7) will highlight the growing influence of industry analysts as providers of community information about the provenance of particular vendors and their offerings.
Once a packaged solution is determined upon, a more tightly coupled set of contractual relationships will be established by the adopting organisation with those charged to deliver an ERP implementation, involving a wide range of actors. As well as the vendor itself, there could be the suppliers of associated products (hardware and bolt-ons) and various sorts of external knowledge providers. These include groups such as consultants who offer expertise in ERP implementation and who, in turn, might be assisted by external providers of other sorts of relevant expertise such as systems integration or change management experts more generally.
The management fashion for outsourcing organisational functions increases the range of external experts utilised, which may also include, for example, training providers. This trend is reinforced in the case of ERP by the explosive growth of the market, which outstripped the capacities of suppliers to provide this expertise, as well as the exigencies of supply of generic solutions that motivated the deliberate decision by suppliers such as SAP to focus on generic supply. These decisions are buttressed by an array of firms, such as the suppliers of interoperable products, systems integrators, imple-menters, trainers, change management experts, and so on (Sawyer 2001; Koch 2005, 2007). Koch points out that this differentiation is crucial to the value-maximising strategy of the package suppliers, for ‘ERP is (big) business and design of these systems occurs under strategies of mass customization, where the encoding of the generic user is a necessary tool to reduce development costs and time to market’ (Koch 2005: 43-4).
We can also track these webs of knowledge and influence from the vendors as they seek to manage and sustain their existing user base and expand their markets. It arises in the course of their efforts to provide service and support to users and also to guide the development of new products, and through linkages to existing customers. In this latter respect this is often indirectly, through knowledge arising from their services (e.g. help-desk queries) and also more directly and explicitly through permanent knowledge linkages (e.g. through user-clubs; marketing department discussions with potential customers).
The increasing resort to outsourcing the supply of business solutions (and their supply as packaged rather than bespoke solutions) and the increasing role of consultants in the supply of knowledge services radically transforms the institutional terrain in which changes in industrial technologies are adopted (though there may be some similarities in the process of implementation [Brady et al. 1992; Fincham et al. 1994]). In particular, it changes the character of the conflicts of interest from a primarily intra-organisational contest for political legitimacy and access to resources to a
primarily inter-organisational contest for contracts and streams of income and services. As already noted, this also changes the character and role of internal expertise and sets up complex alliances between organisational interests and external economic interests (such that public and private organisations carefully regulate such linkages) (Procter et al. 1996; Howcroft and Light 2006).
The selection of consultants and the role delegated to them, shaped by established ways of working and reputation from previous projects, configures the arena in which the project unfolds in ways that may give consultants more or less autonomy and influence over the outcomes (Hislop 2002; Pozzebon and Pinsonneault 2005). We find that change is taking place in a complex social setting, and one that is patterned by pre-existing social relationships. Clausen and Koch (1999) similarly identified more or less stable couplings between particular groups of user organisation and vendors, which they have described as ‘segments’ of the ERP market.
The segment
Clausen and Koch (1999) explored how the shaping of ERP in the 1990s took place across a structure comprising the CSC of the adopters, with their own internal dynamics, and various ‘segments’ of IT suppliers and custo-mers.8 They suggest that knowledge flows within these segments were shaping the evolution of ERP. This included implementation experiences, and the new demands and visions circulated between suppliers and their customers. Drawing on theories of ‘organised capitalism’ (see Lundvall 1985), Clausen and Koch (1999) see the persistence of these segments in terms of the benefits of these knowledge flows and a coalescence of similar views about business improvement. Crucially, they argue that different segments, and the different procurement strategies and associated forms of supplier-user coupling, offer different opportunities for local influence over the design of the ERP system (see Figure 3.1).
Later work by Koch points to the influence of broader and longer-term changes affecting these ‘meso’ structures, including the restructuring of the ERP supply sector in the 2000 economic downturn which swept away some of the weaker and small players. These segments are not stable. There have also been some realignments in the constellations of players around ERP provision (generic solution providers, suppliers of complementary products, implementers and other value-added resellers) in a complex pattern which combines elements of stability as well as dynamism (Koch 2004, 2005).
Our earlier work on CAPM had highlighted the influence of similar mesolevel features on the way these artefacts were shaped and promoted in part through industrial and professional networks (notably in the UK by the British Production and Inventory Control Society,9 a body strongly influenced by vendors and consultants as well as industrial managers). CAPM was also influenced by more general (and changing) models of best industrial
Figure 3.1 Mapping the broader structure in a multi-local analysis. Source: Clausen and Koch (1999).
practice (Webster and Williams 1993; see also Swan and Newell 1995; Robertson et al. 1996; Hislop et al. 1997).
And today, as we already noted in Chapter 1, there is a marked reorientation of ERP supply. This is motivated, on the one hand, by the fact that it has largely saturated its original established markets in large manufacturing organisations and now must find new customers or develop new functionality for existing customers, and, on the other, by a desire to respond to criticisms that its offerings have lagged behind the latest business concepts, in particular of value-networks. So we see ERP being reoriented towards new targets (public services and SMEs whose requirements it has been criticised for neglecting), the development of new technology architectures (in particular of the internet and web services), and new functionality (CRM and supply-chain management) which brings it into competition with established players in these niches. Its future is being debated and contested in this inter-organisational space as much as through organisation-level implementation.
Macro level - visions and imaginaries
This final observation forces us to consider developments at a more macro level. This is, first, in terms of the relationship between these changing conceptions of an organisational technology and the circulation of broader views of industrial improvement (which inform prescriptions of good/best practice). And, second, with visions of how these may be fulfilled by emerging technologies.
Previous work has noted the correlation between technologies and business programmes. For example, in the 1990s, the European Union invested heavily in electronic trading as it matched their vision of an open international market (Williams 1997). Similarly, 1980s conceptions, such as integrated information systems, were the technological correlate of the integrated, customer-focused finance organisations (Fincham et al. 1994).
Swanson and Ramiller (1997) have highlighted the role of ‘organizing visions’ in information systems innovation, encompassing interpretation, legitimation and mobilisation, which help mobilise the material and intellectual resources needed for innovation. Wang and Ramiller (2004: 12) analyse the evolution of attention (in what they call an ‘innovation community of vendors, consultants, adopters’) from:
1	knowing-what: interpretations that help to conceptualise the innovation;
2	knowing-why: rationales for adoption that help to justify the innovation;
3	knowing-how: implementation and utilisation strategies and capabilities that capacitate the innovation.
While initially the focus of attention is on what the technology is and why it should be useful, later attention shifts to issues in its successful implementation and exploitation by user organisations.
Judith Gregory has developed the concept of ‘incomplete utopian project’ to ‘describe the phenomenon of envisioning as constructed, evoked, and employed within an innovative intra- and inter-organizational effort, and to open up theorizing about innovation, work practices, and technology’ (Gregory 2000: 180). The word ‘utopian’ draws our attention to the influence of ‘longstanding deeply shared desires simultaneously characterized by their unrealizability and their devotees’ tendencies to over-reach reality in their pursuit’ (ibid.: 194). Building upon this as well as a shared tradition in activity theory, Hyysalo (2004, 2006) has developed the concept of ‘practicebound imaginaries’ to refer to such visions, which provide an intellectual resource that helps frame, mobilise and pattern expectations around an array of players. Instead of portraying visions as disembodied, the concept of practice-bound imaginaries conveys the extent to which they are coupled with existing and achievable practices and artefacts: ‘relatively integrated sets of visions, concepts, objects and relations that are regarded as desirable, relevant and potentially realizable in and for a practice, and as having cognitive and motivational power for organizing this practice’ (Hyysalo 2006: 602).
This bears directly upon our analysis of the evolution and biography of ERP (and for example Koch’s [2003] analysis of ERP as a political programme for organising change). A key part of the ‘heterogeneous assemblages’ (Koch 2005) of human and material elements that constitute ERP is comprised by inter-subjective elements: promises, visions of best practice and prescriptions for industrial improvement, and criteria for assessing
technologies alongside artefacts, techniques and practices. The biographies perspective, however, helps us analyse the way these communities operate across the diverse set of social actors involved: suppliers and users, consultants, industry analysts, policy-makers and commentators (Koch 2007).
Macro level - external developments
These final observations force us also to consider changes in wider terrains that shape and are reshaped by these developments in the world of ERP. These include, for example, at the broader level:
1	changing models of economic life in an increasingly globalised networked economy;
2	the structure of the information technology sector;
3	the emergence of new technological models such as web-service architectures, which could herald major changes in how the information infrastructure is created through third party hosting, etc.;
4	the structure and recent restructuring of corporate knowledge services involving the increasing influence of a small number of global management and accountancy firms, and the convergence of information systems integration and change-management organisations upon high value-added opportunities from deploying these in tandem (and the associated linkages and mergers between these firms).
This brief review of developments around ERP brings us nearer to being able to sketch out a schema for analysing these multi-local developments in more abstract terms.
Arena and agora: concepts for exploring this complex space
Arenas of technology development and implementation
How then can we conceptualise this complex space, linking together material artefacts, practices and visions within an extended fabric of individuals, organisations and inter-organisational structures and associations? As Koch (2007) argues, we need better spatial metaphors for addressing this rich tapestry, which is characterised by gaps in time and space (e.g. between developers and users, as well as by more or less sharp differences of interest, expertise and commitment). We could theorise this as a ‘distributed innovation process’ (a concept recently advanced by innovation economists) or as the operation of a ‘network’ (in the way Actor Network theorists might do). These, however, represent a very imprecise way to characterise what is in fact a rather structured set of relationships. As others have noted (see Knorr-Cetina and Bruegger 2001, 2002), the concept of network has come to be widely adopted but in a remarkably loose as well as inconsistent manner.
We have been attracted by Jorgensen and Sorensen’s (1999) concept of ‘development arena’. The value of the concept for us is that, seeking to provide tools for Actor Network Theory-based explanation to deal with the broader interactions evident in global technology developments, it conceives of the arena as a space, using the analogy of a circus ring drawn in the sand, in which a number of more or less conflicting actor-worlds collide. In addition, they flag the possibility of radical reconfigurations of an arena through changing boundaries and realignment of players, providing tools to explain destabilisation as well as alignment:
a development arena is a visualizing spatial expression of processes of competition and co-operation. It should convey the idea that several actor-worlds are being construed within the same problem area. It depicts the idea that several actor networks co-exist and interfere with each other within a certain problem space. A development arena is our attempt to bring together processes or entities that would otherwise seem to be dislocated. It can be seen as the place where actors relating to a certain set of problems meet and exchange ideas, etc.
(ibid.: 417-18)
Fleck’s (1988a) innofusion framework had similarly flagged the arena of implementation as a key site of innovation in industrial technologies. He has a rather similar concept of the arena as an inter-organisational space comprising members of supplier and user organisations and constituting a setting for practical learning and struggling, in which different kinds of competence and knowledge are deployed (e.g. the engineers’ knowledge of computer science techniques and artefacts and the organisation members’ knowledge of their context and purposes). Indeed, our initial concept of the biography of an artefact was based on the idea of an artefact alternating between moments of innovation in technology supply and implementation (see Williams 1997).
This kind of formulation seems less adequate to us today as our attention has turned to look in more detail at the myriad forms of direct and indirect relationships linking supply and use and also shaping the overall character of offerings in a technological field. It would be possible to scale up Jorgensen and Sorensen’s (1999) development arena to include implementation, but this would be to overlook the asymmetries and tensions between development and implementation. It is useful to examine moments of design and development separately from implementation and domestication, as we see these moving not always in synchronisation but often exhibiting different dynamics.10 Industrial and organisational technologies are characterised by a small number of global supply-side players, a larger array of complementary product suppliers, and huge numbers of adopters.
Clarke and Casper (1996) have also deployed the concept of arena in their interactionist analysis of medical and scientific developments as part of an
endeavour, which parallels some of our concerns, to address power dynamics that have been muted in ANT and SCOT analyses. They address the relative power of actors by analysing outcomes: the consequences of the actions of social actors within their particular sub-cultures/social worlds and a shared arena. Their approach ‘centres on grasping and representing the perspectives and properties of all the major actors (including collective social worlds and nonhuman actors) in a particular arena of mutual concern or in which certain actors are implicated’ (ibid.: 602).
How then can we characterise the complex spaces in which technologies like ERP emerge and evolve? To characterise these as a single arena may be to underplay the very different textures of the fabric of social relations (which, for example, range from contractual linkages between firms to weak associations of opinion across dispersed communities). We could alternatively describe the setting for development and evolution of organisational technologies such as ERP in terms of a multiplicity of overlapping arenas: these could be development arenas, implementation arenas and specific Company Social Constitutions (for CSCs are surely arenas too). This, however, might distract attention from the fact that many players will appear in multiple arenas. Characterising these as separate spaces may not be helpful to our current concern to develop multi-local theorisation, both of the many kinds of supplier-user relationships and of the overall development of a technological field. Instead, we want to look at the various different kinds of relationships established between broadly similar or at least strongly overlapping groups. We also need to be able to analyse these at different levels of generality and timeframes.
The ‘agora of technology and organisational change’
Antonios Kaniadakis (2006) has introduced the concept of the ‘agora of technology and organisational change’. He sees the agora as a meeting place and a market in which all producers and consumers of organisational technologies potentially interact. The agora, thus conceived, is diffuse and not clearly bounded. However, various particular bounded perspectives on the agora are drawn (by actors and analysts) for different purposes (of action and analysis) depending upon their particular context and purpose. In other words, actors construct particular viewpoints of the agora: they see and engage with particular slices of the complex multi-local multi-actor space of the agora and set boundaries depending on their purposes and relevancies. How the agora is conceived depends upon the actor’s relationship with it. Thus, a user organisation has a very different view and orientation towards the agora from that of a technology vendor.
Viewpoints are active constructs; it is not simply a question of where you stand, it is also a question of the purposes of players constructing it. The agora is also, and perhaps primarily, an analyst’s construct. The researcher makes choices about which tranches of this complex structure to sample and
with what closeness of view.11 The arenas we have discussed may be seen as particular viewpoints within the agora. The agora has a structure (which we shall attempt to map empirically and conceptually). Thus, Kaniadakis sees the agora as having micro, meso and macro levels. However, what appears as local and as broader context also depends crucially upon what is being examined and how. For a study of interactions in a particular workgroup, global technology developers may appear as established features of the macro environment, along with other legal and institutional structures, that are not amenable to influence by the actors in the timeframes involved. In a study of these technology developers, however, the market of (unknown, distant and thus impersonalised to them) potential users may appear as an obdurate and immovable constraint. This is then a relational (not a relativist) conception. The concept of agora would seem to meet Koch’s (2003) call to go beyond a dualistic local-institutional conceptualisation and address ERP as ‘a community’. Moreover, it opens up opportunities to address the intricate structure of this community and develop methodologies to capture this.
The concept of ‘viewpoint’ gives us a means to discuss the necessary choices in research design when trying to address complex social phenomena. It provides a means of steering between naive undifferentiated approaches to approaching the social setting and adopting a particular conception of social structure. It links with our argument that research design needs instead to adopt a ‘variable geometry’.12 The viewpoint concept flags that choices must be made in terms of sites of access and tools for investigation, depending upon the phenomena being investigated, the kind of access the researchers have secured and their analytical purposes. Effective research design calls for thoughtful selection of sites and methods of investigation. The same phenomena could, for example, be addressed through large-scale survey methods or ethnography. Which is most appropriate depends on the research questions involved.
The dynamics of the agora
The agora may be a diffuse and plural array of players. However, it is not an open and equal community - in the way in which we might conceive of scientific communities operating under the Mertonian ideal, for example. Its internal structure comprises not just peer-like communities of practice (a la Wenger 1998), but also communities of (often conflicting) interest. It is characterised by asymmetries and entrenched conflicts as well as alignments of interest. Criticisms have been advanced of the failure of community of practice theory to develop an adequate analysis of power, ideology and conflict, particularly in inter-organisational settings and despite its initial recognition that these were potentially important (Fox 2000; Roberts 2006). Many of the points made by Koch (2007), explaining why his analysis of ERP as a community could not be in terms of a ‘community of practice’, are applicable here.
Moreover, the agora is not only comprised of diverse and heterogeneous elements, it is also a disjointed space, perhaps better understood as a heterogeneous assembly of physical and abstract spaces. For example, the agora of technology and organisational change, though viewed perhaps from particular national settings, is closely coupled with bodies and groups with international scope. However, when we consider a developing country like China, for instance, we find a space only partly integrated with the Western agora for enterprise systems and organisational technologies more generally, with very different traditions among ‘user organisations’ and national software suppliers despite the presence of some Western multinationals (Liang et al. 2004; Xue et al. 2005; Wang 2007b).
Commodification of knowledge networks
The ERP community is a knowledge network. It is also, however, a locus of struggle and conflict. Many parts of this segment of the agora are subject to commodification. This imparts a complex dynamics to the agora. We have already discussed the difficulties of trading in informational and non-material products. Thus in the case of ERP, we find various commercial suppliers of knowledge-based products (ERP solutions, and complementary products) and services (ERP consultants and other change-management and integration specialists). In Chapter 6 we will analyse in detail the attendant difficulties in establishing the provenance of a provider and its products, and of demonstrating the benefits of that solution to a particular user organisation and of overcoming misfits. Not least, because ERP is a generic product, a substantial investment must be made in implementing it within an organisation before its outcomes - before actual and achievable fit - can be realistically assessed. These difficulties in assessing the qualities of a product mean, on the one hand, that the market is a rather inefficient discovery mechanism, which must be supplemented by network or community types of relationships. On the other hand, outsourcing and commodification radically change the incentives faced by players in commercial relationships in the procurement of technology or (consultancy or integration) service, with sharp and very obvious conflicts of interest between competitors but also differences of interest and of commit-ment/world view between consumer and producer. Once the procurement process has been concluded, the arm’s-length externally policed contractual relationships invoked by economists might be presumed (hypothetically) to apply. However, in a context of necessarily ‘incomplete contract’ issues, this strict contract relationship remains notional. Though the existence of contracts changes the legal and governance character of previously voluntary relationships, the exigencies of joint learning in implementation are characterised by the erosion of boundaries and lines of responsibility. Here we may infer a spectrum of market relationships between what we might provocatively term, following Burns and Stalker (1961), mechanistic and
organic relationships, between those in which a more strict versus a more collaborative relationship prevails.
Thus, we see that in both phases of the market relationship, market forms are supplemented by communitarian and network forms of relationship (Fincham et al. 1994; Adler 2001). This is one way in which the relationships of the agora are unlike a community of practice or a scientific community but are shaped by the dynamics of commodification.
Another important way in which commodification shapes the agora is in the making of markets through the alignment of expectations. Here the providers of knowledge-based goods and services find themselves facing another paradoxical situation in mobilising general expectations. On the one hand, they may be drawn to collaborate with their competitors in building expectations that a particular technology/technique represents the way forward for business improvements. They have an incentive to raise collective expectations about this class of offerings, to establish a market for such offerings, and to promote its provenance among other possible ways forward as a road for organisational improvement. Simultaneously, they need find some way to promote particular claims about their own offerings, to convey that their particular product offers competitive advantage over its rivals. Thus, we find developers on the one hand competing with other providers to build their own particular solutions, to position themselves within their market niche, and build their share of the market, but on the other hand also operating in tandem with their competitors to establish the generic idea of utility of classes of artefact.13
This paradoxical situation: the simultaneous necessity of self-promotion and building trust in the class of solutions, places interesting constraints upon vendors and consultants. In practice, they often find themselves operating in a complex collaborative array with suppliers of competitive and complementary products and services of the sort that have been characterised as co-opetition. As Swanson and Ramiller (1997) point out, in discussing organisational visions, although some diversity may lead to richness and robustness in innovation, too much diversity and competition may lead to dispersal and loss of commitment.
James Stewart (1999) coined the term ‘poles of attraction’ to explore the ways in which (ICT supplier) firms seek to mark out their plans and visions of future technology in very clear ways: that is, to mobilise the expectations of potential customers and thereby build confidence in, and win commitments to, an emerging technology, or, at times, to ward off competitors, to mobilise fear, uncertainty and doubt and thus frustrate a competing technology. This concept highlights the influence powerful players can achieve within the agora based on resources they are able to mobilise.
That there are benefits in aligning expectations around new organisational technologies is evinced by the way in which a series of technologies and change-management techniques have acquired solidity and momentum (often by identifying a class of more or less disparate vendor offerings as a
‘technology’, of the sort described by the succession of three-letter acronyms such as CRM, ERP, MRP and BPR), only later to see that term ultimately replaced as supplier strategies and user expectations migrate to the next new solution.
Power and conflicts within the agora
The agora refers to a linked array of locales in which economic and organisational interests, as well as meanings, are at play and are being played out. From our social shaping perspective, we are particularly concerned with economic interests and technological commitments. The agora is a site of conflict and struggle and of negotiation and alignment. The kinds of activities that we note within the agora include many of the processes that have been analysed effectively by ANT (e.g. enrolment and alignment). However, ANT, with its roots in Machiavellian political theory (see, for example, Latour [1988]), tends to portray these processes in markedly voluntaristic ways that are perhaps more characteristic of struggles in a political realm. However, the agora is a site of economic and political power - that is, of political economy.
This is not the place to argue for a particular theory of power or of society - and our Biography of Artefacts Framework may be compatible with diverse approaches. However, in our analysis, we are seeking to address the ways in which local struggles are taking place within a broader institutional context - conceptualised in terms of circuits of knowledge and reward: creating visions and providing resources which constrain and enable local actions. We seek to draw insights from both semiotic and institutional accounts of power, and are attracted by Clegg’s (1989: 186) analysis of power relations within the organisation in terms of the operation of multiple ‘circuits of power’ at different (micro-macro) levels. Clegg’s analysis focuses upon the inter-group competition for resources and influence within an organisation, building upon a tradition that could be traced back to Burns and Stalker (1961). The resort to outsourcing of technology and knowledgebased products and services, coupled with transformations in the organisation of external supply, changes the character of the actors’ interest-pursuing strategies. In the struggle for influence within the organisation, it places greater salience upon inter-organisational as well as intra-organisational relationships. Members of external suppliers equally must act on an inter-organisational as well as intra-organisational terrain to sustain themselves as ‘points of passage’ for their customers (Law and Callon 1992), and in this way to secure resources and commitments to their own project. There is thus a close interplay between organisational political and commercial struggles and between economic and organisational political power. For our purposes, Clegg’s model needs to be adapted to consider the inter-organisational (e.g. meso) structures, as well as the organisational/occupational dimensions he focuses upon.
We seek to explore how local actions and outcomes depend upon a context of knowledge and beliefs which, in contrast to a narrowly semiotic interpretation of power, provides material as well as intellectual resources that generate incentives and penalties for local players and pattern the conduct and outcome of local actions by framing discussions. We are also seeking to explain the ways in which local actions collectively react back on to and produce/reproduce social structures.
In this analysis we are anxious to avoid dichotomising action and structure or reducing outcomes to the operation of one or the other - whether an actor-centred or institutional - account. Unlike ANT, we do not wish to do this by dissolving everything into the homogenising framework of the language of actor-networks, compressing all the different components into an actor-centred account and linked by ANT’s limited repertoire of relationships (such as Callon’s [1986b] generic translation strategies). Instead, we wish to pay due attention to the complexity of operation of socio-technical phenomena, differently constituted and observed at multiple levels of generality.
In terms of our analysis of the agora, the distinctions between micro, meso, and macro refer not to different things, insofar as they all are of the stuff of human socio-technical action (a point made by ANT writers that we are happy to take on board). Furthermore, in this relational framework, scales and perspectives of analysis can shift: what appears as an external macro constraint could figure as a meso layer in another analysis, as may be evinced by considering the example of the status of corporate strategy in a study of the short-term behaviour of a work group or in a study of the management of a major technology change programme. The distinction instead relates to the very different practical experience from particular viewpoints of immediate short-term and local actions and the more generalised aggregate outcomes of those actions - embedded in norms, practices, habits, organisations and technologies. This micro-macro scale, then, is in part a distinction based upon the level of generalisation, whereby macro analyses may address widely adopted routinised behaviours that constitute institutional constraints, as well as being a question regarding their level of local malleability to local actors.14 Standardised technology artefacts thus present themselves as part of the objective landscape for firms choosing organisational technologies, though in a study of settings of technology design their embedded technological choices will be more accessible and malleable to members of the vendor organisation.
The agora for technology and organisational change, analysed by Kaniadakis (2006), is a site for the mobilisation of promise and expectation (and likewise a site for counter-enrolments and mobilisation of uncertainty and doubt) at various different levels of generality. This may, for example, range from particular organisational implementations and supplier offerings to classes of organisational technology, and ICT capabilities more generally.
We thus see the agora as itself a product of a series of enrolment efforts and struggles, which may be described from different perspectives and at various levels of generality/pervasiveness and historical timeframes, ranging from immediate contexts of local action to more generalised patterns of behaviour, sustained over longer terms, which in turn constitute economic, technological and institutional structures. These broader ‘structures’ act to pattern innovation, providing resources and material constraints to actors in terms of their choices regarding which options appear do-able, which factors can realistically be changed, and which are to be taken as part of the landscape.
This double-sided character of the agora, as both shaping and shaped by socio-technical processes, may usefully be approached through the concept of negotiation, with its two distinct connotations. First, this is negotiation as a meeting, a place for alliance-building, conflict and struggle with more or less obdurate or amenable human and non-human elements; and second, it is negotiation as a set of manoeuvres needed to accommodate or bypass those elements which are effectively ‘non-negotiable’. This second usage of negotiation, which is akin to the way we might negotiate ourselves down a mountain pathway, is informed by the fact that some of the things we encounter present themselves as more solid and permanent from the view of particular local actors, including institutions and technologies, which in this sense are a kind of materialised institution, and have to be negotiated around.
This is not to say that these external factors directly imposed determinate constraints on local actors. Instead, we note another form of negotiation, as a kind of bridging between these spaces: for example, whereby key organisational players/systems re-presented external factors within the organisation. Finally, though we have for illustrative purposes discussed the case of elements seen as malleable or as fixed to local actors, there is no necessary polarisation between these. Actors can exercise choice about which ‘black boxes’ to accept as fixed and which to open up - depending, for example, on the relative costs and benefits of so doing. An organisation could, in principle, choose to develop a bespoke solution rather than be constrained by existing availability of packages, but this would depend on the level of resources they can deploy to such ends and whether such an investment would appear justifiable.
Finally, we notice that the agora is a space where very different kinds of commitment are being played out. There are, in particular, important differences in orientation to the agora between suppliers and organisational users of technologies and other knowledge-based products/services. Thus, for a technology supplier or a consultant the agora of technology and work organisation is the space where their commercial future is worked out. In contrast, a user organisation will have a more contingent orientation to the agora; they are keen to benefit from technologies but it is not normally part of their perceived mission to carry forward their experience with implementing technology as a resource for technology development.
Intermediaries as strategic players in the agora
The agora concept provides a space for analysing the various kinds of social relationship beyond the immediate inter-organisational level of direct interaction between supplier and user. To be useful, however, the detailed operation of the agora needs to be filled out and explained. Our concern to analyse procurement stimulates us to address the broader terrain of suppliers of classes of products and the ways in which beliefs about the provenance of a technology are constructed across a community of supplier and user organisations. Our final addition to the framework is to examine the emergence of new kinds of intermediaries who are also market makers and conveyors of community information. In Chapter 1 we reviewed research findings regarding the powerful influence of the technology supply side over views or what constitutes ERP. In Chapter 7 we will draw attention to the role of various kinds of intermediaries, and in particular the growing importance of a relatively distinctive class of intermediary, industry analysts.
Our Social Learning Framework flags the importance of different kinds of intermediary in innovation, in linking supply and consumption and the wide range of roles they play (Howells 2006). However, industry analysts appear to occupy particularly strategic sites in the agora. Our work will flag the importance of the Gartner Group in two ways:
1	we will show how this actor acts as a repository and organiser of what we call ‘community knowledge’ about the implementation of particular products and about the reputations of their suppliers’;
2	we will see (as noted in Chapter 1) how Gartner’s sectoral reviews consolidate the existence of a domain of technological activity (in this sense of constituting a technology like SAP’s R/3 as an instance of ERP), charting the overall development of particular technologies and their future development trajectory (Mabert et al. 2001; Judd 2006).
The industry analysts seem thus to play a crucial role in configuring particular development arenas and in mobilising consensus. It might appear that in some instances it is they who hold the ropes and set the rules of game - defining the boundaries of technology and the criteria by which particular vendors and their offerings may be judged. However, it is also important, while addressing their influence, to attend to the limits on how industry analysts proceed. Thus, we find that they are not able to impose their views. Their ability to play their role (and sell their services) depends on their being seen to operate in a close relation to practice, reflected, as we shall see, in the strenuous attention they devote to legitimating their position as impartial bearers of community knowledge in the face of criticisms of partisanship.
We consider the existence and profile of industry analysts like Gartner to be indicative of a broader development. Our review of the difficulties of the
operation of the market for complex technologies like ERP had pointed to the importance of indirect indicators of the behaviour or suppliers and their products - in particular of experience-based trust. However, this kind of trust is slow and expensive to acquire. In looking at the mechanisms for selecting management consultants, Gluckler and Armbruster (2003), as already noted, highlighted the value of networked reputation as a more effective mechanism for overcoming the buyer’s uncertainty. The role of industry analysts in IT procurement points to one mechanism for enhancing the efficiency of networked reputation formation through the commodification and canalisation of the circulation of community knowledge (and the way this is subject to particular forms of accountability). We may see this as a response to the deep uncertainties surrounding the procurement of organisational technologies that seem to be compounded by the growing pace and increasing organisational significance of technological change.
The need to address multiple historical timeframes
A corollary of our insistence upon the need to examine socio-technical change at multiple levels of generality, in terms of addressing immediate contexts of action and broader contexts, is that we need to consider socio-technical processes temporally, in terms of:
1	the unfolding of multiple histories; and also
2	the different historical timeframes around which an object, event or activity may need to be analysed.
Multiple histories and timeframes are intrinsic to our attempts to capture the evolution of a new technology, addressing, for example, both its development and its adoption. In this way we seek to capture the complex set of developments taking place across a variety of locales, encompassing both the ‘local’ context of immediate action and interaction and its patterning by a broader context. This broader context is constituted by the aggregate outcomes of previous actions which, in turn, provide a less-readily negotiable set of factors that frame and pattern outcomes and which need to be analysed over longer-term timescales.
It is important to pay attention to the multiple dynamics and timeframes surrounding innovation. We have noted that the dynamics of technology development and appropriation may differ. For example, in the case of ICTs, where development cycles may have shortened to a year or two, appropriation cycles may be an order of magnitude greater, with new consumer products taking decades to diffuse into widespread use and having greater longevity (Williams et al. 2005) (though both timeframes are becoming shorter). This longevity in appropriation and replacement cycles is particularly marked in the case of organisational information infrastructures such as ERP.
Particular episodes form part of multiple histories. Thus the implementation of a technology constitutes a moment in the history of a particular Company Social Constitution (Clausen and Koch 1999). It is also one of a number of sites of implementation of a particular supplier offering, contributing through its innofusion and appropriation to the further elaboration and wider adoption of that specific artefact. And that specific story in turn forms part of the evolution of the class of artefacts with which the supplier offering is associated. We have coined the concept ‘biography’ to refer to this history of relationships and sites implicated in the evolution of a specific artefact and a class of artefacts. And the latter can, at a more general level, be seen as a phase in the development of organisational technologies more generally. In the last three instances, the specific history is nested inside another more long-term generalised set of relations. However, a technology implementation can also be seen as the linking together of two specific histories that may not have been previously conceptualised together: the Company Social Constitution of the organisational adopter and the biography of a specific artefact.
In theorising the multiple tempos that we may need to address in analysing particular episodes, we find considerable merit in the framework articulated by Hyysalo (2004). He draws on Hutchins’ (1995) study of how quartermasters learn naval navigation in a system of distributed action, which portrays the simultaneous unfolding of different histories: ‘any moment in human conduct is simultaneously a part of the unfolding of a task, the development of the individual doing it, the development of the work community, and the development of the professional practice’ (Hyysalo 2004: 12).
Hutchins’ cube (Figure 3.2) represents these as different speeds of change within a single moment of practice (rather than portraying these as separate levels).
Hyysalo introduces us to attempts within Activity Theory to characterise timescales for analysing social and technological development.15 His study of the development of new healthcare technology highlights three key timescales in the coupling of design and use:
1	the prevailing ways of organising design and use in industrial production. Hyysalo refers here to features of the innovation system liable to be stable over many decades: ‘pervasive and relatively slow changing ways in which design and use are generally organized in industrialized countries’ (Hyysalo 2004: 13);
2	the coupling of a technological field and a societal practice, which he sees as relatively stable institutions, potentially stable over years and decades, though noting the possibility of changes in practices, in technologies and in the ways these are coupled together; and
3	the development of a particular innovation and the organisations and people connected to it.
Figure 3.2 Hutchins’ cube.
Source: Hutchins (1995: 372), cited in Hyysalo (2004: 12).
We can adapt this schema to our own analytical concerns. Hyysalo’s longest timescale (1) prevailing ways of organising design and use would perhaps correspond in our study to the resort to packaged solutions for organisational technologies. Our concept of biography would encompass his other shorter timescales: (2) the technological field which corresponds to the biography of a class of artefacts (e.g. ERP systems in general); (3) and the development of a particular innovation to address the biography of a specific artefact (e.g. SAP’s R/3 system).
The comments we made earlier, in discussing viewpoints and research design, about different ways of slicing through the complex social space represented by the agora, depending upon our location/orientation to it and our concerns, also apply to the historical framing and timescales of our research. Such choices about the temporal framing of enquiry have important implications for what may be viewed. For example, local studies of immediate settings of action inevitably draw attention to the scope for discretion (such as ‘user work-arounds’) but provide a poor vantage point for exploring longer-term processes of technology-organisational alignment (for example, around common business process templates within enterprise systems). This may need to be captured by other modes of research (for example larger-scale surveys or longitudinal studies).
Rather than invoke one modality of research, our approach seeks to retain awareness of the multiple historical registers that surround a particular
phenomenon. The choices we make regarding which timeframes and historical registers are to be centrally addressed parallel our earlier discussion of choices regarding the adoption of a local or of a more global gaze. While the agora concept provides tools for looking at social space, the temporal distribution also needs attention.
We are minded here of the critique of constructivism made by Kallinikos (2004a: 12) on the grounds that the ‘study of technology and its social impact cannot be exhausted at the very interface upon which humans encounter technology. Essential strips of reality are not observable.’16 Kallinikos is highlighting issues of social structure, of particular relevance when we consider technologies that typically come to us as the result of a more or less elaborate (occupational, organisational and industrial) division of labour. If we are to address the material character of artefacts, many elements are developed at a remove (socially and temporally) from their sites of implementation and use and are not under the control of actors in user locales. This observation can also usefully be applied to the existing institutional context that provides resources and sets constraints for local action.
We are proposing a relational approach that brings to the foreground certain features for detailed analysis - but within a broader historical register that also records other levels of generality and tempi.17 Our work seeks to find ways of probing and addressing these other levels/tempi through the adoption of a complex methodology. We contrast this, inevitably messy, endeavour to other dominant social scientific research approaches which recognise only a single register for analysis (whether of immediate action or of broader structuring). We see this failing, for example, in economic accounts which are founded upon conceptions of human behaviour that are demonstrably flawed, and also in the ‘atomistic individualism’ which characterises much recent work from a constructivist background which only recognises immediate contexts of action. We contend that this yields an unhelpful reductionist account of complex social processes.
Rather than propose a particular level of analysis, we emphasise the benefits of multi-level analyses, which may have different depths and centres of focus depending on the issue under analysis. The particular scope and framing of analysis selected depends upon the matters under examination. For us the matter of research design and epistemology should be driven by a critical reflection about which (spatial/temporal) slices of complex technosocial fabric are brought into the centre of our analytic gaze by particular modes of research and from what viewpoints.
Multiple methods
We propose the concept of biography as an instance of a ‘variable research geometry’ that can be applied to diverse issues and in differing contexts, depending in particular upon what issue(s) are being addressed and which
entities are being tracked. The biographical approach focuses upon social (or rather socio-technical) processes involved in innovation and how these are shaped by their context and history. Many kinds of biography are thus possible. For example we could address the biography of an organisation -indeed, this is how we understand Clausen and Koch’s concept of Company Social Constitution (Clausen and Williams 1997; Clausen and Koch 1999) -the biography of an occupation or, indeed, the biography of a managerial innovation such as BPR or TQM (see Mueller and Carter [2005]).
Our concern here is to understand the biography of an artefact which may be conceived narrowly in terms of the development/implementation of a particular innovation, or more broadly of a class of artefacts, or of a technological field and their complex couplings with social institutions, actors and practices. This has been the (often tacit) objective of a diverse array of Social Shaping of Technology studies. These have deployed various research geometries in terms of the historical scale and the level of generality of the phenomena under study. However, what is at stake here is not only a matter of temporal and social framing - of zooming in and out, to use our photographic analogy - but also involves important choices also in terms of the methods and concepts deployed and the relationship of the study with existing knowledge. Multiple methods may be required, knitting together different kinds of evidence including historical studies, ethnographic research, qualitative studies of local and broader development and the use of larger-scale research instruments and quantitative data. These differing kinds of evidence have differing strengths and contributions to mapping the dimensions of an issue. For example, local qualitative research may provide better tools for drawing out intricacies and particularities of social process and is particularly pertinent to exploratory research opening up new understandings of novel and emerging phenomena, while larger-scale research provides a more effective base for addressing regularities and trends as well as for testing hypothesis and models and confirming findings from exploratory qualitative studies (MacKenzie 1988). It may be further adduced that combinations of different kinds of evidence are liable to produce more robust and richer understandings.
Multiple theoretical orientations
As well as proposing a ‘variable geometry’ in relation to the temporal and technical/societal framing of research, we argue for a certain level of critical eclecticism in relation to broader worldview, and the theories and concepts that inform it. Of course, theories and methods cannot simply be combined on a pick-and-mix basis; they are underpinned by different and often incompatible presumptions and tools. Though some have interpreted this truism as constituting a case for sectarian theoretical purity, we suggest a different response. We argue instead that we can interrogate differing analytical traditions in terms of their robustness and applicability to the
phenomena in question and their compatibility with other perspectives; we can reason and make judgements about these questions.
Though informed by our close association with the Social Shaping of Technology and social learning perspectives, particularly in our emphasis upon material and social structural influences, the biographies approach is not ‘hard-wired’ to a specific theoretical perspective. And many of the schools and analytical currents within STS have common and convergent concerns (Williams and Edge 1996).
We contrast our analysis with the widespread espousal within current STS of what we may call the ‘atomistic interactionism’ in many explanations of the world with roots in social constructivism and phenomenology, which see the world as constructed and reconstructed anew in sites of everyday action. A similar analytical consequence arises from the rejection by ANT of explanation in terms of the operation of broader social structures, accompanied by their rejection, as unwarranted generalisation, of social scientific theories regarding the operation of these structures. Though appealing, these exemplify what could be described as the ‘fairy cake theory of the universe’. This theory was propounded in Douglas Adams’ (1979) now classic Hitchhiker’s Guide to the Galaxy ‘trilogy’, such that
since every piece of matter in the Universe is in some way affected by every other piece of matter in the Universe, it is in theory possible to extrapolate the whole of creation - every galaxy, every sun, every planet, their orbits, their composition, and their economic and social history from, say, one small piece of fairy cake.
(Adams 1980: 198)
In place of basing their choice of research setting and methodology upon social science theory, these actor-centred accounts generally resort, as noted in Chapter 2, to a ‘naturalistic’ (or perhaps empiricist) approach; seeing society constituted in the observable actions and interactions they study. For example, ethnomethodological research is founded on the view that the social order is constituted in social interactions: it selects particular sites of interaction for study and presumes that the relevant social relations will be present or represented there (Lynch 1993). Studies of particular sites and settings of action, what we may call ‘flat ethnography’, encounter the problem, that Kallinikos (2004a, 2004b) also identified, that many issues regarding the material character of artefacts are determined outside the setting of technology adoption (including the availability of technologies as well as the institutional context which provides resources and sets constraints for local action). Perhaps as a result, ethnographic researchers frequently have the sense of not being in the right place or at the right time (Law 1994; Magolda 2000). One temptation faced with this incompleteness of vantage point is to elevate the importance of the particular settings and interactions studied. This could be exemplified by workplace studies of technology that
present organisational information and communication processes, including the appropriation of IT, as of paramount importance, and correspondingly neglect technology design and other distal processes. We would instead propose an alternative solution involving what we describe as ‘strategic ethnography’, addressing multiple sites, selected according to the matter in hand based on our preliminary knowledge thereof. Such an analytical move requires researchers to explicitly recognise and make accountable the strategic choices involved when deciding upon the location and boundaries of ethnographic work. It would in turn require reflection upon the theoretical commitments and presumptions that inform these choices (rather than pretend that it is possible to avoid such choices, for example by empirical sensitivity). Some strands of ethnographic research, particularly associated with Ethnomethodology (see, for instance, the articles with Button [1993]), have difficulties in acknowledging such an approach (which, we would argue, is however more in line with the anthropological tradition).
ANT, with its nostrum of ‘following the actor’, does not limit itself to particular settings, but accepts that research involves making strategic choices about which sites and people should be tracked. It justifies these choices, however, in terms of empirical outcomes; in this sense ANT claims to be able to see ‘where the action is’ (Latour 1987). However, ANT does not provide tools to guide those choices or make them accountable. This claim to be able to resort to a naturalistic method leaves ANT open to criticisms of empiricism (Russell 1986; Russell and Williams 1988). Moreover, a multiplicity of accounts would be possible from different perspectives; any ANT account of necessity involves choices about which actors and perspectives to foreground (Sorensen and Levold 1992). Since ANT has rejected other theoretical knowledge, these choices are made based on largely unacknowledged presumptions (though see Law 1991) and commonsense knowledge.
What is at issue here is a particular orientation to theory with which we differ. Across the social sciences, we can find a spectrum of styles and approaches to theorisation, between work that in its insistence upon particular theoretical and methodological approaches becomes purist, and more eclectic approaches.
Analytical purism has attractions. It is easier to justify the particular methodology and epistemology within a well-honed worldview. However, this strategy also brings weaknesses and intellectual rigidities. In particular, such purism typically involves a process of simplification of the object of study (as we saw, for example, in Chapter 2, in the construction of disciplinary domains in which phenomena were conceived in narrowly ‘economic’ or ‘technical’ terms). This has unhelpful consequences, particularly where it leads to the exclusion from consideration of other forms of knowledge, theoretical and methodological tools and empirical evidence. As a result, it generates accounts of the world that do not match the complexity of issues under examination. Of course, this kind of reductionist analytical strategy has been extraordinarily successful, particularly in science and
engineering. Within the social sciences, perhaps only Economics and Psychology have succeeded in replicating the reductionist disciplinary strategy of the natural sciences. Processes of disciplinary specialisation within other social sciences have more generally involved the formation of specialised schools of analysis rather than cohesive broader disciplines. The formation of these specialised schools has facilitated rigorous development of analytical concepts and instruments - but, we would argue, at a cost of narrowing the frame of enquiry. In particular, we note the tendency for a single level/frame of analysis to predominate. This is perhaps most evident in relation to what is perhaps the key social scientific debate - between explanations of the world in terms of action and in terms of structure - where social sciences tend to coagulate into either action-centred or institutional-centred accounts.
There have at the same time been movements against theoretical purism in the social sciences that have led some to embrace thoroughgoing eclecticism. However, as the rather disappointing achievements of ‘systems theory’ indicates, this strategy has been demonstrably unsuccessful. Much of this work is marked by a loss of rigour in relation to the development of concepts and tools (Hoos 1973; Keat and Urry 1975). We therefore do not propose a retreat from theory. Understanding in social science proceeds by advancing competing interpretations of a complex and interconnected world. Theory is needed to provide critical insight. Atheoretical approaches perennially threaten to become overwhelmed by diversity - lacking the theoretically informed tools to help the analyst impose order on unruly reality (put boundaries around problems, sort out and rank diverse potential influences, select sites for examination, and so on). In consequence, we would argue, they end up with empiricist and descriptive accounts; and they are hard pressed to extrapolate or draw general lessons for practice. A clear theoretical perspective seems to be a prerequisite for effective social scientific enquiry.
Our work, and our view of the biography of artefact perspective, is rooted in and inspired by STS, most immediately social learning and social shaping analyses, but also deeply influenced by writings from ANT. However, we differ with the latter’s rejection of existing social scientific knowledge (despite the articulate defence of this approach found recently in Latour [2005]). What we are proposing is not just an ‘in-between’ position - balancing between eclecticism and theoretical purism - but rather a different relationship to theory.
Our approach to understanding the Biography of Artefacts Framework addresses the technology-society relationship at multiple levels and timeframes and also acknowledges the multidimensional character of these phenomena and thus the potential pertinence of analyses of these phenomena from different (technical, economic, etc.) analytical perspectives. The analysis of the biography of an artefact, by acknowledging these multiple dimensions of the phenomenon under study, brings the researcher into contact with
other areas of (social and technical) knowledge that are pertinent to the questions under examination. A multiplicity of theories and methods may therefore be pertinent. Acknowledging this point does not, however, answer the question of how to bring them together. As is clear from the above, there are dangers in eclecticism.
What is at stake here is the question of how we relate to (heterogeneous) theory. This is a question of whether social science theory is to be used as an analytical machine or as a tool for understanding.1,8 Rather than admitting all knowledge claims as being of equal validity, we retain our STS perspective as our core analytical commitment. At the same time, we can hold a range of other findings as potentially pertinent forms of background knowledge that can inform a particular study. Social science theory and methods do not constitute some kind of analytical machine (you turn the handle and out come facts).19 Instead, these bodies of knowledge provide potentially valuable resources, sensitising and guiding the analyst. Rather than rejecting and ignoring other forms of knowledge (as espoused, for example, by ANT and some related approaches), or accepting that somehow ‘anything goes’, we can make some assessment of the relevance, robustness and pertinence of these theories. We can critically examine the presumptions underpinning these concepts and methods and the available evidential base, in terms of their consistency/compatibility and their applicability in other contexts or for other analytical purposes. For a particular analysis it is necessary to develop an analytical framework that advances our own understanding and acknowledges other areas of relevant knowledge. Rather than resort to a single-purpose analytical schema, in the way propounded by ANT or Ethnomethodology, we suggest that it is necessary to argue for the adequacy of the methods and concepts deployed according to the issue and phenomena in hand.
Anthropological perspectives on artefacts
In a moment we shall explore how our biographical perspective can be applied to understand the history of ERP, but before we do this we want to discuss some links with other fields which also share our interest in investigating technology over longer timeframes. This includes work on the social and cultural history of technology consumption (e.g. Marvin 1988; Pantzar 1997) and by cultural and economic anthropologists of technology (e.g. Appadurai 1988; Kopytoff 1988; Thomas 1991). The latter have coined a parallel usage of the term ‘biography of artefact’ in a discussion that we find informative but which differs from the aims of this book in important respects. The most well-known contribution to this strand is, perhaps, Kopytoff (1988), who argues that an object cannot be properly understood at only one point in time. Rather, we should look across its whole life-history to analyse its production, consumption and circulation. Just as we study the life course of people, we should also do the same for technologies. In the same
way as the status of people changes over their lifetime, so does the status of an artefact. Yet while Kopytoff sets out these general aims for uncovering how objects ‘accumulate histories’, he is in fact interested in objects for much more specific reasons: to explore how they mediate social relations in particular settings and what they reveal to anthropologists about those relations and places. He writes:
Biographies of things can make salient what might otherwise remain obscure. For example, in situations of culture contact, they can show what anthropologists have so often stressed: that what is significant about the adoption of alien objects - as of alien ideas - is not the fact that they are adopted, but the way they are culturally redefined and put to use. The biography of a car in Africa would reveal enormous amounts of information about the relationship of the seller to the buyer, the uses to which the car is regularly put, the identity of its most frequent passengers and those who borrow it, the frequency of borrowing, the garages to which it is taken and the owner’s relation to the mechanics, the movement of the car from hand to hand over the years, and in the end, when the car collapses, the final disposition of its remains. All these details would reveal an entirely different biography from that of a middle-class American or Navajo, or French peasant car.
(Kopytoff 1988: 67)
As we see it, Kopytoff’s conception of the term ‘biography’ is useful for the following reasons. First, he is interested in how artefacts are transferred from one place to another, and denotes how they are ‘alien objects’ when they arrive in their new settings. Though he is also careful to point out that even though it is ‘alien’, the object is not imposed on actors since it is typically adapted and redefined according to the needs of each new place. This has obvious parallels with the STS and social learning work reviewed previously. Second, it focuses on how various communities in domesticating these artefacts leave ‘traces’ in the sense that they shape the object in some form (though he falls short of suggesting that the users of the artefact actually contribute to the production of the object, which is what we are suggesting with the development of software packages). Third, the current significance and meaning of an object today is always related to the settings and communities to which it was once connected. Finally, the same objects would have different biographies depending on the range of settings through which they travel.
While this form of analysis might be applied to the study of ERP systems - and we have attempted an initial analysis using this framework (Pollock et al. 2003; Pollock and Cornford 2004) - we think there are limitations with the term as it is currently conceived within Anthropology. Kopytoff’s focus is principally on the significance and meaning of an artefact and how this
changes through its career. It is the multiple ways an object is viewed and understood by the different communities who consume it that make up its biography (the more and varied these meanings, the more ‘eventful’ the biography). By contrast, while we think ‘meaning’ is important in the context of generic packages we also want to show how systems are not only symbolically but also materially changed over time. A system like ERP, as we have already described, is a heterogeneous assemblage where the various affordances built into the technology are constantly developing and evolving. In addition, while reading Kopytoff’s work, it seems that the biography of his artefact turns out to be the story of a highly bounded object.20 As he describes it, the notion of the ‘artefact’ is the circumscribed one common to Anthropology and not the messier notion of artefact found with STS. In this respect, echoing the point above, enterprise-wide solutions are not material artefacts in Kopytoff’s narrow sense but can more usefully be thought as a heterogeneous assemblage (Koch 2007). Thus, we intend the biography of an artefact in this wider more encompassing sense.
Finally, we are not wholly convinced the most useful way to study artefacts is solely at the place where the user encounters them. Nor are we convinced that these encounters tell us much about the wider contexts in which the objects are situated (which is Kopytoff’s intention). As we have suggested, local studies of adoption offer an inadequate lens for exploring the longer-term development of a complex organisational technology like ERP. Contrary to what Kopytoff suggests, extrapolating from the specific object to the wider context can only ever be done in quite general ways. Rather than study the variety of ways in which the same object is adopted by different groups of users (these ‘horizontal biographies’, if you like), we are interested in the production of ‘nested biographies’. Local adaptations, improvisations and work-arounds, etc., should be seen as one moment in the biography of a specific object, which go on to form part of the evolution of the class of artefact, which in turn is part of the history of the resort to packaged solutions. Arguably, through moving out from the specific user’s interactions with a specific object in this way, the technology and society relationship is bridged in a more sophisticated way.
However, there are other aspects to Kopytoff’s approach that we find more useful, such as his discussion of ‘commodities’, for instance, where he gives particular attention to the movement of an artefact between different economic states (see also Appadurai 1988). It was the way in which an object could be ‘commoditised’, ‘decommoditised’ and even ‘recommoditised’ that Kopytoff found interesting. Indeed, it is the movement of objects between different commodity states that constitutes the biography of a technology (Dant 2001). He cites the example of one of the most common and exchangeable of technologies, the mass-produced car, and notes the process by which some vehicles, as they grow older, may develop into ‘vintage’. In so doing they become ‘unique’ and no longer a straightforwardly exchangeable commodity (a process Kopytoff describes as ‘singularisation’):
In the homogenised world of commodities, an eventful biography of a thing becomes the story of the various singularisations of it, of classifications and reclassifications in an uncertain world of categories whose importance shifts with every minor change in context.
(Ibid.: 90)
Now it is clear an object might acquire new meanings (or further develop its biography) in other ways than through exchange (Dant 2001), but for Kopytoff it is the economic life of an object that has particular significance. This is what he intends when he talks of things as having an ‘eventful biography’, in that artefacts are not always commodities but can (and often do) move between different states (thus the commodity phase is only one part of an object’s social life). Interestingly, there is a somewhat similar discussion by the economic anthropologist Thomas (1991), only he reaches somewhat different conclusions. He is also interested in the process of commodification, but rather than simply signifying one more stage in an object’s biography this is concerned with detaching an object from its history. For this reason in his book Entangled Objects he describes commodification as a process of ‘alienation’:
Commodities are here understood as objects, persons, or elements of persons, which are placed in a context in which they have exchange value and can be alienated. The alienation of a thing is its dissociation from producers, former users, or prior context.
(Ibid.: 39)
His definition is interesting to us since it highlights the possibility that while commodification/alienation can occur there are sometimes opposing forces present; for some objects, ‘disentangling’ can never fully occur. The notion of ‘entangled object’ suggests that some artefacts are inextricably connected to their place of birth. To exemplify this he contrasts those objects that have little difficulty in travelling - i.e. those things that were specifically created for exchange - with those that have become so entangled with previous owners that there is the impossibility of exchange. There is, he writes, ‘[v]ery close association between people and some particular objects’ such that the object ‘may not be transmitted at all’ (ibid.: 72-3).
This strand of work is relevant because software packages as a class of technology are, as we have already mentioned, not simply born ‘commodities’ or as ‘generic’ products but only become so through various complex processes, one of which is that software package vendors work to ensure that their systems are never too much like the place(s) for which they were built and never too connected to specific people or events. Their fear is that if the package becomes overly connected to, or identified with, these places then it will weaken the potential/ability of their systems to move beyond these places/settings. Thus, one of the things they do is to actively manage the biography of their system: this is the history of relationships and sites
implicated in the evolution of a specific artefact, so that they avoid ending up with an ‘entangled artefact’.
Reinterpreting ERP through a biographical lens
The starting point for this focus on biographies was the observation by Edinburgh scholars past and present (Fleck et al. 1990; Fleck 1993; Webster and Williams 1993; Pollock et al. 2003) that workplace technologies were often condensation of existing work practices, coupled with a view of achievable change geared towards current conceptions of best practice. In other words, information systems were not extrinsic developments coming from outside the industry but at least in part were intrinsic developments. This was obviously true in relation to the earliest phases of process innovation that arose within the ‘user organisations’, for example in the Industrial Revolution (Rosenberg 1976) and in the earliest stages of the application of computing (von Hippel 1994). However, it continued even after a specialist supply side had emerged, which continued to be linked to the user, inter alia through the implementation process. Building upon these debates, Brady et al. (1992) suggested that packaged software artefacts had biographies. Williams (1997) applied this concept to analysing the evolution of CAPM, describing its historical evolution from a family of artefacts (MRP, MRP II, etc.). And Pollock (Pollock and Cornford 2004) later used a version of the same argument to study the transfer of ERP across other sectors.
In this early work, while the influence of the institutional setting was highlighted - including the role of professional associations and of public policy in promoting ideas of best practice (Webster and Williams 1993) - our initial explication of the biography framework did not include a comprehensive set of conceptual tools for analysing the social fabric beyond the supplier-user nexus. And in this respect some valuable further work has been undertaken elsewhere, for example, by Swan et al. (1999), who noted how national differences in the structure and operation of professional associations had consequences for the uptake and manner of utilisation of these technologies. Clausen and Koch (1999) likewise drew attention to the fragmentation of the supply-user nexus into a number of distinct segments with relatively stable linkages between suppliers and users within segments. However, the challenge now is to theorise in more detail the structuring and operation of this institutional setting.
This is what we will attempt to do now in the case of the emergence of ERP as a field of technology where we will reinterpret its history through the lens of our biographies framework.
Understanding the evolution of technological fields: the history of ERP
We have already suggested that the concept of biography could be applied to analyse (in Hyysalo’s [2004] terms): the development of a particular
innovation (and the organisations and people connected with it), and the coupling of a technological field and a societal practice. In relation to the former, this may encompass the evolution of a particular supplier offering as well as particular episodes of its design, implementation and use. In relation to the latter, the biography concept can be applied to explore the operation of relatively stable institutions over a period of years and decades in the emergence and evolution of particular technological fields. We can explore this in relation to packaged organisational software - which became known as ERP. Koch (2007: 428) reminds us that ‘ERP ‘systems” need to be understood as heterogeneous networks, assemblages of human and material elements’. The concept of biographies lets us explore the historical emergence and evolution of this heterogeneous assemblage.
We briefly reviewed in Chapter 1 the extended biography of ERP which can be traced back to early 1960s stock control systems in vehicle and aerospace production. ERP, like many other popular technologies, has a remarkably well-rehearsed history, characterised by a clear succession of predecessors with their own acronyms (though there is less agreement about the distinctions between these stages).21 We found a process of incremental development of the artefact and conceptions of its business application, punctuated by more radical changes, with discontinuities often loosely associated with changes in terminology. We will discuss the role of a technology name in more detail below. These labels refer not to specific homogeneous artefacts but to a more or less heterogeneous collection of artefacts (software, management techniques) which link a community (or, rather, several overlapping communities) of suppliers, intermediaries and adopters.
It is instructive to focus on these discontinuities - and changes in designation. They do not reflect simple ‘technical changes’, though they are often associated with changes in the underlying technical architecture. They are also associated with changes in overarching paradigms for business improvement. Perhaps the key periodic driver of a change in name arises when suppliers decide to port new developments to different technical architectures - and need to sell them to their customers. Perhaps the key event in the evolution of ERP from its predecessor, MRP II, was the 1992 launch by SAP of its R/3 product based upon client-server architecture. The current debate about the future of ERP (and the idea of extended ERP or ERP II) revolves around novel technology architectures based around the internet and web-service architectures, though also accompanied by a shift in views about the search for competitiveness from the enterprise to the value-network. Software suppliers and management consultants (and latterly, commentators and business analysts, as coordinators of community expectation) appear to exercise particular influence over these changing prescriptions. A combination of factors thus appears to be at play in these shifts. We shall explore the operation of these factors in more detail by revisiting the historical development of ERP and its predecessors.
The origins of Materials Requirements Planning (MRP): techniques, technologies, communities
Swan et al. (2003) and Jacobs and Weston (2007) have analysed the origins of MRP in a small group of US industrial practitioners and academics with a background in operation research techniques and interested in applying these in manufacturing organisations through computer-based systems. These accounts highlight the coming together in 1966 of three individuals -George Plossl, Joe Orlicky and Ollie Wight22 - who collaborated in developing the conception of Materials Requirements Planning (MRP) and later became widely known as ‘MRP gurus’. Initially their emphasis was on the application of production planning techniques within firms rather than software - the systems depended upon computers but were developed in-house or provided as bespoke systems, building upon existing stock control systems developed in the manufacturing of complex assemblages in vehicles and aerospace. This informal community (primarily of management specialists, connected to or employed by firms adopting MRP techniques) had links with IBM, which invested in the development of solutions resulting in 1972 in a successful packaged solution. This was the Communications Oriented Production Information and Control System (COPICS), designed to run on their new IBM Model 360 series mainframe computers. Specialist knowledge was codified, for example, through COPICS handbooks and commodified not just in artefacts but also more saliently in consultancy. The shift to packaged solutions was less marked in this period, however, than the movement towards the education and professionalisation of managers and consultants (Mabert 2007), subject to some certification and quality control procedures. Ollie Wight, for example, set up a network of ‘Class A’ companies and consultants applying his concepts.23
While the early stages of these innovations were characterised by opportunistic encounters, later developments were pursued in a more organised manner (Swan et al. 2003). The three MRP gurus became the core of a broader network and in particular promoted their ideas through the American Production and Inventory Control Society (APICS) through what they described as an ‘MRP Crusade’ (Clark and Newell 1993; Swan et al. 2003; Jacobs and Weston 2007).24 In that decade today’s major ERP providers were established including SAP (1972), J.D. Edwards and Oracle (1977), the Baan Corporation (1978) (Jacobs and Weston 2007) and a host of other providers which no longer exist today. APICS strengthened itself, building its membership significantly through the MRP campaign - and we see the development of the technology and of its supporting institutions proceeding hand in hand (Swan et al. 2003; Jacobs and Weston 2007). This is linked with a concerted effort of education and professionalisation of production management, through the production of textbooks and courses (Mabert 2007).
It is instructive to note that, in the first phase of MRP, the main institutional repositories were practitioners: user organisations, management
professions and professional associations. A market was also being built for knowledge-based products. We also note a pattern familiar from other innovations: the establishment of a division of expert labour and the partial convergence of knowledge in specialised supply. During the 1970s, we see the increasing influence of management consultants and of technology suppliers (and by the 1990s educationalists, suppliers and consultants made up over 40 per cent of APICS members [Clark and Newell 1993]). The MRP crusade seems to have been successful. By the early 1970s only 150 US companies were using MRP techniques, a figure which rose to 750 by the mid-1970s (Swan et al. 2003) and which by the 1990s had risen to over 60,000 (Mabert 2007).
From MRP Materials Requirements Planning to MRP II Manufacturing Resource Planning
SAP launched its highly successful R/2 software in 1978 (Jacobs and Weston 2007). New systems emerged that could run on cheaper minicomputers, such as the IBM System 38, which were affordable by smaller firms. In the early 1980s, the progressive extension of the functionality of Material Requirements Planning systems inspired a search for a new name. Jacobs and Weston (ibid.: 360) describe how:
Ollie Wight began calling these new systems ‘Business Requirements Planning’ only to find that this name had already been registered as a trademark. So he referred to them as ‘MRP II’ systems, which by the late 1980s, was ‘translated’ as ‘Manufacturing Resource Planning’ to distinguish this new capability from the original, simpler, system.
An alternative account suggests that Wight was happy to stick to the MRP acronym with which he was closely associated.25 However, MRP and MRP II are not wholly distinct: at ‘the heart of any MRP II system was the fundamental MRP logic, now typically re-written in modern code’ (ibid.: 360),26 and there was often ambiguity in discussions as to what was being referred to - MRP or MRP II.
Computer-Aided Production Management: the vision that was not sustained
In the late 1980s, we find a new terminology being introduced in the UK: Computer-Aided Production Management (CAPM), kicked off by a report published by the Institute of Production Engineers (Cork 1985). The ACME Directorate, the Science and Engineering Research Council’s Application of Computers in Manufacturing and Engineering Directorate, subsequently launched an Initiative in CAPM Research (Waterlow and Monniot 1987). In this period, MRP II systems were being strongly promoted (for example by
BPICS, set up as the British subsidiary of APICS and strongly influenced by vendors) and were being implemented in medium-sized UK firms, with mixed results. In this context, we see attention directed towards overcoming some of the shortfalls of existing technology. Chief among these were the complexity and inflexibility of MRP II systems, which made them difficult to implement successfully, particularly for smaller firms which were very different in their production environment and management systems from those in which MRP had emerged. There was also a debate about the appropriateness of these tools as vehicles for pursuing the current vogue for flexible Japanese-style Just-In-Time systems (Clark and Newell 1993; Webster and Williams 1993; Swan et al. 1999). Though MRP systems were designed to operate in stable and predictable batch-manufacturing environments, there was an attempt to offer these as vehicles for Just-In-Time even though these were based on very different principles and approaches.27
The availability of research funds and other support attracted a range of suppliers of MRP and MRP II and related systems, which were motivated to rebrand their diverse offerings as CAPM. The CAPM terminology acquired a certain stability in academic writings, particularly in the UK and in some associated research centres; but did not catch hold in the USA and was not sustained.28 In 1990-1, when our investigation of CAPM was concluded, there was no clear sense of the future of MRP.29 As suggested at the outset of the book, the keynote paper for a high-level European Workshop on the future of MRP (Wijngaard 1990) noted three competing scenarios: the evolution of MRP offerings by current large suppliers; partnerships of MRP suppliers and users; and the emergence of factory management systems offered by systems integrators rather than today’s generic MRP suppliers. The workshop emphasised reducing software complexity for the user (not necessarily for the designer) through context specific systems.30 Though there was no clear vision in 1990 of where MRP was going, in the aftermath of SAP’s launch of R/3 only two years later, we find the sudden and all-encompassing resort to the alternative terminology of Enterprise Resource Planning developed by Davenport and industrial analyst Gartner (Lopes 1992).
From Manufacturing Resource Planning (MRP II) to Enterprise Resource Planning (ERP)
The factors underpinning the shift in terminology from MRP II to ERP seem rather amorphous. On the one hand, there was the widely expressed dissatisfaction with earlier MRP II systems regarding the shortcomings identified in discussions of CAPM (Maskell 1993). On the other, early ERP systems were not distinct from MRP II. Gartner’s key paper, which coined the term ‘ERP’ and proclaimed it as the new paradigm, was entitled: ‘ERP: avision of the next-generation MRP II’ (Wylie 1990).31 Definitions of ERP, despite the ambiguities we reviewed in Chapter 1 (Klaus et al. 2000), generally emphasise the addition of Accounting and Human Resource
Management functions on to the core MRP II systems (see, for example, Chung and Snyder [2000]). However, these were already a feature of MRP II systems in existence and being implemented in the early 1990s (Kampf no date). These extensions did, however, take these systems beyond the manufacturing process; and the Manufacturing Resource Planning terminology no longer seemed appropriate.32 Perhaps the one clear technical feature is the adoption of client-server architecture offering cost and implementation flexibility advantages (Lopes 1992; Knutton 1994), coupled with a high level of integration between modules whereby a transaction on one module would be accessible to all the system modules, achieved through an integrated database.33
After Gartner coined the term ‘ERP’, other players (most notably vendors and consultants) began to flesh out what ERP was and how it worked, followed by adopter accounts of the organisational benefits of its adoption (Wang and Ramiller 2004). The theme of integration and process orientation seems to have been very attractive to corporate managers, and appeared to dovetail with ideas about good industrial practice in the context of the prior espousal of Business Process Redesign, together with a view that packaged solutions represented the way to achieve these goals (Deloitte and Touche 1997).34 Wang’s (2007b) analysis of the (mainly trade/practitioner) press points to the explosive growth in discussion of ERP in 1997-9, coinciding with a corresponding reduction in papers on BPR. He concludes that the ERP community has benefited from recruiting members and attracting attention from related innovation communities hitherto looking towards BPR and MRP (see Figure 3.3).
De facto, what seems to be driving the shifting terminology and the new paradigm is the success of SAP’s newly launched R/3 system (launched in Europe in 1992 and in USA in 1995) which, along with other similar offerings, established itself as a standard (Lopes 1992; Davenport 1996; Pairat and Jungthirapanich 2005). Existing MRP II users were early adopters of these new packages, and SAP in particular became widely adopted by global organisations. Added to this, organisations such as APICS promoted ERP (Berchet and Habchi 2005). However, in contrast to the previous stages in the growth of the MRP (the so-called ‘MRP crusade’), the institutional basis for promoting ERP had already been put into place. As we saw in Chapter 1, a range of factors reinforced the adoption of ERP, leading to the explosive growth in uptake, which was initially within manufacturing, but was progressively extended to other areas as versions of these offerings were created for process industries and various service sectors.
From Enterprise Resource Planning (ERP) to Extended Enterprise Resource Planning (ERP II)
While the shift from MRP II to ERP appeared seamless, its further development has been subject to much debate. We already noted how a number of interlocking factors have underpinned calls for radical repositioning:
Figure 3.3 Article counts of old innovation concepts and ERP. Source: Wang (2007b).
1	technology push: in the form of radical new technical concepts; in particular of web-service architectures, and software as a service, and component-based architectures (Kumar and van Hillegersberg 2000; Markus et al. 2000);
2	market pull: the saturation of its core markets provoked a search for additional markets (notably in smaller firms for whom ERP had been too expensive and cumbersome) and for new value opportunities in linking ERP with related technologies concerned with decision support, customer relationship management, and supply-chain support; and
3	new business concepts: notably the imputed shift from an enterprise to a value-chain perspective (Moller 2006).
Industry analyst Gartner declared ERP dead in late 2000 and proposed a wholly different vision under the label of extended ERP or ERP II. The changes involved are summarised in Table 3.1.
Gartner’s death sentence has been shown to be premature. Rather than the radical reorientation of ERP proposed by industry analysts under the label ‘Extended ERP’, we find an incremental development described elsewhere as
‘ERP 1.5’ (Judd 2006). This involves the adding on of functions rather than Gartner’s vision of a wholesale shift towards componentisation and a valuechain focus (Light et al. 2000; Jakovljevic 2001; Pairat and Jungthirapanich 2005).
Looking at this brief review of ERP and its predecessors, we note a constant pattern of incremental development, progressively extending the scope of integration, accompanied by more radical re-presentations of the technology. Though each of these transitions has been subject to historical contingencies we can see some overall homologies in these innovation processes. We note the discontinuities occasioned by a coincidence of changes in business prescriptions and in technology supply strategies (in relation to both the development of specific application elements product and in the overall technology architectures of the product). We find an interesting pattern of linkages between classes of technology/their nomenclature and managerial prescriptions of best practice and broader visions of business improvement. We can observe stable linkages, for example, between ERP and the idea of process improvement. We can also find instances in which looser, more opportunistic and ephemeral couplings are made (between JIT and CAPM or between ERP and e-business). Finally, we note the longer-term intertwining of more general conceptions of technology and organisation; above
Table 3.1 Extended ERP
Enterprise Resource Planning ERP		Extended Enterprise Resource Planning ERP II
Role	Optimising an enterprise	Optimising the supply chain through collaboration with trading partners.
Domain	Focus on manufacturing and distribution	Cross all sectors and business segments.
Function	Vendors cater for different sectors and segments	Vendors focus on providing deep functionality for selected users/segments
Process	The processes were focused on the four walls of the enterprise.	Connect with trading partners, wherever they might be, to take those processes beyond the boundaries of the enterprise.
Architecture	Systems monolithic and closed	Web-based systems able to integrate and interoperate with other systems, built around modules or components that allow users to choose just the functionality they need.
Data	Information-generated and consumed within the enterprise.	Information available across the supply chain to authorised participants.
Source: Adapted from Bond et al. (2000).
all regarding the evolving concepts on the one hand of information integration and on the other of process integration. Thus we found the linking of the idea of the flexible customer-oriented firm and CAPM and broader visions of Computer Integrated Manufacture (Webster and Williams 1993) and again today linking ERP systems and BPR.
The timelines for the evolution of ERP points to something like a ten-year cycle: a periodicity which (doubtless not coincidentally) broadly approximates to the replacement cycle for corporate information infrastructures (see Figure 3.4).
As well as these cyclical processes, there is also a clear progression in the process of building a technology and associated organisational and institutional framework. At the outset, the new institutions of information technology and business improvement were rudimentary and inchoate. The early accounts emphasise improvisation and chance encounter. As time progresses, we find the gradual laying down and elaboration of technical frameworks in terms, for example, of the sedimentation of component technologies and their incorporation into solutions. The first stock control and MRP systems were written from scratch (to work on whatever mainframes were used by the firm concerned). Later we see the recycling of code. In other words: porting functionality from the particular technical and organisational context in which it arose to other areas.35
Importantly, over the decades the institutional frameworks for promoting enterprise systems have become better established (evidenced by the shifting terminology of a crusade (1972) to a movement (2005)). We suggest that the weave of the ‘socio-technical fabric’ changes over this period from a loose and open weave to a denser and more intricate pattern. Finally, we can reflect upon changes in the processes of assessment of technologies in the course of procurement. Thus, in the 1990s, consultancy organisations were beginning to collate information about supplier offerings, while by the twenty-first century we find a much more elaborate system of consultancy and advice, and the emergence of specialist industry analysts, making
Figure 3.4 Evolution ofERP
Source: Pairat and Jungthirapanich (2005: 289).
available community experience on a more commodified basis and providing the basis for more formalised and systematised assessment of particular vendors and their offerings.
What’s in a name?
This analysis has pointed to technological continuities between ‘different technologies’ or, to be more precise, different terminologies applied to a class of broadly similar artefacts. What is at stake in these classifications and reclassifications? The name applied to a technology is far from trivial. It proposes boundaries that link a class of often quite various artefacts while differentiating them from others. As we shall see, the designation of a technology field reduces uncertainty for adopters and for developers:
1	it allows adopters to develop a generic case for particular innovation pathways (based upon an analysis of the potential performativity of that class of technology for certain types of organisational challenge), and, once this is accepted, paves the way for a comparative analysis of the relative advantages of particular offerings for their specific organisation;
2	the designation of a technology draws boundaries around a set of artefacts and their suppliers, and thereby creates a space in which some ranking may be possible; and
3	it allows developers to assess their offerings, their promotion and enhancement in relation to the features of broadly comparable products and their likely future development trajectories. In addition, we see a clustering of offerings that may serve to reinforce expectations about what functionality should be included and where the technology will go in future.
4	Fitting standard software to non-standard organisations
Recycling software
As we have already discussed, few large-scale information systems are, today, developed completely from scratch. Rather, most software applications are constructed by adapting and recycling existing packages to new organisational contexts and settings. Generic software packages, such as Enterprise Resource Planning systems, cover the fullest range of organisational activities and processes and are adopted with the aim of achieving substantial cost savings over bespoke solutions as well as improved access to ‘tried and tested’ solutions, new releases, and an opportunity to update procedures and align them with perceived ‘best practice’. However, while organisations choose packages because of their economic benefits, they are also a ‘site of struggle’ where there are various tensions between system suppliers and users as competing agendas are worked out, and where there are discrepancies between the universal logics of IT components and the specificity of user contexts. While suppliers aim to extend their solutions into as many different settings as possible, it has been pointed out these systems seldom translate easily across organisational and sectoral boundaries (Walsham 2001; Ciborra 2000). There is often a gulf between the system and the specific contexts, practices and requirements of particular user organisations.
Some of the consequences for those wishing to capitalise on the benefits of packages is that they often undergo unwanted organisational change in adapting their practices to the models of work and organisational process embedded in the software. These dilemmas are particularly acute with enterprise-wide solutions that seek new kinds of organisational flexibility and performance by capturing and integrating the full range of activities and transactions across an organisation. Despite a growing literature on the uptake of enterprise-wide systems, very little is known about how the gulf between standardised solutions and the specific contexts, practices and requirements of adopting organisations is reconciled between user and supplier organisations. In the first part of the chapter, we focus on what might be seen as an extreme example of the gulf, the application of ERP packages in universities. We discuss the various incommensurabilities that arise as a
system is rolled out and adapted for use in one particular institution (a university we are calling ‘Big Civic’).
More generally, as concerns rise concerning the incommensurability of systems and contexts, there are demands for solutions that are already partially adapted to particular business settings and for increased user-involvement in the shaping of packages. Alongside the adoption of these systems, then, there is an equally important story of innovation within supplier organisations and collaboration with package adopters as the technologies are adapted to these new contexts. In the second part, we analyse the development of new ERP functionality, a university-specific ERP module that we call ‘Campus’, which is being built to facilitate the take-up of ERP by higher education institutions. The module is being designed around the needs of Big Civic and a number of other ‘pilot sites’ around the world; and the eventual plan is to market Campus as a ‘global university product’.
How are ERP systems designed?
Over a relatively short space of time, ERP systems have become key organisational technologies - with certain suppliers being highly successful in both creating and meeting this demand. How can we begin to explain the extension of these systems to new sectors and market segments? We identify two important aspects, only the former of which has been the subject of much detailed research. These are the related processes of standardisation and ‘generification’ that surround and accompany ERP systems and other software applications. By generification, we point to the supplier strategy of taking a technology that has worked in one place and attempting to make it work elsewhere, and, in principle, everywhere. This is evidenced by the fact that the ERP package we have studied, for example, was initially conceived for and used by manufacturing firms before being applied within non-manufacturing settings and, more recently, non-commercial contexts (heath care, public sector, higher education and so on). Today, ERP systems are so widely diffused that they are now commonly described as the de facto standard for the replacement of legacy systems in medium- and large-sized organisations, and it is said that some companies find it impossible to work ‘without one’.1 The transferability of this software is possible because, unlike conventional software development, packages are designed for a market and not a specific customer. Just how software is designed in this way is the subject of this chapter and a theme to which we shall return in a moment.
In order for suppliers to reap the benefits of scale these systems must function in new settings in much the same way as they have functioned in all other settings. Several studies have considered the ‘impact’ such solutions have on organisations and how organisations often undergo expensive (and unwanted) organisational change and standardisation in adapting their specific organisational practices to process models embedded in the software, a topic about which much has been written. Davenport (1998), for example, to
quote the most cited author, discusses the case of a company with a well-established practice of giving its largest and most important customers preferential treatment, such as sending them products originally assigned to other customers. Once the new system was adopted, however, this was no longer possible as this practice was proscribed by the technology. The benefits of this strategy for software suppliers are that they have to cater for only limited amounts of variation in product maintenance and new upgrades; for user organisations the benefits are having simple and guaranteed upgrade pathways. The downsides for suppliers are foregoing the high value-added markets for customised solutions; for user organisations, the downsides stem from the risk inherent in packages that these do not adequately match their requirements. It appears that among these conflicting assessments, the claims of the package supplier generally holds sway. While marketing their systems as ‘entirely flexible’ and capable of coping with such idiosyncrasies, many suppliers actively encourage adopters to limit their attempts to tailor or modify the software by releasing upgrades and new software that are compatible only with the ‘standard system’. According to a recent study of SAP, this prescription is not only limited to such direct methods of influence but also permeates the whole ERP domain and its community of users. The systems embed established ways of being used, as well as the way the implementation is organised, all of which are reinforced textually through user documentation, visits to other reference sites, and through the ‘experience, competence and practices established in and shared by the SAP ‘development community”’ (Hanseth et al. 2001: 34).
As one would expect, however, there is no consensus about just how much (or little) customisation can be carried out. Davenport also discusses the case of Visio, a small software company with unusual methods for accounting for its revenues and inventory, and how both these ‘idiosyncrasies could be accommodated, but only with substantial extra programming’ (Davenport 1998: 152). Light (2001) similarly points out that some organisations cannot completely adopt the standard model and therefore have no choice but to attempt customisation. Other, ethnographic-based, research has attempted to emphasise how these technologies are typically ‘localised’ by adopters. Scott and Wagner (2003) in their study of a US university describe how the standard templates in the ERP package were ‘compromised’ through ‘skirmishes’ and user resistance and this allowed the emergence of a much more ‘local information system’.
One body of literature, then, tends to emphasise how most adopters end up fitting their organisation to the system (rather than the other way around), while another pays particular attention to the ‘work-arounds’ and other strategies users deploy to adapt the technology to the specific setting. A final strand has sought to reconcile these positions through emphasising how technology and organisation are often brought into alignment through a combination of quite complicated organisational change and software configuration; a process that is sometimes known as ‘mutual adaptation’.2
In general, ERP systems tends to be portrayed in the literature in one of two ways: either as potent technologies likely to transform all before them,
or as systems that work only because they are entirely domesticated by their users. It is as if for some, the wider applicability of ERP is beyond question, and for others, there is no technology or standard able to work across many sites. Even those studies that have considered packages in terms of mutual adaptation tend to emphasise one side or the other (Hanseth and Braa 2001).
While highlighting important issues, we argue that the theories and studies within the social study of computer system adoption and implementation have failed to keep pace with the challenges and dilemmas raised by the widespread extension of standardised software packages. We identify a number of shortcomings. First, much of the research is based on snapshot studies that emphasise only single phases or aspects of the software package lifecycle, and there is no attempt to follow software as it evolves, matures or crosses organisational boundaries. We have already articulated how our emerging Biography of Artefacts Framework might address this problem so we will not dwell on it here. Second, where there has been research on package suppliers, this has typically centred on work organisational issues and the management of expert labour. These include the occupational hierarchies developing between those who design packages and those who implement them (Friedman 1989), the contrasting working cultures of package and bespoke system designers (Sawyer 2000), and the difficulties of co-ordinating packaged software teams who are seldom co-located and distant from those who will eventually use their systems (Carmel and Agarwal 2001). More specifically, however, there has been little focus on how the suppliers of packages manage the tension between designing a system for a specific user and for a wider market. ERP suppliers have incentives to build systems that can be applied in the widest range of settings, for instance, and therefore design software with general or ‘ideal types’ of businesses in mind (even though no such form of organisation actually exists).
Third, there has been little focus on the relationship between supplier and user organisations, and very little is known about how package suppliers actually interact with or get to know about their users, for instance. While in the design of custom systems, close links between suppliers and users are assumed to be crucial (Regnell et al. 2001; Sawyer 2001), this is not thought to be the case with generic packages. It has been argued by Bansler and Havn (1996), for instance, that there is little interaction between these groups other than that brought about through initial procurement activity.
In terms of how vendors acquire knowledge and understanding of current and future users, then, a number of writers have offered quite general and often overly politicised accounts. Regnell et al. (2001: 51), for instance, emphasise the autonomy of suppliers and the space they have to ‘invent’ requirements before they offer them to their chosen users and markets. Howcroft and Light (2006) describe vendors very much in control when designing packages and emphasise their ability to ‘decide’ which customer requirements should be included and which should not. All this is evidence, they suggest, of the vendors’ ‘technical exercise of power’ during interactions.
Added to these there are also a number of other accounts that provide more detail (albeit somewhat speculatively) on the specific approaches adopted by suppliers. There are two principal characterisations. According to Bansler and Havn (1996), the first is where the vendor finds a ‘representative’ or ‘proxy’ organisation and then develops an initial version of the system based on the specific organisational characteristics and processes found at the site. The next step is then for the supplier to attempt to redesign the system by identifying those ‘universal’ aspects (i.e. those things that might be required by firms more generally) while coding out those more particular user features. Unfortunately, there is no detail given in the paper on how suppliers might actually identify a universal feature from a specific one. Indeed, the authors themselves, rather bizarrely, then go on to conclude that this is probably extremely difficult, if not ‘impossible to do’.
A second approach is to base understandings of the target sector not on interactions with user organisations but through compiling theoretical ‘textbook’ models of the application area. This was one of the main means by which early Computer-Aided Production Management (CAPM) packages were designed (Webster and Williams 1993). This strategy is said to have its advantages as it avoids suppliers affiliating their package with any one group of organisations, which is a problem because in the ‘proxy approach’ there is a danger the software will become too attached to these actors and therefore will not be marketable as a generic package. However, it is also one of the reasons CAPM packages were said to be so difficult to implement and use. The models of working (based primarily on conceptions of large manufacturing firms in America) contrasted greatly with the actual practices found in the much smaller and often more informally organised manufacturing firms outside the US (ibid.).
While not discounting these findings, we hope to build on this understanding though applying our framework in an attempt to trace the ‘accumulated history’ of one package and show how this history continues to influence the structures and practices of later adopters. We apply the approach to emphasise the way artefacts move around (across national borders or the boundaries of several industrial sectors) and are adapted and redefined according to the needs of each new setting. The notion of a biography - particularly the way it has been conceived within Material Culture (Kopytoff 1988) -might also be used to highlight the various relationships and meanings an artefact that is established within one community may have for actors and communities in other settings. This latter perspective has obvious sympathies with Social Worlds Theory, which has discussed how distinct communities interpret and put ‘boundary objects’ to use (Star and Griesemer 1989). It also parallels writing from Actor Network Theory, where not only humans but also artefacts and technologies are treated as actors with ‘histories’ (Latour 1999). In keeping with our approach, we also explore how end-users shape and exploit software through customisation and other strategies and how their organisational specificities influence the evolution of the software.
In this respect, the notion of biography as it is applied in this chapter shares some characteristics with Bruno Latour’s notion of chains of transformation, where changes to the technology might be seen as actors leaving behind, and the selective carrying forward of certain aspects of the system biography (ibid.). Here, we use the notion of biography to emphasise how new packages often emerge out of previous solutions that were built for others and intended for use in different kinds of settings. This in turn is generating a number of interesting issues and tensions.
One aspect of these strategies adopted by package developers has been the setting up of ‘pilot sites’ and networks of user organisations with whom they meet regularly. We are in agreement with Bansler and Havn (1996) that suppliers are likely to build up their systems around the organisations to which they have access and with which they have long-term established relationships. More specifically, we will show how in our case the supplier is designing the module around the needs of a number of ‘pilot sites’ from around the world. It is in this sense we can say that packaged solutions will usually reflect the characteristics of their previous market (these pilot sites). One of the core issues in this chapter, then, is the supplier-user nexus as revealed, for instance, by the operation of these groups. Another focus is how the development of the new module proposes a challenge for the supplier. It is moving into a wholly new area about which it has little knowledge and competence. This move into a new sector will present a number of complex dilemmas with which it will have to struggle. This includes making decisions about how much of its existing solution can be applied and reused in the new domain versus the need to recognise the diversity found here, and to include it within its generic package.
Studying a software module
We have followed the development of SoftCo’s Campus module since its inception (which is now nearly a decade ago). This chapter focuses on the initial development of the Campus module through the eyes of one of the ‘pilot sites’ involved in its development. This is a large university in the north of England, which we describe as ‘Big Civic’. Prior to the introduction of Campus, we had been conducting an ethnographic study on the implementation of a number of other SoftCo modules at Big Civic as part of their wider ERP implementation. In line with our aim to focus on the biography of such packages we include and build on insights from this earlier work (Cornford and Pollock 2003).
Background of fieldwork conducted
An ethnographic study suggests a long-term involvement in a particular field site, during which time a variety of methods are deployed to understand and participate in the relationships and activities ongoing in that setting. The aim
of this activity is to say something about the various, often tacit, ways in which the subjects of the ethnography organise their lives. We therefore studied the module as it was being planned, built and used. During both phases of the research, we employed a wide range of qualitative methods, which included direct and participative observation of various meetings and user testing sessions. Within Big Civic, we observed the weekly ‘Sponsors’ Group’ meetings where day-to-day technical and organisational decisions were made. The monthly ‘Strategy Group’ meetings were observed where issues more relevant to the future direction of the university were the focus. Project away-days, comprising mainly those technical and administrative staff involved in the actual implementation of the project, were also attended. Within the ERP supplier, one of Campus’s testing sessions was observed, where technical teams and ‘end-users’ from the various pilot and early adopter sites gathered together at the SoftCo headquarters to provide input and help shape the software. During this session, and at a subsequent Campus pilot site workshop, we met and talked with programmers and analysts from the technology supplier as well as staff from the other participating universities. We were also able to conduct a number of individual and group-based semi-structured interviews, as well as participate in informal discussions with members of the Big Civic technical team and user community. Finally, supporting material, such as meeting notes, email exchanges and reports were also collected and analysed.
Reasons for choosing an ERP package
Until recently, universities like Big Civic relied on computer systems that had been developed ‘in-house’ and that had grown over time in an ad hoc manner. These systems were typically maintained by dedicated university staff who, as the need arose, would develop new software or, as was increasingly happening, ‘bolt on’ commercial packages to meet changing institutional requirements (Bull et al. 1994). While there had been little research done on the actual usability and effectiveness of these early systems, there were numerous complaints about the quality of management information that these systems provided, particularly that which was reported upwards to government (Goddard and Gayward 1994). The ‘MAC’ initiative, which was a bespoke management information system developed during the early 1990s, was an attempt to remedy this through standardising the way in which information was collected and reported across universities. Widely adopted, these systems were largely acknowledged as ‘unsuccessful’ as many institutions quickly looked to replace them with alternative solutions (Pollock 2001; McLaughlin et al. 1999).
Big Civic, which was not keen to repeat their MAC experience, decided to procure a generic off-the-shelf system and thus invited a number of well-known enterprise-wide system providers to tender for the project. From the potential bidders, the large global supplier SoftCo was selected.
One of the reasons for this choice was that it was said to be the most high profile of enterprise system suppliers around, its product having established itself as the market leader. Another was that there were at the time very few niche-specific ERP suppliers oriented to the particular needs of higher education institutions - thus early adopters of ERP had little choice but to adopt one of the generic types of solution. In addition, while the supplier had little experience of working in higher education settings, it had demonstrated an intention to commit resources to developing software for this new market. It saw the higher education sector as a rich and as yet unexploited market. Finally, another perhaps rather counter-intuitive reason was that SoftCo’s ERP system was known to be highly ‘prescriptive’ with the integration of its various modules demanding a wide-scale business process change. Let us explain why this was seen as important and advantageous by the university.
Tough medicine
The most radical aspect of ERP, perhaps, is its attempt to capture and integrate the full range of activities and transactions across an organisation. This is in contrast to earlier information systems, which were typically ‘discrete technologies’ that were applied to specific or closely related functions. In universities, information systems were usually kept within the domain of the centralised administration and had little influence on the primary functions of universities (and their chalk-face workforce). It was common for academic departments to maintain their own smaller information systems (which did not interface with the centralised system in any useful way). By contrast, ERP is applied to all aspects of an organisation in an attempt to bring together unrelated functions under the umbrella of one system. In this respect, enterprise systems are often deployed with the aim of ridding organisations of ‘functionally based silos’, to use the jargon, and replacing them with organisation-wide processes. For this reason, the systems have become synonymous with radical change and the large scale redesign of organisational practices. In order to bring about the level of integration desired, there is a need to replace the often ad hoc and inconsistent procedures found in departments with more formal, standardised processes and roles across departments. The benefit of this is that it allows organisations to have consistent processes throughout the organisation as well as a single source of data. While the re-engineering stage is widely recognised to be difficult, it is regularly presented as an opportunity for organisations to update procedures and achieve new kinds of organisational flexibility and performance.
Indeed, at the university where we conducted our research, the management team finally decided on SoftCo because this supplier had presented its solution not in a narrow technical way but as an opportunity to restructure the university’s entire business processes. This met with approval as the university managers were in the early stages of putting together a plan to restructure and
devolve much of the management and administration issues to departments, and the system appeared to offer a useful template in structuring such changes. In short, the rationale for embarking upon the project, as given by the Pro-Vice Chancellor in charge, was to support a fundamental restructuring of the institution’s information management. To paraphrase his words, the idea was to follow what multi-nationals were doing through having highly decentralised structures where line managers were given greater autonomy and responsibility within a framework of an overall corporate entity. In this respect, the goal was to build a devolved structure where senior management had oversight of the university but schools and departments were given greater autonomy. The generic ERP solution, then, including their involvement in the building of the new Campus module, was seen as ‘tough medicine’.
Becoming a pilot site
The actual work of building the Campus module began a year or so after the decision to implement the larger ERP system. The university’s status in this project was as one of several ‘pilot sites’ or ‘development partners’, another of the terms that was often used. It was explained to us by Big Civic staff that the difference in nomenclature suggested more engagement than one might normally expect from a mere pilot site. This is because the new module was to be built upon the existing practices and needs of Big Civic and these other selected institutions. Seemingly (and something confirmed by various SoftCo staff) this was one of the primary means by which this vendor built new solutions.
Interestingly, not only do they construct the bulk of their solutions around the firms with which they have already established relationships but they also apparently spend a great deal of time actively ‘selecting’ the organisations upon which new modules are to be based. They seemingly look for organisations with particular characteristics and which contain certain kinds of actors. Although not able to witness the actual search and selection process ourselves, we did receive a detailed description of the selection criteria SoftCo had purportedly adopted. This was contained in an email which was sent from the Campus Solution Manager to one of the other pilot universities some time after the actual choices had been made:
Equation
Email from the ‘Campus’ Solution Manager at SoftCo to a pilot site
Here are my thoughts what [sic] you can mention as selection criteria from us on the [Campus] pilots:
1	[Pilots] had to represent the major global education/cultures/pro-cesses as well as a potential market: US, UK, Central Europe;
2	Had to be institutions which support ‘forward-thinkers’, ‘thoughtleaders’ (‘visionaries’) in helping us to develop best business practices, a system for tomorrow, not just for today;
3	Had to have strong academic and administrative leadership at the very top to support this sort of project;
4	Had to understand and help us prove the value of a SIS [Student Information System] which is an extension of ERP. So these are not just SIS pilots, they are ‘ERP for HER [Higher Education and Research]’ pilots;
5	Had to be ambitious as institutions - want to be ‘the best’ at what they do, on the leading [edge] of using technology for Higher Education administration and teaching and research;
6	Had to be true partners, not just ‘test sites’ - representing a longterm commitment by [SoftCo] and by our customers;
7	Had to hold [SoftCo] to high standards, raise the bar, be demanding;
8	Had to have a strong sense of what were their priorities, goals, missions as institutions to help us focus our priorities;
9	Had to be an institution committed to implement the solution, not just develop it. Therefore had to be an institution looking for immediate benefits: switch off legacy systems, get rid of manual processes, use more self-service [and] automated [or] online services.
There are many interesting aspects to this message, most of which highlight the tension between Campus as a particular innovation and as a generic product.3 Clearly, the module is being built with the needs of the participating institutions in mind, but also, as is pointed out, for a larger as yet untapped market. In this sense, these pilot sites are ‘proxies’ for the entire higher education sector. And interestingly, inclusion in the project, this role as proxy, demanded certain (often quite arduous) responsibilities. There was the expectation that the chosen sites would embrace ERP in its fullest sense. This included attempting the various (often quite radical) kinds of integration and automation required to have a fully functioning enterprise-wide system. They were being pushed to do this not simply because individually they might reap supposed benefits but because they could become exemplars or ‘reference sites’ which could demonstrate to others the possibilities of this kind of technology. In other words - and something explicitly acknowledged by everyone involved - they were not simply rolling out a system but were potentially helping to establish the concept of ERP itself within this particular market.
This form of work was particularly important because during this period (the late 1990s) ERP had become popular in large corporations but had still not caught on within public institutions. SoftCo therefore saw it as crucial to prove to potential adopters that, just as ERP had been successful in many other industries, it could now achieve similar results in the public sector. Indeed, this was turning out to be quite a critical concern as the reverse
appeared to be more likely: some institutions were beginning to voice concerns that the concept of ERP, and particularly SoftCo’s version of ERP, could not work in higher education (see Pollock and Cornford 2004).
An exemplary pilot site
Right from the beginning, Big Civic took on its role and responsibilities with much vigour. It was particularly active locally and, encouraged by the supplier, established a dedicated ‘student management system team’ comprising experts on student administration as well as computer systems. Many members of this team had been (or still were) involved in the wider ERP implementation and so had some understanding of the nature of the work ahead. From Big Civic’s point of view, their participation was predicated on the belief that they might be able to influence the shaping of the package through allowing the software to be (partially) designed around their own organisation. The whole project appeared to be a unique opportunity to have all the benefits of a standard software package without many of the problems. From the supplier’s point of view, it seemed a good opportunity to have a university involved in the development from one of the biggest and most prestigious of higher education markets. It was widely anticipated that Big Civic would be used as a ‘flagship reference site’ to encourage other institutions to adopt their ERP solution (Big Civic internal report, April 2000).
Moreover, throughout Big Civic, there appeared to be initially a good level of support for Campus and the deal the university had managed to broker. As part of the partnership, for instance, it was agreed that SoftCo would meet all the software costs as well as some of the implementation costs. On the Big Civic project website, it was described as a ‘pioneering’ development between a well-respected software supplier and several prestigious universities from around the world - with Big Civic at the ‘forefront’. The eventual plan was eventually to merge Campus with the other ERP modules already installed, meaning that not only would Big Civic be among the first to roll out the Campus module but it would also be the first in the world to have a fully integrated ‘university ERP solution’.
The genesis of Campus
As we have said, the package investigated in this study was understood to be among the most rigid of ERP systems on the market, carrying both prescriptive and proscriptive assumptions about the nature of work and organisational process. Below, we describe how the new Campus module developed out of and attempted to carry forward this biography.
Recycling software
SoftCo were keen to enter the higher education market but there was one clear obstacle. While they thought their system would be able to cater for
many of the functions conducted within these new kinds of institutions, they did not as yet possess a module specifically for student administration. This was widely seen as an important limitation. To rectify this they therefore promised to commit resources to building a new module specifically for the management of students - one that could seamlessly integrate with the other parts of its ERP solution. However, some months later, and to the surprise of the pilot sites, this is not what SoftCo actually delivered. Rather than build new software per se, they attempted simply to re-use a number of existing systems. These were the Training and Events Management (TEM) module, which was typically used to run internal training programmes within large corporations, and the Real Estate (REM) module, which allowed firms to manage their buildings and estate.
This strategy was not altogether surprising. Software package suppliers operate by taking existing software and adapting it to work in what vendors regard as ‘similar classes’ of organisation and setting. In the past, this tended to be within the same or connected industrial sectors but it has now become common to translate software for use across different and unrelated sectors and organisations. In this sense, enterprise systems form part of an established practice in the supply of standardised information systems whereby suppliers recycle the same system within different organisations. They can do this, proponents argue, because enterprise solutions are fundamentally based on the notion that organisations contain common elements (Laforest 1997). Thus, for the programmers working on Campus there appeared to be many obvious similarities between the existing software and these new settings.4 However, from the point of view of the pilot sites, this recycling posed a number of problems, as the module did not ‘fit’ very well with the university environment. We show one aspect of this problem.
Special types of employees
When Campus was delivered, the new module met with much displeasure from the end-users within the various pilot sites. For example, an email from a Belgian university involved alongside Big Civic describes some of the limitations of re-using the existing software:
Until now the Real Estate module has always been referred to for student housing. This only contains the functionality to ‘let’ rooms (very commercial). For some universities this is not enough. Student rooms are often part of student aid. A lot of extra activities have to be organised in association with this (e.g. meals).
(Email from Belgium University to other pilot sites).
Here this particular pilot points out that the Real Estate module is not sophisticated enough to capture her university’s interactions with its students (for example in the way it collects rent, etc., and how it deals with special
cases where students receive financial support). There were also much more strongly worded complaints from many across Big Civic - particularly those responsible for student accommodation:
the Real Estate Module is so far removed from our requirements that [SoftCo] would do better to start again from scratch than try to adjust the existing module. The most obvious shortcomings of ‘Real Estate’ for us is that the module is designed for the commercial sector where longterm lets of 12 months or more are standard. It is not designed for the levels of volume and turnover that characterise the student market and, even more obviously, the conference market. In short, Real Estate does not set out to be a retail booking system, which is what we are looking for.
(Memo from Estates Department to internal Campus team, Big Civic)
The basic problem was that the existing ERP modules are comprised of highly formalised representations of organisational practices and actors. This includes generic models of organisation structure and functions down to very detailed descriptions of business processes, together with models of particular products, transactions, organisations and people. To give an example, ERP systems are structured around general notions such as ‘supplier’, ‘customer’ and ‘employee’, and while these may share some of the characteristics of actors found in universities, they do not map straightforwardly. None of these categories appeared to fit with the notion of student, for example. Within the TEM and REM software there was no category called ‘student’. Thus, after lengthy discussions about the characteristics of this actor, the programmers decided to represent them in the module under the notion of ‘employee’, albeit a special type of employee, one who was undertaking a ‘long-term training course’ and thus ‘permanently renting a room’. In this way, they had seemingly found a to way develop the student module through re-using much of the existing software.
However, as we have said, this raised numerous tensions among university staff, particularly those with specific responsibilities towards student administration. They found the module could account for some but not all of the circumstances and characteristics they routinely had to deal with when administering students. In the light of growing complaints, the supplier would have to admit to the pilots that re-using software in this case was not as appropriate as it had first thought. Nor would it be possible to move the universities towards the organisational assumptions embodied in the software. It seemed, and this was clear from our later discussion with SoftCo employees, that they had underestimated the complexity of the university environment and, just as important, had overestimated the transferability of existing systems. One member of the Big Civic team, who we interviewed, describes this problem rather elegantly:
Part of our problem in the beginning, with them [SoftCo] and with other consultants, [was that] we didn’t want to go through the ‘sausage machine’ and they kept trying to push us through, and it took a long time for them to realise that we wouldn’t go.
(Author interview)
The biography approach draws attention to the way these technologies are always in a constant process of transformation and translation. Suppliers will attempt to reconcile the gulf between their solutions and the specific contexts of adopters in a number of interesting ways. One method is to ‘localise’ the system, not to the many idiosyncratic practices of the adopters but to ‘common process’ throughout the university. Below, we discuss how, at Big Civic and the other pilots, no such processes existed and thus had to be created.
There is no organisation there...
Now under pressure to deliver a more effective system, SoftCo set about a major software-writing effort in order to turn these generic ERP modules into something that was more specific to the university user. They were going to rebuild their modules to map on to the needs of the pilot sites. Thus, a development project was initiated. It was organised in the following way. The actual work of adapting and expanding the software to the new context of higher education was organised as a complex chain linking Big Civic and the other pilot implementation sites with analysts and programmers back in their home organisation. In this process, requests for information about existing management and administration processes at the pilot sites would be passed down the chain from SoftCo analysts to the various sites. Theoretically, at least, these would be collated, collected and passed back up to the programmers, who, in turn, would pass back code based on their understanding of the information they had received. After this code had been received there could also be individual requests for changes to the system (for example, the addition of new functionality) which would be passed back up the chain to SoftCo programmers.
In practice, however, the SoftCo staff would often receive back incomplete or ambiguous responses to their questions. When asked to describe processes, the staff at Big Civic could not agree with one another on what these actually were. The university contained not one but many different ways to carry out the same process. This was because the various schools and departments had grown up, over a century at least, doing things in their own fairly idiosyncratic ways. Throughout the university, there were no clearly articulated institution-wide policies or standards for many of the things that needed to be included in the new system - a situation typical of what McNay (1995) describes as the classic ‘collegial academy’.5
We found out later that this was a common theme across many of the pilots. In one meeting conducted on the SoftCo’s premises and attended by
Big Civic and a number of other pilots, for instance, there was a session where institutions were asked to clarify and explain their policies for student registration. What emerged was that there was a diversity of processes for carrying out the same task within each institution. ‘We have three different ways of handling credit points’ (GreenField University). Moreover, some activities were clouded as to who did what and when: ‘What I can determine is that you don’t have a clear-cut process, it is informal’ (SoftCo analyst summarising one particular conversation). Others activities appeared to be so complicated as to be shrouded in mystery - when asked to explain how their university assigned students to particular faculties and departments, for instance, a GreenField University representative somewhat ironically described how: ‘It is magic! I don’t know how to clearly explain our situation.’
Within Big Civic there were similar problems. We observed, for instance, how the implementation team attempted to uncover the recent history of practices regarding the handling of student fees so that they might be explained to SoftCo and included in the module. In one conversation, the team is trying to work out the process for setting and administering fee categories, which were classifications that had been established on the existing MAC system. It was not obvious, however, why the categories were there or who had decided which academic course was applied to which category:
Every degree programme is currently assigned to a fee category within MAC. However, the process by which new programmes are allocated to categories is unclear and haphazard. The fee category is an important parameter in the new system.
(Big Civic internal doc)
Further investigations provided no clues and the team were forced to conclude that the staff responsible for inputting these fee categories must just have been simply ‘making them up’. To characterise the problem here more analytically, it seems the issue SoftCo was coming up against during requirements-gathering was that even though it was looking to build new software there was often no organisation there to fit the system to. What we mean by this is that there were none, or at least very few, of the common enterprise-wide processes that the supplier had expected (hoped) to find. This delayed the project somewhat, but did not dampen SoftCo’s enthusiasm or that of the internal team. If these enterprise-wide processes did not yet exist then they were going to create them - and this is exactly what they set about doing.
Bootstrapping
Creating institutional-wide processes presented the internal Big Civic team and SoftCo with another set of problems. Designing new sets of common standards (such as a ‘fees policy’) that the supplier might then draw on when designing
Campus was complicated by the fact that the internal team themselves had little to guide them while conducting this work. There were the existing practices, but these were too numerous and inconsistent to be of any use as a template. Initially they had hoped to be guided by SoftCo and the module itself - for they did not want to have to redesign the processes again when the final system was implemented. Faced with this problem, then, they were unable to push ahead with the process redesign that was needed. At the same time, SoftCo also found itself unable to proceed because it was waiting for Big Civic (and the other pilots) to describe the new institutional-wide rules and procedures that could be embedded within Campus. It could not - or rather would not -design the module around something it considered to be ‘idiosyncratic’.
Thus, this led to an interesting period of design. For much of the initial phase both parties had to imagine how the other might work. Many of the questionnaires sent by SoftCo to the various pilots during the initial requirements-gathering phase, for instance, began with the statement ‘Imagine the case where...’ That is, SoftCo would suggest a particular scenario about an aspect of student management and ask the pilot sites to tell them, first, whether it was accurate, and then, second, how they might deal with it. The responses to these questionnaires were at the beginning very long and detailed as the Big Civic staff attempted to ‘correct’ the scenario and bring it closer to their own practice - and many at Big Civic complained that it was often necessary to provide the same information over and over again to the supplier before they finally ‘got it’. In turn, while reading the supplier documents describing the emerging system, the Big Civic staff could essentially ‘speculate’ about the shape the module was taking - and thus could begin to expedite the development of common processes. These then became part of discussions with SoftCo analysts - where the internal team could give better and more accurate descriptions within the various questionnaires, and so on. Thus, we can say that the Campus module was designed through a process of ‘bootstrapping’. This was the careful (and often frustrating) interplay between the supplier’s conception of what the pilot sites did, and the pilots’ attempts to work out for themselves what they did and simultaneously align this with the emerging view of the new module before feeding it back to the supplier.6
Building a global product
Reconciling this gulf between the generic model and the specific mode of use required by Big Civic was not the only barrier to the implementation of the module. Many other issues began to grow because of the supplier’s intentions to market the module around the world as a potential ‘global product’. Consider the following extract:
[Campus] is intended to be a global product. In order to facilitate this, the software is being designed to be as flexible as possible to allow
individual universities to configure the functionality according to their own processes. Because the basic design has to accommodate university structures and processes across many countries, much of the functionality can be configured in more than one way and the overall functionality is likely to be much wider than that required for any one country.
(Big Civic internal report)
The module therefore was to include the ‘local standards’ of the various participating sites, and where specific features could not be included each site would carry out its own individual customisation work. In other words, SoftCo was not only attempting to move the module from a generic ERP user to a specific university user, but it was attempting to move it back again to a more general form of ‘university user’. To paraphrase the words of the Solution Manager quoted earlier, the module is an ‘extension of ERP’, and the universities are not just pilots for a ‘Student Information System’ but pilots for a university ERP system.
Interestingly, one of the methods used by the supplier to do this was to invite the pilots and other early adopters to regular ‘testing sessions’ of the module. We observed one such meeting within the SoftCo offices, where the aim was to find where there might be some common requirements among universities, and if none were apparent, to explore how these might be constructed.
Constructing general university users
One method of arriving at common needs was through the development of what SoftCo called ‘generalisable concepts’. These appeared to be concepts that were sufficiently flexible to be able to work across multiple sites, but also not so weakly structured as to lose their shared identity. Below, we discuss an attempt to construct one such concept for the practice of ‘holding’. Holding is the process by which a student might be blocked on the system from reregistering for a new academic term because of outstanding debts to the university (tuition fees, unpaid rent, library fines, etc.) or through failing an exam or coursework. The issue being discussed below is whether holds should be put in manually by administration staff or whether they should be automatically triggered by the Campus module. According to SoftCo, manual inputs are a ‘system limitation’, and if the pilots could only arrive at a common set of understandings about the holding process, then the procedure could be automated. In the meeting, one analyst asks for comments on what currently happens at each institution:
SoftCo analyst: Students with bad marks. What do you do with them, leave them in limbo or give them a second chance?
Southern University: Depends on timing. If just before a session and there is no chance of them bettering their mark, then we refuse them. Or,
alternatively, we could say we’ve not decided yet. That is not a hold but a ‘waiting status’.
Technology University: If you’re doing something that might pick up your grades?
SoftCo analyst: I wouldn’t call that a hold, that’s a ‘provisional situation’. Rural University: We have a ‘partial hold’, so holds affect some things... GreenField University: Isn’t that a ‘half-hold’?
(Fieldwork diary)
The discussion goes on for some time and finally everyone (including the analyst) appears to be in a state of confusion as to what a hold might be (a ‘waiting status’, a ‘provisional situation’, ‘a half-hold’.).7 Reaching common understandings and developing generalisable concepts is far from straightforward - and we develop this issue in the next chapter. Interestingly, it was SoftCo and not the universities that was keen to have these discussions. For them, such a process was useful as it allowed the construction of a robust concept and thus one that is applicable to the widest variety of users. In contrast, the participants at the testing session were becoming increasingly frustrated by the attempts to understand every difference among the institutions present.
Incorporating specific contexts
In these initial stages of Campus’s biography, the module was all-inclusive as the aim was to build a generic package for use across many types of higher education institutions and not around the specific needs of any one adopter. However, some time later, it appeared that in order to produce a working solution there was a need to build a specific and local context into the software (Berg 1997). It was widely thought, for instance, among those participating in the pilot project, that Big Civic and ‘GreenField’, an American university also participating as a pilot site, had been particularly successful in ensuring that many of their features were included in the software. This was seemingly because they would be the first to ‘go live’ with the module. A Big Civic internal report describes how:
Although [SoftCo] is aiming to build [Campus] with a ‘global common core’ to the software, [Big Civic] has been able to influence the content of that global core product to its benefits, along with the UK specific requirements. This should ensure that the amount of ‘customer specific’ development and configuration is kept to an absolute minimum.
(Big Civic internal report)
However, towards the later stages of the project, Big Civic found that though it had initially been able to shape the content of the module to its benefit, now it too was beginning to lose its influence in light of pressure
from GreenField. The supplier, according to the Big Civic Project Director speaking at a team away-day, had problems coping with wide-ranging and sometimes diverging needs of the pilot sites:
[SoftCo] has problems: too many requests from the pilots, together with [GreenField’s] demands for changes, means that they have more demands than resources and are trying to prune down on what they need to give to us go live with.
(Fieldwork diary)
Part of the problem as the Project Director saw it was that the supplier did not yet have a clear vision of a general university user; the need to bootstrap had been extremely time-consuming and resource-intensive, and it was becoming apparent to them that there was rather less commonality among the various institutions than they had originally assumed. On top of this, the pilot sites were becoming increasingly demanding. As described by the Big Civic Project Director: ‘There are lots of people from around the world asking for different things and because they don’t have a model of university to work with, they can’t decide on what is important’ (our emphasis, fieldwork diary). Moreover, SoftCo was finding it increasingly difficult to continue to resource the production of generalisable concepts, particularly when many appeared to work across a few sites only. Therefore, the universities were drawn into a struggle with the supplier and with one another over the inclusion of their specific needs. This, as described by the Project Director at Big Civic, led to a ‘push and shove’ between them and GreenField.
Campus takes shape
In the final stages of the development, there was often ‘competition’ between universities about whose needs would be represented. When presented with requests from several users, the supplier would conduct a selection and ranking process concerning which functionality would (and, importantly, would not) go into the system (see Chapter 5, where we attempt to conceptualise this work more fully). While Big Civic had been particularly influential in the early stages, since it was both promising an early implementation and represented the large and important UK market, it now found that it was no longer able to exert the influence it once had. The supplier was now prioritising the requirements of a US university, which was also promising to attempt an early implementation, and, more importantly, which appeared to be more open to incorporating some of the more advanced features that were already available in the generic version of ERP. For instance, they were pushing SoftCo to include functionality to cope with differing forms of student enrolment and registration: they argued that the inclusion of features and software such as ‘self-service’ and Customer Relationship Management (CRM) would suit their more ‘market-oriented’
admission process. The Project Director at Big Civic, however, saw this as having implications for their student management processes:
Rather than a UCAS [Universities and Colleges Admissions Service] type system, [GreenField] has a competitive marketplace and therefore wants online student registration. [SoftCo] has finite support and [GreenField’s] needs are more complicated than ouirs...[GreenField] also plonked on the table more requests for CRM and that has blown the whole thing apart. If [SoftCo] meets this request, then that is less for us.
(Fieldwork diary)
This left Big Civic concerned that the Campus software was now heavily favouring the US model of student administration rather than their own. Their concerns were that the module was about to take a direction that would not be workable in the context of Big Civic. Although they were initially open to the possibility - and team members from Big Civic had been among those asking for the inclusion of such functionality - colleagues in central administration and throughout the wider university were less convinced that self-service registration or CRM was desirable in their context at the moment.8 On top of this, much of the new technology appeared to be at odds with their more ‘handicraft’ and ‘paper-based’ approach to the registration and management of students on campus. More importantly, as both sets of needs could not be reconciled within Campus, SoftCo was forced to prioritise one set of requirements over another - and thus chose to build the system according to the needs of the larger US market.
Accommodating unwanted functionality
While suppliers of generic packages are careful to ensure that a package does not fully mirror the needs of one particular adopter, there are times when later adopters have to accommodate the specific requirement of an earlier adopter into their own organisations. Thus, ERP may have important implications for the autonomy of adopting organisations and their ability to decide to operate in their own way. In the case of CRM and self-service functionality, for instance, their inclusion in the Campus module meant that pilot universities were forced to consider adopting this functionality. While on one hand this simply meant that the various pilots debated CRM and self-service earlier than they otherwise might have, on the other there was more direct pressure placed on institutions.
At Big Civic, for instance, their ability to influence the direction of the module was directly linked to their acceptance of these new features. As a result, back at their university the Big Civic team would present self-service, and allowing students access to their own records, as an inevitable and an integral part of the Campus module. They also openly warned colleagues that rejection of this functionality would lead to their having a ‘waning’
influence with the supplier. However, after a long and hotly contested struggle, self-service was eventually rejected by the committee overseeing the project on the grounds that it was too early for the university to move in this direction (see Pollock 2003).
Moreover, universities often found that there were other (often more mundane) features that were impossible to reject. As Big Civic found, it was having less influence: the part of the module used to record applications by prospective postgraduate students, for instance, was built simply according to procedures common within American universities. There, applicants are required to pay a small fee when submitting an application. As a result, Campus automatically generates an ‘accounting record’ for each new prospective student so that the appropriate financial information can be logged. However, there is no similar fee requirement in Britain and this therefore leaves Big Civic (and all the other non-US universities) with the problem of deciding what to do with all the unwanted accounting records (some 30,000 being generated each academic year). There were many examples of this type of unwanted functionality. If we accept the customisation argument described in Chapter 1, then the solution appears straightforward (simply remove this functionality). However, there were also compelling reasons why Big Civic should accommodate inappropriate aspects in some form rather than rewrite the code each time. If the module was to be localised too much then there was the risk that the university would be unable to make use of later upgrades and new functionality released to the worldwide ERP community (i.e. the Package Paradox). In the case of the accounting records, therefore, these were simply stored on the system as they were generated, a fix that ultimately reduces the efficiency of the system (and goals of reducing data redundancy).9
Conclusions: building a package through bootstrapping
Organisations are increasingly reliant on the use of packaged software solutions because of the economic benefits of commodified solutions, the inter-operatibility benefits of standard platforms and the desire to align with best practice. However, there is often a large gulf between standardised generic solutions and the specific contexts, practices and requirements of particular user organisations. While there is a growing focus on the incommensurabilities between technology and organisation, much of this research is limited (focusing only on single aspects of the package lifecycle) and simplistic (emphasising either the ‘global’ or ‘local’ aspects of the package). Our argument has been that we need better mechanisms for understanding the coupling between technical and institutional change. This is in terms of the implications of software evolution for organisational change; by this, we mean the various characteristics a package accumulates through previous stages of design and the process by which its history or ‘biography’ continues to shape later adopting organisations. We also need to understand the
influence of existing organisational practices on software design; here we mean the factors that mediate an organisation’s ability to shape and domesticate software and the way their demands can come to influence the evolution of the package itself.
In this chapter, we have highlighted a number of stages that occurred in the biography of this ERP package. In the first part, we described how incommensurabilities between the package and the setting were highlighted. Indeed, reconciling the package with the context of universities provided for an acute problem. There are fuzzy boundaries between universities and organisations more generally, and while they engage in many similar activities, they are arguably ‘different’ (Pollock and Cornford 2004). It is difficult, for instance, to translate software to a new setting when packages embody conflicting categories and institutions have good reasons for continuing to use their own rather than accepting the generic classifications embodied in the software.10 This makes the use of standard software all the more difficult. The depiction of students as special types of employees and the subsequent tensions this provoked suggests that this is far from a trivial problem and not one that will go away. Standard software is unable to support the diversity of classifications at use in a particular site, and therefore some form of standardisation or convergence with the package is inevitable.
The second stage we described was the actual means by which the module was translated for use in Big Civic and by which both the software and the institution had to be created, but each depended on the other and this represented something of a ‘bootstrapping’ problem. Indeed, this was a familiar theme among many of the other institutions involved in the project. It is one of the reasons why the supplier ran into difficulties when attempting to move the module from a generic to a specific university user (‘they don’t have a model of the university to work with’).
Similarly, in the third stage, when attempting to fit the module to the demands of all the pilots, they found themselves flooded by requests (‘lots of people from around the world asking for different things...they [SoftCo] can’t decide on what is important’). Here the supplier has to take decisions about product design and markets. These are choices between the advantages of increasing a package’s scope and specificity (its greater utility and fit to the user organisation) and the cost advantages of increasing its market size. Increasing the range of functions within a package, for instance, gives benefits and potential value to the user. However, embodying ever more knowledge and presumptions about organisational practices within the software will have implications for its applicability and fit in other organisational settings. The simple and ‘discrete’ application can be readily applied in many settings, whereas the most complex integrated applications will ‘have a market of one’ (Williams 1997).
Therefore, they began a fourth stage, the process of developing the system for a generic university user (i.e. the production of ‘generalisable concepts’). This stage was particularly interesting as it involved a process of ‘generification’
as the supplier attempted to replace idiosyncratic features with ‘common’ ones, what Schumm and Kocyba (1997) refer to as a process of ‘decontex-tualisation’ and ‘recontextualisation’. While this had initially worked well, the generification process was beginning to be too resource-consuming and we saw the fifth and final stage, where the module moved towards one particular design (that of GreenField). This led to strains on the relationships between the supplier and Big Civic managers, whose agreement to pilot the new software was predicated on the belief that they could influence the shaping of the package through allowing the software to be designed around their particular institution. Once the supplier attempted to make the module more generic, however, they experienced a ‘loss of control’ as their specific features were ‘designed out’ of the system (see also Ellingsen et al. [2002], who describe a similar phenomenon).
To summarise, what we have presented here is an initial reflection on the various tensions and complexities that surround the design and evolution of software packages. It is clear that suppliers actively manage the biography of their systems, specifically the sites implicated in the evolution of a particular artefact. More studies, particularly comparative ones, are needed to examine just how suppliers take decisions about product design and markets, as well as how they manage the tension between designing systems for a specific user and for a wider market. This is the topic of the next chapter, where we look in more detail at the later design phases in the evolution of the Campus module and compare these with the biography of another software package.
5	Generification work in the production of global solutions
Systems do not travel
This chapter builds on and extends the previous one through tackling in detail the conception harboured by much of the Social Study of Information Systems that complex organisational information systems do not ‘travel’. Marc Berg (1997), for instance, suggests that the difficulties in transporting such systems from one place to another arise because they become fixed in ‘time and space’. His argument is that software becomes so thoroughly imbued with the local idiosyncrasies of its place(s) of production that it only works at the site(s) for which it was designed and built. Scholars in Science and Technology Studies (STS) and Information Systems (IS) research have spent much time describing how building anything other than the simplest artefact produces this kind of particularisation. There are dozens of such examples in STS and IS research of how Manufacturing Resource Planning (MRP) systems, finance sector administrative systems, hospital information systems and, to use Berg’s example, expert systems resist transfer to other settings.1
Yet there is a curious contradiction. Despite familiar-sounding stories of failed or problematic technology transfer there are, of course, many types of software that do appear to be highly mobile. For instance, Enterprise Resource Planning (ERP) systems are used in diverse places and appear oblivious to the form, function, culture or even geography of organisations.2 Such has been their ability to transcend their place of production that they are now described as ‘generic’ or even ‘global’ solutions.
How are we to understand travelling software through the lens offered by the critical social sciences? Some have sought to question their existence, disputing whether there is such a thing as a generic system. According to this argument, a truly global system is a modernist dream: there are no ‘genuine universals’ in large-scale information technologies (Star and Ruhleder 1996: 112) and their creation is akin to ‘hunting for treasure at the end of a rainbow’ (Hanseth and Braa 2001: 261). An alternative approach has been to highlight the effort of local actors in making these systems work in specific local settings (McLaughlin et al. 1999). From this perspective, we might
focus on the all too apparent gulf between the software presumptions and actual working practices at the settings where the solution is adopted (as well as the active processes whereby humans repair these deficiencies). Despite these objections from within STS and elsewhere, the notion of a generic technology continues to be a powerful and attractive idea. There are many software suppliers, for instance, which act as if it were possible to build such an object. It is not our intention to refute the rhetorics of technology suppliers who claim to create universal solutions to organisational activities; instead, we intend to take seriously their ambitions and strategies to create such solutions. Rather than focus on the effort of ‘localisation’ and thus highlight the already well-researched ‘collision’ of system and setting, we seek to examine the much less investigated and poorly understood process through which systems are designed to work across many contexts. Indeed, we think it odd that STS in particular has little to say about generic software, given that, as we discuss below, from its earliest days it has concerned itself with how knowledge is made to transcend its place of production.
Why might this be so? Perhaps this relates to a more fundamental problem where contemporary social scientific analyses are not good at thinking about movement (Cooper 1998). STS and IS research is interested in how technologies are translated for new contexts; and, of course, a kind of movement is examined in these studies, but the primary interest is in the process of translation as a matter of localisation: both of how software is made to work within a specific setting and of how it transforms that setting. There are limitations with movement as ‘simple location’ applied to generic software, to use Alfred North Whitehead’s term (1967; cited in Cooper 1998: 108). For instance, there is little concern for any transformations in the thing that is moved, such as with the ways in which the software package is explicitly designed (and redesigned) to work across settings. We find it odd that there is such a wide-ranging set of terms to describe the way standardised technologies are ‘imported’ (‘domesticated’, ‘appropriated’ or ‘worked-around’) into user settings, while there is a comparative lack of emphasis on the reverse process through which an artefact is ‘exported’ from the setting(s) in which it was produced. This is striking since the bulk of organisational software in use today is produced in this way - the same systems are recycled from one context to another. By attempting to develop the beginnings of a vocabulary to capture this exporting, we describe the practice of making software generic (generification work), including its various explicit and revealed gener-ification strategies, as the process of generification3 Through discussing a number of generification strategies, we hope to offer novel or fresh insight into the design and use of software packages.
The design of generic packages differs from earlier software development traditions. Suppliers traditionally developed close ties with customers, the conventional wisdom being that increased knowledge of users would lead to better design. In contrast, generic solution suppliers are said to actively keep users at a distance, fearing that their software will become identified with
and tied to specific user organisations and thus will not be widely marketable (Bansler and Havn 1996; Williams et al. 2005).
Consequently, in the IS literature, software package construction is conceived of as design for markets (Salzman and Rosenthal 1994; Sawyer 2000, 2001). Accordingly, it is said that programmers work without concrete notions of users in mind, a process Suchman (1994) describes as being akin to ‘design from nowhere’.4 However, we are sceptical that complex organisational systems can be designed for abstract markets in such an asocial manner. To explore this, we present material on the design and evolution of two software packages, and describe how suppliers actively manage users through configuring them within ‘communities’. In these groups, suppliers control which functionality and whose particularity will be accommodated through various forms of generification work. Before turning to the empirical material, we review how the literature on Information Systems has dealt with generic systems and then we turn to relevant work within STS.
Narrative biases: localisation
The nature of software development has changed in the last thirty years (Friedman 1989). Whereas user organisations once built or commissioned their own software, they now prefer to buy ‘commodified solutions’. Initially these were ‘low-level’ software systems (such as operating systems, utilities and application tools), but increasingly they are also the ‘higher-level’ organisational information systems (such as payroll, procurement and HR) and industry-specific systems such as those we are discussing (Brady et al. 1992; Quintas 1994a, 1994b; Pollock et al. 2003). From the point of view of scholars sensitive to organisational diversity, this move is highly implausible, since software packages like ERP encompass a wide range of organisational activities that, because of their intricacy, are likely to vary from one organisation to another (Fincham et al. 1994: 283). In contrast, and buoyed up by the seeming success of these systems, proponents argue that they can be adapted to work in most organisations within the same class and, in principle, across different classes of organisations. In explicating these arguments, scholars point to the similarities that exist between organisations, as well as to the ‘flexibility’ of generic systems that allows them to be custom-fitted to even the most idiosyncratic of settings (Davenport 2000b). As a rejoinder to these ‘universalistic’ presumptions, a large body of finegrained empirical research has pointed to the difficulties adopters have with implementing them, as well as the large levels of unwanted organisational change they require - standardised systems may thus bring risks and unanticipated costs. The aim of much of this research has been to demonstrate that getting these systems to work is an ‘accomplishment’: an active process whereby users reconcile the gulf between system and actual work practices (McLaughlin et al. 1999).5 If they can transfer between
settings it is only as a result of this major localisation effort; they work because they have been redesigned around the cultures and practices of user organisations.6
In our view, the STS and IS research literature tends to overemphasise the collision between specific organisational practices and generic system presumptions at the point of implementation within specific user organisations (see, for example, Walsham 2001; Avgerou 2002). This, we would argue, reflects the various narrative biases within current STS and critical Sociology: that contexts of use are always individually different, unique and typified by highly idiosyncratic practices, whereas technologies are ‘singular’ and ‘monolithic’, and localisation is the means by which the standard and the unique are somehow brought together.7 A further concern is that localisation studies do not adequately address the longer-term co-evolution of artefacts and their social settings of use. This is not to say that we should view generic solutions as embodying features that can and should be applied in all contexts. We must also resist universalistic accounts and develop a language and set of concepts to describe how generic solutions are designed to pass over organisational, sectoral and national boundaries while embracing aspects of the specific features within these settings. In this respect, we argue that the notion of localisation, together with the concept of gener-ification, can be taken further to explain this circulation. Our argument is not that the organisations in which the software circulates are the same; rather, it is that, through various generification strategies, these local sites can be treated as the same. How, then, are we to account for those times when the generic systems do actually travel across many contexts (Rolland and Monteiro 2002)?
From importing to exporting
Ophir and Shapin (1991) asked a similar question some years ago in relation to scientific knowledge. This was a reaction to the ‘localist turn’ in the Sociology of Scientific Knowledge (SSK): scholars, sceptical of the claim that knowledge diffuses because it is ‘true’, sought to show how the universality of science was both an ‘acquired quality’ and a ‘local affair’. They did this by emphasising the way facts were produced with reference to specific places and times, that they were the product of particular communities, and that there were tacit practices involved in their production (Knorr-Cetina 1981b; Turnbull 2000; Hanseth and Braa 2001). Ophir and Shapin’s (1991: 15) question was ‘If knowledge is such a ‘local product”, then how does it manage to travel with such ‘unique efficiency”?’ Others voiced similar questions at the time and this led to a growth in ‘laboratory ethnographies’ and an interest in demonstrating just how knowledge escaped its locality: this was the claim that knowledge only became universal after contextual features of locality or ‘particularity’ were deleted. Moreover, to ‘solve’ this problem of how knowledge moved from one laboratory to another, Latour
(1987, 1999) introduced various terms such as ‘immutable mobile’ and, more recently, ‘circulating reference’.
While these terms have become commonplace within the STS vocabulary, they also have been criticised. First, much of the criticism objects to the overly imperialistic language used by Latour and other proponents of Actor Network Theory: ‘immutability’ seems to suggest that devices remain standardised at the centres at which they are produced, the locales at which they are used, and as they pass through the channels between these places. In particular, the notion of immutable mobile directs attention away from the localised work of adapting an inscription or innovation to a local context of use and setting up the conditions for its effective ‘travel’ (Knorr-Cetina and Amann 1990).8 Second, the terms are also criticised for implying that marks of locality are simply deleted. On the first point, and writing some years earlier, Ravetz (1972) had attempted to give a more sensitive treatment of the spread of knowledge by arguing, not for the immutability of scientific knowledge, but for its ‘malleability’. Knowledge, tools and instruments, he argued, were widely adopted through processes of ‘smoothing’. That is, scientists importing methods or techniques from outside their normal domain would ignore any obscurities or unresolved conceptual difficulties surrounding that object.9 In terms of the second point, Turnbull sought to build on Latour’s work by showing how the local, rather than simply being erased, was often ‘aggregated’. He illustrates this through a discussion of the way in which indigenous knowledges spread though a process of bridging:
I argue that the common element in all knowledge systems is their localness, and their differences lie in the way that local knowledge is assembled through social strategies and technical devices for establishing equivalences and connections between otherwise heterogeneous and incompatible components.
(Turnbull 2000: 13)
In other words, local knowledge diffuses through the creation of ‘similarities’ and ‘equivalences’ between diverse sites. Such equivalence-making requires a number of different devices and strategies, such as ‘standardisation’ and ‘collective working’, some of which we will explore further with empirical material.10
The studies
We analyse two software packages that are at different stages in their biography and characterised by different levels of product maturity and standardisation. The first is the student administration system, the Campus module, which we described in the previous chapter and which was developed by the global software giant we are calling SoftCo to integrate with its already highly successful ERP system. As we previously described, to develop Campus the supplier had involved a number of universities as the
pilot sites on which the software would be modelled before it would finally be launched to the wider market as a ‘global university solution’. While SoftCo was new to the higher education sector, it has developed software for unfamiliar settings many times before. The second study is of the student accommodation system, PAMS, which was built by a company we call EduCo. This is a small software developer, set up less than a decade ago. Having created a successful accommodation management application for a Scottish university, it sought to extend this into a packaged solution for other higher education institutions. PAMS was initially designed around the needs of its initial customer but is now being used by over forty other institutions in the UK, and the supplier is currently investigating the potential market overseas (as well as exploring the possibility of porting it across to accommodation management in the private sector). PAMS has associated with it a growing and active user group that meets regularly to learn about new product developments and petition for the building of further functionality. While SoftCo already had in place established design methods and processes for software package design, EduCo did not; the latter company was new to both higher education and the development of software packages.11
Birth of a package
The ‘birth’ stages of the biography of a software package are the most dramatic. In this phase, there are few users in place and the large community upon which the package will depend for its circulation is yet to be enrolled. Seemingly, there are many choices influencing the extent to which the package will become ‘generic’ and therefore attractive to the widest possible groups of users. Suppliers will spend time deciding which organisational practices will be catered for and which will not. In truth, however, and despite the seeming importance of this stage, the suppliers appeared initially to follow a strategy of simply and rapidly ‘accumulating functionality’.
Accumulative functionality12
Software packages are designed around a basic organisational functionality: what is sometimes described as the ‘generic kernel’. The idea is to paint the organisational reality of adopters on to this kernel by developing numerous ‘templates’, which users can then choose between and tailor to meet their local conditions. These are templates that form the ‘outer layer’ of the package and are built up over time through interactions with past customers. Suppliers only reap benefits from developing new templates when they are able to use them repeatedly (thus recouping development costs). In the birth stages, both suppliers found that, rather than simply re-using templates, they were repeatedly forced to modify them or build new ones. For instance, EduCo found that with each new customer for PAMS, the templates required modification. The Sales Director describes this in relation to the ‘Payment Schedule’ process:
When we first wrote PAMS for [Scotia University] they produced a Payment Schedule that gave the student the choice of paying in three equal instalments (one per term) or equal monthly instalments. The logic was therefore simple in that PAMS added up all of the charges and divided by the number of instalments.
(Author interview)
However, when they made the next sale to Highbrow University there were some differences that required changes to the software:
The next customer, [Highbrow], also offered the choice of paying in termly instalments, but they massaged the amounts to take 40 per cent in term 1, 40 per cent in term 2, and 20 per cent in term 3, as they wanted to get as much paid as possible before the student ran out of money. We therefore added a tick box on the payment plan to say ‘use ratios’, and this then gave access to an extra column that allows them to enter the percentage against each instalment.
(Author interview)
He describes how they could accommodate the next user with the changes conducted for Highbrow:
The next customer [Seaside] also produced a termly plan, but used the number of days in each term to compute the amount. Fortunately, the work we had done for [Highbrow] was capable of managing this, as the days in each term could be entered as numbers as well as percentages.
However, once again, when another user adopted the package they were forced to make changes:
[Central] came along. And they offered students a discount if they paid by a certain date, so we had to add another (optional) column that stored the settlement date for each instalment, and we added the code to compute the value of this discount.
The Sales Director goes on to describe the modifications required by two further universities: ‘[City], on the other hand, charges a penalty for late payments. So we added a process that calculated a charge for late payment’. And on the other hand, ‘[Rural] wanted this banded as their fees change according to the amount owed, so we added extra functions to band the charge according to the value.’
What is clear is that as each new site adopts the package, new and different requirements need to be catered for. Importantly, this occurs not simply in the Payment Schedule process but in all the other templates stored in the system library. The supplier appeared to be building into the
system whatever functionality was asked for. However, it was becoming obvious to EduCo that accumulating and not re-using functionality was particularising PAMS. In the case of the Payment Schedule, for instance, every time a change was made to the template this would be accompanied by a modification to the graphical user interface. A user was then forced to view a screen that included buttons and menus specifically intended for other institutions. As a result, there was now a need for increased training where users were told which options and buttons related to them and which did not. However, this mode of redressing the particularisation of PAMS became problematic once the system was made available for operation by students over the internet. One of the managers describes the problem:
[H]ow do you get rid of the things that a particular site doesn’t want? For example, in our payment process we handle things like settlement discount. Somewhere like [Welsh University] do not use settlement discount but they just ignore the fields on the screen. If you put that on the Web, all you do is end up with calls from customers, from students asking, ‘Why haven’t I got any settlement discount?’ when actually the answer is that we do not use it, so we do not want to display it. So how do we get over that?
(Author interview)
During the birth stage, then, suppliers are presented with choices. If they continue with the strategy of accumulative functionality, PAMS will become increasingly baroque, locked in to the particular requirements of their specific array of existing users. This realisation led to a switch in strategy. As the Managing Director of EduCo puts it: ‘We are not going to accommodate as much diversity as we have in the past because it constrains our ability to grow and resell.’ Any changes made to the package from now on, he says, will have to have wider applicability: ‘When we built change into the software we have always tried to build it in a way that isn’t customer-specific and we try to always broaden it a bit so that we have functionality that has a potentially wider audience.’ During one particular conversation, he described how they now try to ‘discourage too much diversity’. Yet this presents the supplier with an interesting problem: How do they continue to make the software attractive to - and, indeed, encourage - a wider range of new users without having to include every demand for new functionality? Importantly, how do they ‘discourage too much diversity’ without discouraging the users attached to this diversity?
Management by community
If the software is truly designed to travel, then it seems that the suppliers must avoid dealing with individual users. Indeed, the translation from a particular to a generic technology corresponds to a shift from a few isolated
users to a larger extended ‘community’ (Cambrosio and Keating 1995; de Laet and Mol 2000). Moreover, it is through establishing the kind of forum described above, and primarily engaging through it with the users, that suppliers are able to shape these communities and to extend the process of generification. In other words, through participating in community environments, such as the user-group meetings and requirement prototyping sessions, individual organisations were often dislodged from attachments to particular needs.13
Community management strategies
The suppliers had close ties with individual user organisations in the earlier phases, but they felt forced to shift to an alternative form of relationship as the technology matured and the user-base grew. The openness of the software that was stressed during initial interactions was reversed: where they had previously negotiated on a one-to-one basis with users, they now appeared increasingly reluctant to differentiate users. Individual conversations about design issues were shifted to a more public forum (see Figure 5.1 and Figure 5.2).
This shifting out is also demonstrated in the case of SoftCo, which had elaborate routines for managing its communities (and though the same kind of strategies were visible within EduCo, they appeared much less developed). SoftCo had developed Campus by gathering requirements through a process we have described in the previous chapter as ‘bootstrapping’ - where they visited and had direct correspondence with specific user-organisations. The problem in accumulating functionality in this way was that it was both timeconsuming and resource-intensive. In particular it encouraged users - as the system began to take shape - to submit to the vendor increasing numbers of requests for organisational-specific functionality, and SoftCo was now flooded by such requests. How might the vendor construct something more generic from these requests? Moreover, if they were to ‘discourage diversity’, how would users react if they felt their needs were not being met (when perhaps those of a neighbour were)? Thus, there was potential for this problem to become a focus of conflict (and the precious pilot sites on which the future of the product depended might be discouraged or, worse, lost).
Witnessing
During the requirements prototyping sessions, a wide number of potential users were invited to the SoftCo offices. The reported functions of these meetings, sometimes lasting as long as two weeks, were to receive feedback on Beta versions of the software and to continue the requirements-gathering process. It was the latter process that was the most striking, where participants from over a dozen universities and as many countries were seated together in a large room. Each appeared determined to spell out in
Figure 5.1 Performing community: the Campus user-group meet in Leuven, where pilot sites and lead-users meet to discuss, and learn about, the future direction of the Campus module from SoftCo.
Source: Photograph taken by Annemie Depuydy; permission to reproduce it here has been granted.
magnificent detail just how their particular requirements differed from the prototype on the screen in front of them or, just as likely, from the view being articulated by their neighbour at the next desk. In the excerpt below, they discuss the storing of student transcripts and whether universities need to store details on both passed and failed courses. A SoftCo analyst, standing at the front, attempts to make sense of the comments by scribbling them on overhead projector (OHP) slides:
SoftCo analyst: Does everyone want the ability to store two records? GreenField: We would maintain only one record...
Figure 5.2 Pilot site at the lectern: one pilot site member presents to others on how they have implemented the Campus module.
Source: Photograph taken by Annemie Depuydy; permission to reproduce it here has been granted.
SoftCo analyst: Is there a need to go back into history? If transcript received and courses are missing, do you need to store this?
America North:...no record is needed.
GreenField: We need both to update current record and then keep a history of that.
Belgium: In our case, things are completely different.
(Fieldwork diary)
This exchange points to the diversity of institutions present and the extent to which their requirements are similar or, at times, contradictory: while some users require one kind of record to be stored, others need a more comprehensive record, and one institution records things in a different manner altogether! Yet it is here that SoftCo was finally able to observe the similarities and differences between institutions (and to begin to shape them in some way).
These meetings were also interesting for the way in which they appeared to shape the users’ attitudes towards the overall generification process and their determination to have particular needs represented in the system. Through spending time getting to know the size and complexity of the task at hand, the participants appeared far more accommodating towards collective requirements, even to the extent that they would often compare institutional practices (‘Oh! You do that.’). They had to concede that, even though it was a generic system, SoftCo was determined to search for each difference between sites. No differences were ignored. No one group, or so it seemed, was explicitly favoured. Towards the end of one particularly long session, some of the users even began to suggest that SoftCo was perhaps ‘over determined’ to find and articulate differences. The GreenField participant, for instance, described to the others sitting at his table during a coffee break how he thought SoftCo had ‘too much patience’ in allowing everyone present to spell out their particularities in such detail.14 This comment was insightful in that it suggested an interesting shift in the provenance of the generification process and in who takes responsibility for it. Problems were seen to be the result of users, who were intent on describing their particular needs, while SoftCo, which had actually gathered them together in this way, was guilty only of being ‘too patient’.
In summary, by shifting design from the level of the individual to that of the community SoftCo moved the software package from the private domain of each user site, where only particular needs could be articulated, to a public setting, where community or generic requirements could be forged. Through physically detaching users from their specific locales, it seemed they were also intellectually detached from the concerns of their home organisation towards those of their fellow pilot-site members and, importantly, the vendor. A further advantage of allowing users to participate collectively was that they were able to ‘witness’ the continued openness of the process. Indeed, somewhat ironically, some participants express concerns that it was
not SoftCo that was prolonging or complicating the generification process but the users who were doing it to themselves.
Management by content
While management by community revealed diversity, there was also a need to shape and smooth this diversity - to manage through content (Knorr-Cetina 1999).15 There were two aspects to these strategies: first, to translate collective requirements into functionality that might be used by all of the sites present; and, second, because these sites were surrogates for potentially all other universities, to translate the community functionality into a much more generic functionality. One method of establishing such templates was through searching for similarities between sites. These similarities did not emerge easily, but had to be pursued and actively constructed. Consequently, we think it is useful to describe this process in more detail, and so we focus on a discussion of ‘progression’ within the Campus module.
Process alignment
One analyst has asked participants to describe their rules for progressing students from one year to another, and to explain how a student’s grades contribute to her overall programme of study. A complicated conversation develops, with various people interjecting. The analyst struggles to bring the discussion back on topic by attempting to summarise and name the particular process being described:
SoftCo analyst: We’ve got one aspect now. Just want to get some common things. How [do] we name the baby? Let’s go to the grading issue. Want to specify if module will contribute to programme of study in any way as a credit or grade. Is there any rule how it contributes? Is it linked to students? What is it linked to that it gives credit?
Swiss Uni: Could be a rule or a decision given by someone?
South African Uni: The student can still do the exam and be graded, but it might be true that the grade or credit did or did not influence the student’s progression...
Canadian Uni: We wouldn’t use these rules: we take all courses into progression. We have rules based on courses students take.
SoftCo analyst: It is the same at [America North]. It is the US model. It is the difference between the European and the US model.
(Fieldwork diary)
There are a number of interesting aspects in this exchange. When faced with diverging requirements, the establishment of generic features seems impossible. However, the analyst does not admit defeat but accepts the next best thing to a single generic process: ‘two’ generic templates. Moreover, she
constructs these two templates by aligning or superimposing processes that are already roughly similar to one another (‘It is the same at America North’). This then leads to the establishment of a generic feature (‘It is the US model’), which means that the requirements of a large group of universities are now seen to have been captured under one process. We also see in this exchange the naming of a further generic template, described as the ‘European model’, which emerges to capture all the differences that do not fit into the ‘US model’. From now on, there will be two modes of progressing students within the Campus module (meaning that they will adopt either the US or the European process). Drawing on Epstein (2006), we might describe this as both the production of ‘generalised differences’ and a form of ‘process alignment’. Finally, once these two categories were established, they were continually compared: both the supplier and the participants acted as if it was self-evident that everything inside each of these processes was identical, and that anything or anyone outside one classification could be easily accommodated in the other. Indeed, only one of the participants, a South African university, was from an institution outside the US or Europe. In addition, since interactions during these meetings had shown them that they had many similarities with other users, particularly the British participants, they appeared to be happy to align themselves with the European model.16 Process alignment appeared to be a successful method, with SoftCo representatives routinely framing their questions in ways that promoted this form of generification (‘Does everyone want the ability to...?’ ‘Does anybody else have this?’).
Having an issue recognised
An interesting, though not altogether surprising, development was that the users began to learn that if they were to have their particular needs represented in the system then they too should engage in alignment work. A GreenField participant makes a case that the system should record grades for failed courses and, very quickly, other users begin to give their support:
GreenField: We have concepts called ‘forgiveness’: a student retakes a course he’s not done well in and he is ‘forgiven’. The old grade is recorded but not included in the GPA [Grade Point Average].
Canada West: We do the same thing, when we have symbols that aren’t graded - like ‘withdrawn’ or ‘incomplete’.
SoftCo analyst: This is a big issue for everyone.?
Canada West: We definitely have to store it. These non-grade things don’t have a pass value or fail value; they are a ‘third’ value.
SoftCo analyst: I call it ‘additional module results’.
(Fieldwork diary)
Here, then, an issue is recognised as generic through this accumulation of support. Moreover, the analyst appears happy to include the feature in the
system since she is able both to name it (as ‘additional module results’) and to establish equivalences among the other institutions whose needs are catered for under this one concept.
The organisationally particular
It was common during these sessions to find requests that could not be made compatible across sites. Consequently, they had to be rejected or sifted from the process. The most common method for doing so was simply to categorise requirements as ‘specific’. For instance, during a discussion around the storing of surnames, an America East University participant describes how they have a specific need to record maiden names after marriage. They suggest adding a new field to the screen (an Info_Type) but the analyst dismisses this as unworkable: ‘If we went for country-specific or customer-specific Info_Types now, then we could not utilise [the wider ERP] resources. The resources would be too great.’ On this issue, unlike previous ones, the other universities do not align and thus it is not recorded on the acetate. The official reason for this was that the change would not link back to the generic system (and this meant that Campus would no longer integrate with the ERP system of which it was a small part). The suggestion instead is that America East should create a new Info_Type themselves when they customise the module back at their own institution. In other words, making the system fit America East’s needs is postponed and shifted on to the customisation stage at the user site (Hartswood et al. 2002: 28).
Smoothing strategies
Throughout these requirements-gathering sessions, many of the participants would go into great detail concerning their specific needs. The analysts would often use an interesting range of social strategies and devices to simplify and curtail particular requests, and we explore one such strategy with overhead projector (OHP) slides (acetates).
Working the acetate
In response to one lengthy description, the analyst used the physical limitations of the acetate to abbreviate a request (‘Just trying to think how this can fit all on one line’). On other occasions, particular issues would be rejected for being already covered under existing themes. Pointing to the acetate, the analyst would say, ‘We had that issue already’, even when it was not always clear just how the new issue had been covered. Indeed, the acetate was something of an ‘obligatory point of passage’ (a device or gateway through which the requirements needed to pass, see Callon 1986a); once scribbled down, an issue could be considered to have been recognised by the supplier but, of course, it was far from easy to inscribe it on the acetate. The
participants also recognised the importance of the acetates. In one discussion, the university representatives are describing progression rules and a GreenField participant prefaces his intervention by stating that ‘You’ll need a new page’. While, of course, he is attempting to signal his university’s uniqueness, the analyst dismisses this by pointing to the existing, well-annotated acetate and stating - with a broad smile on her face - that there is still ‘one line left’. Later, when the GreenField participant appears to be about to list a further set of differences, the analyst states loudly that ‘the page is full’ and he is halted in his tracks. We would say that this working of the acetate was a particularly strong form of smoothing because it appeared as a simple material necessity (and was thus not necessarily recognised as generification work).
From generification to generifiers
In the final stages of the Campus development there was once again a notable shift concerning the shaping of the package and the locus of gener-ification. Dragging the design from the private domain of direct user engagement to a public setting had, apparently, been a drain on the supplier’s resources, and the requirements-prototyping meetings were no longer seen to be as ‘productive’ as they once had been. Below, one participant from Belgium University writes in a report that:
The current way of working with workshops is very labour intensive for the people of product management and development at [SoftCo]...The biggest problem is that there is a very mixed public attending these workshops. Some of them already have a lot of expertise in [Campus] and they see the workshops as a roll-in of requirements and for giving feedback after testing. For others this is their first experience with [Campus] and they see it more as a kind of training. [SoftCo] wants to change this. In the future there will be standard training courses for larger groups. For roll-in activities there will be focus group meetings. These will only be attended by experts on the subject (limited groups of people) and they will focus on narrow subjects.
(Report circulated to pilot sites)
This shift was met with objections from users who stated a preference for collective engagement rather than the smaller group or individual interactions. While this appears somewhat counter-intuitive, the reason for the objections became clear some weeks later when one user reported that it was now increasingly common for their requests for functionality to be rejected. The university from Belgium, for instance, reports in a series of emails to the other pilots how:
We have the feeling that it’s becoming a strategy to try to label issues as ‘university specific until proven differently’. Should it not be the other
way around? Should [SoftCo] not search for generic concepts behind the specific situations at the different pilot universities?
(email from Belgium University to pilots)
This rejection was occurring because it was said by SoftCo to be functionality required by only one university. The participants found that it was becoming increasingly difficult for the universities to prove their needs were not simply specific to one university:
Now we have to prove that we are not the only one needing something, and that is not easy if we don’t come together anymore in workshops. The basic concepts seem to be fixed (based on past roll-in?), and ‘sacred’. Everything that doesn’t fit in is ‘specific’. An example of this is timetables per program and per stage. This is labelled as ‘[Belgium University] specific reporting issues’, although we see them also on the websites of [Big Civic] and [other universities].
(Email from Belgium University to pilots)
In other words, because there were no longer community meetings, it now appeared difficult for SoftCo to work out, and for the user to determine, what was a generic need and what was not. In addition, it appeared that they had decided to assume that the majority of the requests did not represent generic needs. In order to prove their needs were generic and not particular, therefore, the universities had begun to search for similarities between themselves and the other sites. This they did through telephoning and emailing each other as well as checking each other’s websites (see Pollock and Cornford 2004). In other words, once back in the private domain, the burden of generification was pushed on to the users. The participants had no choice but to become ‘generifiers’ themselves. If they did not fully participate in the generification process, if they were not good generifiers, their needs would not be effectively represented within the package. And it appeared to be better to have your needs represented in a generic format than not at all!
Management by social authority
The ability of a software package to become mobile is a result of the successful extension of a particularised application, and, at the same time, the extension of the community attached to that system. It is the latter aspect that is of interest, and specifically how the process requires the enrolment and configuring of a user community that is subject to, and actively participates in, this generification process. However, the kind of work required in this form of ordering varies from the sophisticated smoothing/sifting strategies and boundary work described above, to what might be described as more direct ‘social authority’ strategies. This was particularly evident in later phases of the package’s development when the heterogeneous nature of the
user-base and the fact that it was beginning to swell with ‘latecomers’ resulted in pressures to pull the package in different directions.
Segmenting the user-base
The initial ‘openness’ of the package was a useful strategy for building the community by enrolling users into the design process. Now, in the later stages of the package biography, this openness was something of a drawback. As was evident in the quote from Belgium University above, users were still expecting to have their particular requests met. What was unsettling some of the established pilots was that the latecomers were also making additional demands that might slow or complicate progress. This also occurred in the case of PAMS. The Sales Director describes how, early on, when the company had not yet had a finished system, it had had to create an expectation among users that their specific needs would be met. It was now difficult to correct this view:
but, of course, it raises a level of expectation...you can be a year downstream in an implementation with somebody, and suddenly they throw up this requirement that has never been vocalised before, but because they bought as an early adopter they perceive that they have that type of relationship that means that you will do it for them. Even though they may well be the only people in the UK that actually want it!
(Author interview)
Rather than simply refuse to cater for any kind of particular requirement, however, EduCo had segmented the community into three distinct categories: as either ‘strategic’, ‘consultative’ or ‘transactional’ customers. While these terms were part of the vernacular of the PAMS team, they were still thought to warrant some explanation by the Managing Director when he mentioned them to us:
it is where we perceive it is worth putting the effort: strategic customers, consultative customers and transactional customers. Transactional customers don’t want to spend money. They want everything for nothing. So for every day you put into them you get nothing back. So you put your days into consultative customers who want to work with and spend with you. Whereas strategic are all about people who help share the vision of where the product is going to go over the coming years.
(Author interview)
From his point of view, strategic and consultative customers were central to the future development of PAMS whereas transactional customers were
peripheral to its evolution. The former were regularly quizzed and consulted on the addition of new features and the general direction of the package, while the latter were actively kept at a distance. One example of how this strategy structured the users’ interactions with the package was seen in the issue of ‘customisation’ and the question as to whether a user could modify the generic kernel.17 During a conversation we had with a PAMS programmer, for instance, he praises a modification carried out by one early adopter and describes how this has even been fed back into the generic package for use at other sites: ‘[The London University] have done a fair bit...80 per cent of that has been incorporated into the standard package... They were willing to run ahead.they had the resources.’ During the same conversation, he criticises another user for making a modification to the kernel and describes how it was explicitly stated that they are not allowed to make changes to the source code: ‘We make sure that it’s in the contract that they don’t do things like that. We have had customers manipulating the data.from the back-end.Very dangerous.They promised not to do it again.’ This suggested that the ability of a user to customise PAMS, and still have their system supported by EduCo, was directly related to the status they held at that time. This, of course, prompts the question of just how a user might find themselves placed in one or another category.
Good generifiers
Typically, the status of a user was simply related to when they adopted the system, the first group of users being closer to, and later-comers further from, the supplier. One other key criterion was related to how willing a user was to reshape practices to conform to the templates embodied within the system. The Managing Director of EduCo describes how:
One of the other things we found about consultative customers where they have entered into a dialogue with us is about how they might change how they do things. There is a lot of functionality in PAMS and there are areas where the universities aren’t particularly efficient.So the consultative customers are more willing to look at how they do their business and how they might improve their business based on suggestions for us based on existing functionality or commissioning us to add extra functionality.
(Author interview)
Encouraging users to carry out organisational change to align with the system is an important strategy for managing the user-base, and also a way to reduce the need for the further accumulation of particular functionality. It is a method, in other words, of moving users towards the ‘organisationally generic’. Moreover, suppliers actively recruit customers who appear willing
to engage in such change, and they reward them with greater access to the shaping process (see Figure 5.3).18
In summary, EduCo does not have the large user-base enjoyed by suppliers like SoftCo, and thus it has to be sophisticated in how it brings pressure to bear on users. We saw a form of selection where the supplier was prioritising which functionality might be allowed into the package. Users were divided into those who sought to align with the organisationally generic features being developed, often through conducting processes of change within their own organisations, and those who did not. The former group, as a reward for being ‘good’ surrogates, were actively involved in shaping the evolution of the package and were regularly consulted on which features they would like to see in the package. The latter, by contrast, were pushed to the margins of this shaping process, where they were not consulted or involved in design or evolution. Just what they could do with the system was policed (see Figure 5.4).
We also might suggest that there is something interesting about the kinds of individual user staff who become involved in these kinds of projects. Many begin their involvement in an effort to more strongly represent the interest of their particular organisation but, interestingly, over time begin take on more of the concerns of the vendor. In the case of SoftCo, for instance, and towards the final stages of the development of Campus, there appeared to be much common ground between the vendor people and the various user representatives. Intriguingly, this common ground was not visible at the beginning of the process. We might say, then, that just as the vendors were interested in detaching their systems from local contexts, so this was supplemented by the users themselves, who were busy, back in their own organisations, doing the work of detachment.
Promising future?
We now delineate a final stage of the software packages biography: the future. How might a biographical approach (which, after all, is commonly understood as a retrospective account) be useful for understanding the future of a generic solution? The software packages might be thought to have a promising future or ‘career’ ahead of them, based on their ‘past life’. They were promising because the effort to create a generic technology required that they escape particularisation while moving towards maturity. As a result, if they are able to do this, it means that there are still many places to which the software can travel. There are many potential pathways still open to the package. In its promotional literature, for instance, SoftCo boldly states how the Campus module embodies ‘no country specifics’. Yet, despite what this says, there were times when specific requirements appeared valuable for the circulation of the software package. Alternatively, perhaps, it was simply impossible to avoid including the particular within the generic technologies being built.
Figure 5.3 The Campus community: some of the users and SoftCo staff alike pose for a group photograph after one particular user-group session.
Source: Photograph taken by Annemie Depuydy; permission to reproduce it here has been granted.
Figure 5.4 Proximity of users to artefact.
Source: Adapted from Karadedos (2003). Permission to reproduce it here has been granted.
Surrogate for whom?
Some users were able to convince the suppliers that their needs had ‘generic potential’. One criterion determining the ability of a user to get features embodied in the system revolved around the issue of just who they were a surrogate for. The UK market was seen as a ‘strong subsidiary’ by SoftCo, meaning that the inclusion of a British university in the community might open up potential markets elsewhere. And as a result, the British university was able to wield significant influence. For instance, the supplier agreed to build the ‘UCAS admissions link’, a piece of functionality that would be a significant drain on resource and, importantly, one that could not be applied in other countries. During our research, we began to learn that the Campus module embodied many other particular features. One document describes how: ‘In addition to generic functions, Campus also offers country-specific functions. These are functions only used in one particular country and cover needs arising from local legislation or business practices.’ In other words, including particular functionality allowed the Campus module to move within the same sector but also to different countries.
The case of EduCo raised a different issue, as the addition of particular functionality offered PAMS the potential to move both into a new country and across an industrial sector. The supplier was considering whether to launch PAMS in the US and, of course, one issue of import was how well PAMS would fit with the peculiarities found there. One area where a difference was perceived was in how student rooms were allocated. While UK students are simply assigned individual rooms, US students typically share a room and can therefore state their preferred type of roommate. The Managing Director described how this difference would require that ‘social engineering’ software be added to PAMS. Initially sceptical about the costs of such a development, he also saw how this might be useful for the evolution of PAMS:
That is a piece of functionality that we could add-in and usefully use over here. So it may well be something we can use. One of the things we can certainly use is the ability to have multiple layouts in a room...So we can build those changes into the software in a way that actually positively impacts on our ability to sell the software in the UK.
(Author interview)
The addition of this ‘social engineering’ functionality would mean that PAMS would have more utility in existing UK universities and the private sector hotel industry, one area the supplier had recently targeted. Their aim, in other words, was to identify where particular characteristics could have a more general appeal. We might describe particular features that aid the circulation of the package (‘the UCAS admission link’, the ‘social engineering’, etc.) as ‘generic examples of the particular’.
Paths of diversity
There were other forms of diversity included in the system. Earlier we discussed the template for the ‘progression’ of students and how the consultant had developed not one but ‘two’ templates. This was interesting as it was one of the rare occasions when the supplier had to create ‘multiple’ templates for the same process - what we might describe as poly-generic templates. In their promotional literature, the supplier describes these poly-generic templates as giving the system extra flexibility through allowing adopters more choice:
Progression - Depending on your particular environment, you may want to measure the progress of your students in different ways. One option is to determine the academic standing...Another option is to evaluate a student’s progress... [SoftCo Campus] supports several progression methods thanks to our global approach to solution design. The flexibility of this application allows an institution to change processes in the future without the need to install a new student information system.
(SoftCo document)
By allowing poly-generic templates, the supplier has created the basis for internally segmenting the user community, so that the templates allow users to follow different routes depending on their particular circumstances. They have, in other words, established ‘paths of diversity’ through which users might navigate. This was still a form of generification as the supplier was allowing users to choose between one of several large groupings. In this final section, we consider what the inclusion of diversity and generality means for shaping the generic system and the community of users.
Opening the black box (and finding a ‘black blob’)
We have shown how the generic system results from various kinds of boundary work. With the drawing and redrawing of borders, the system embodies a range of features and potentially caters to a wide range of organisations (see Figure 5.5).
Let us describe the system. The bulk of its features are the organisationally generic templates that suppliers attempt to build. These form the majority of the organisational ‘outer layer’, where suppliers hypothesise that organisations are similar and that the participating sites are good surrogates for all others in that class of organisation.19 There are also compromises in which designers, unable to devise a single template, build in several templates to carry out broadly equivalent bundles of organisational processes. These ‘poly-generic’ features reflect the diversity of user organisation practices and contexts that cannot be readily captured within a single template. Finally, there are ‘generic particulars’, where idiosyncratic requirements are deemed to be important for aiding the future circulation of the package. These are
Figure 5.5 Generic solution as a ‘black blob’.
only a few examples of how the generic and the particular are made to fit together. With further research, we would be able to generate further instances and a more complex picture. However, our point should be clear: when examined closely, generic solutions are not the monolithic systems that much of the literature seems to suppose (see, for example, Walsham [2001] and Avgerou [2002]). Rather, they are the result of intricate boundary-work involving generification (the creation of generic templates), the particular-isation of the generic (the poly-generic templates) and, at times, the gener-ification of the particular (the generic particular templates). In our view, the design and evolution of software packages are characterised by the working out of the relationship between the generic and the particular.20 Indeed, this occurs not simply in design but throughout the lifetime of the software package.
During the research we thus began to recharacterise these generic solutions as ‘black blobs’ (Michael 2000). Within STS technologies are commonly described as ‘black boxes’ in order to emphasise how their form and function are stable, that prior processes of shaping are obscured, and that the user is configured into using the object in certain ways. By contrast, the software packages are also bounded objects, but their internal workings continually contort as they move around and as new functionality is added.
While the overall appearance of the software package (and in the case of the highly modularised packages like SoftCo, as its core ‘kernel’) may seem to remain intact, the addition of a new template, for example, causes the packages to morph and extend themselves in different directions. It is through this morphing/extension process that software packages are able to move from place to place and to reach out into new settings. Such amoeboid movements, in turn, enable users to grab on to and then align themselves with the various protuberances and protrusions.
Conclusion: black blobs travel better than black boxes
Certain software packages can be made to travel with ‘unique efficiency’, to borrow Shapin’s (1998) description of scientific knowledge. In doing so, they unsettle prevailing core assumptions in the sociological understanding of organisational technologies. Put simply, much of the sociological and STS literature pays particular attention to the mismatch between system and actual work practices and emphasises the local adaptation necessary to bridge the gulf (McLaughlin et al. 1999; Walsham 2001; Avgerou 2002). While we do not downgrade the importance of this focus on how technologies are imported, we point instead to the need to go beyond studies of ‘simple location’ and also examine how systems are able to work across different organisational contexts and how they are exported. Rather than focus on the collision between unique organisational practices and the generic solution, we should also address how technologies are made (and continuously remade) to bridge these different locales, as part of our enquiry into the broader and longer-term co-evolution of artefacts and their social settings of use. We have argued that generic solutions do exist and that they do travel to many different places, though, of course, they don’t go everywhere. They arise through the broader extension of a particularised software application and, at the same time, the management of the user community attached to that solution.
We noted some interrelated moments in the biography of these solutions. There was a distinct birth stage at which suppliers designed specific user requirements into the software. This was followed by a number of delimited responses in the subsequent maturation of the package, when the suppliers attempted to move away from the simple accumulation of particular functionality. One interesting aspect was the shift to capture collective rather than individual requirements, in order to establish organisationally generic features through alignment and smoothing practices. Such practices helped establish greater compatibility across sites, as equivalencies were established in organisational practices, and differences were worked together and generalised. Suppliers attempted to align processes that were already roughly similar, what we called ‘process alignment work’. The collective gathering of requirements also had a secondary consequence of shifting expectations about the kinds of need that would be met by the system. Through
‘witnessing’ the level of user diversity, and realising that the only way to represent needs was to engage in the process, the users’ conceptions of their own needs shifted in a way that aligned with those of other participants. In other words, users were in some respects self-governing concerning the articulation of their level of particularity and generality. This raises questions about which users have the capacity to extend and broaden a template: on what grounds and by which methods?
To summarise, it is not just sociologists of science and technology who are interested in the relations between the particular and the generic, and how the boundary between them is established, managed and shifted (O’Connell 1993). Software packages are a high-value industrial product, necessitating extensive interactions between suppliers and users. Building software packages calls for suppliers to develop and sustain sophisticated strategies for managing diversity, and setting boundaries and priorities for dealing with their market of user organisations. User organisations similarly need to learn how to respond to and interact with such strategies. As communities grow and inevitably encompass a wider range of organisational types and requirements, this user-base also needs to be organised if the supplier is to avoid being confronted with a potentially overwhelming array of requirements. This, as we have shown, involves different kinds of boundary work: in terms of understandings of which types of organisations lie ‘close to’ and which ‘further from’ the supplier’s conception of the ideal type of user, and in terms of the willingness of the supplier to accept or sift particular requests from users. The ‘black box’ view of the generic solution where it simply ‘invades’ and ‘disciplines’ is too crude. What we have shown is that establishing a generic solution is a precarious achievement of various kinds of generification strategies. These are strategies in which the suppliers and users of software packages constantly work towards a pragmatic resolution of the tension between the generic and particular. As a result of this generification work, software packages can circulate and user communities can grow: that is to say, diverse organisations and standard technologies can be brought together.
6	Technology choice and its performance
Towards a Sociology of Software Package Procurement
Two incommensurable positions
This chapter tackles the issue of how software packages are selected. As with many topics in the social sciences, the debate surrounding the choice and purchase of technologies is polarised across a number of incommensurable positions. A major line of argument has been between, on the one hand, technocratic analyses advanced, for example, by Economics, Management and Engineering accounts, where the assumption is that sufficient information is available about the properties of artefacts to enable rational choice to be made, and, on the other, more sociological and critical approaches which emphasise the profound uncertainties surrounding procurement, the consequent contestability of claims about the properties of technology, and the ‘negotiability’ of the criteria used to assess objects. In this latter view, choice of one technology over another is seen to reside not in the objective properties of the artefact as revealed by a formal technical or economic assessment but to be, of necessity, refracted through or, in some accounts, driven by the micro-politics of the organisation, the commitments of the various actors, prevalent rhetorics, fads, etc. (Grint and Woolgar 1997; Neyland and Woolgar 2002).
While sympathetic to the critical account, we are dissatisfied with the relativist outcome and portrayal of technology selection as divorced from formal decision-making criteria, particularly when it seems to us that procurement is subject to powerful, albeit complex, rationales. In contrast, we suggest that the formal assessment criteria we have adopted will guide and transform the technology selection process. Our thinking is influenced by the work of those scholars who have become sensitive to the interrelationship between these two positions, in particular the work of Callon (1998, 1999; Callon and Muniesa 2005) and MacKenzie (1992, 2003, 2006a, 2006b) which has examined the contrasting explanations that economists and sociologists offer for the functioning of economic and financial markets. They both suggest that the gulf that exists between these two viewpoints is unhelpful. MacKenzie (1992) raised the idea that these disciplines offer tools with different kinds of explanatory power. Economic tools, he suggests, are
well honed for assessing the aggregate outcomes of highly regularised behaviour, where there are more or less formal criteria in play. Sociological tools (and here he refers to contemporary actor-centred accounts) are best honed for exploring the particularities of behaviour in their more or less unique historical, geographical and social settings. Callon too focuses on the efficacy of the tools that both disciplines bring to bear, but he takes the discussion further by investigating why one set of explanatory mechanisms appears to be more successful. His conclusions, which have sparked an intense debate in the field of Economic Sociology and beyond, are that certain theories and tools not only describe but also help to create the settings in which they are applied. In other words, certain theoretical constructs appear to more accurately reflect their setting because they are performed in (and in so doing help constitute) that setting.
It is the notion of ‘performance’ (and ‘performativity’) which is crucial here. The term suggests that certain phenomena are, to a substantial degree, brought into existence and sustained through the actual ‘doing of them’ (Callon 1998, 1999; MacKenzie 2006a, 2006b). Callon and MacKenzie have applied this concept in Economic Sociology to explain how markets may be brought into being. Callon suggests that if people are to trade and purchase goods in a ‘market’ (as opposed to any of the other ways the exchange of goods might occur) then the market has to be continually performed. The strength of his work lies in the conceptual framework he introduces to show this performance. For instance, he discusses and confirms the existence of homo economicus, suggesting that economic man does indeed exist but only because ‘he’ is brought into being through a process of ‘disentanglement’ and ‘framing’.1 It is because of disentanglement/framing, says Callon, that actors can make decisions which appear ‘calculated’ and ‘rational’. Callon also introduces a third term, ‘overflowing’, to emphasise the constant need to reframe as new information relevant to the decision comes forward.
Petter Holm (2007) describes how, in Callon’s approach, actors and objects are so thoroughly entangled in ‘sticky cultural contexts’ that these processes of framing and disentanglement are crucial if market actors are to evaluate and calculate the likely results of their decisions: buyer and seller must be constructed as ‘autonomous agencies’ and the object to be sold must be constructed as stable and commodity-like. Importantly, the mechanisms that enable this decontextualisation are not part of ‘human nature’ but have to be actively constructed (ibid.). To put it more in the terms we want to develop in this chapter, various assessment and comparative measures must be defined, constructed and put in place if framing and disentanglement are to take place. Paraphrasing Holm, the more taken for granted, embedded and ‘thing-like’ these measures become, the more effective they will be in untangling objects from their social, cultural and technological contexts, and thus enabling actors to make the calculated decision they desire (ibid.). These are important ideas, which, if they can be applied to the study of decision-making during economic transactions, can also be useful for understanding the choice of one software package over another.
Grey space
This chapter argues that technology choice and purchase should not be reduced to one single dimension (either the outcome of rational decisionmaking or the result of discursive struggles). Rather, it is the interaction and tension between these two positions that is interesting and that should be explored. We point to how scholars investigating technology selection within the IS and STS traditions have struggled to deal adequately with what might be thought of as a ‘twilight’ or ‘grey’ space (Tierney and Williams 1990). One side of the debate fails to take into account the variety of measures -particularly non-quantitative measures - that are used to inform and stabilise a decision (Lucas et al. 1988; Heiskanen et al. 2000; Kunda and Brooks 2000). The other downplays the possibility that formal assessment occurs at all (Neyland and Woolgar 2002). If some kind of formal process does occur, this is portrayed as mere ‘ritual’ or ‘ceremony’ (Tingling and Parent 2002, 2004). We argue for and develop an alternative approach to technology selection in such grey spaces. Although uncomfortable places to operate within, we argue that these are the spaces with which STS and IS research should be concerned. Our own response to the schism is that, rather than reduce technology choice to one or other position, we wish to keep the tension between them open. We do this in two ways: first, through showing how assessment and comparative measures are performed (in the sense that they bring into being that of which they speak); and second, through showing how these measures can come in different forms (i.e. they are not simply quantitative measures - see Callon and Muniesa [2005] and Callon and Law [2005]).
We explore this framework through a case study of procurement of a packaged solution, focusing upon the process by which the user organisation chose between competing supplier offerings. Our interest is in how the organisation’s Procurement Team (hereafter ‘the Team’) attempts to put the various properties of the systems onto a common plane - to disentangle and frame them - so that a selection can be made. They attempt to come to a decision through constructing a ‘like for like’ comparison of the different software packages. Comparison does not occur easily (or spontaneously) but instead requires much effort in disentangling the systems from their existing contexts and framing the decision so that only certain information is taken into account. This work of disentangling and framing revolves around the establishment of a number of ‘comparative measures’, constituting a ‘scaffolding’ through which a decision could be reached. These measures come in many forms: some pre-exist the procurement (‘value for money’, the ‘fit’ of the systems, etc.) and others are established during the process (assessing the ‘provenance’ and ‘status’ of the technology, demonstrations of ‘competence’, investigating the ‘standing’ of the suppliers, being able to ‘see’ a working system, and so on). In describing these measures we see no reason why we should limit ourselves to those things that are conventionally seen as
assessment criteria (‘fit’ and ‘price’, say). They include much broader issues (e.g. regarding the future behaviour and the survival of vendors) around which knowledge and criteria are more amorphous and difficult to determine. The important thing to understand is those measures that help detach the properties of the systems from their contexts and allow the choice to be framed. The actual status of these measures (on which side of the schism they belong) appears to us as relatively unimportant (Callon and Law 2005).
In describing this grey space, we are not limiting ourselves to an easy case. The acquisition described here exemplifies all the organisational exigencies that one might expect around information system procurement (and, if anything, is more complex since the procurement was part of a failed attempt at a joint venture partnership). There is also much uncertainty about the nature of the software packages being procured (many of the systems being offered were only ‘partially’ built). Our argument is threefold. First, we suggest that comparison is possible but that the construction of these measures requires much effort. To reflect this effort, we describe these comparative measures as a kind of decision-making ‘scaffolding’ that is erected as the Team move towards the procurement decision: it is through building up this ‘comparative scaffolding’ that the Team attempt to disentangle the various software packages and map out their shape and boundaries. Second, it is through the establishment of the scaffolding that the properties of the objects within their intended context of application, as well as the comparative relationship they have with each other, are actually framed. Finally, what makes our case particularly interesting was that no one actor was able completely to frame the process. While certain members of the Team sought to propose, sometimes ‘impose’, comparative measures on others, these measures (these parts of the scaffolding) were subject to considerable uncertainty and tension (disentanglement and framing broke down). What this suggests is that these kinds of assessments are to a significant extent locally produced and that the local context is important (and there is considerable discretion with regard to which comparative measures are enacted and how). The framework also allows us to distinguish different contexts in terms of the level of local discretion and changes in the external context (e.g. in our case European competition law) which impinge upon the decision.
Our case material is based on an ethnographic study that one of the authors conducted at a large UK public institution (which for reasons of anonymity we call ‘Melchester Council’) over the period of a year. The system they were attempting to procure is a Customer Relationship Management (CRM) system. We begin the chapter by contrasting how technology choice has been depicted within the Economics, Software Engineering, IS and STS literatures. Then, before moving on to our empirical material, we describe the more practitioner-based view of software selection as well as a number of research issues relevant to the study of software package procurement. Finally, turning to the empirical part, we focus on how the software choice is performed in four different ways: the collection of ‘testimonies’ from existing
users; viewing demonstrations of competence; soliciting expert advice; and visiting reference sites to witness the software in action.
Theories of technology choice
Formal comparison
In Economics and Management, procurement is typically portrayed as a process of selection between different products. The emphasis on ‘selection’ is telling as it suggests the existence of two or more possibilities that will be compared to understand differences and similarities. Underpinning this view is the notion that the specific capacities of the objects can not only be identified but are intrinsic to the technologies and are a determining factor in choice. In other words, properties are amenable to objective assessment, even in cases where the complexity of the product makes such assessment difficult (Lucas et al. 1988; Kunda and Brooks 2000). The role of adopters is thus to collect ever more information about those properties, such that artefacts can be compared to explain how they differ from or resemble one another. Once the characteristics of the systems have been laid out for all to see, the presumption is that they can be ranked, as in cost-benefit analysis, for instance, where technologies are compared through placing their costs and benefits on a common economic plane (Heiskanen et al. 2000). In a perfect market, ‘price’ is often seen to be the means by which such comparisons can be conducted, such that the price alone will give the necessary information for purchasers to make a decision (Williamson 1991). This would seem to apply to software that is becoming commodified and sufficiently standardised that the term ‘software package’ is often used (Brady et al. 1992).
At the same time, there has been extensive discussion of software as an ‘informational product’ and the difficulties in ranking a commodity whose characteristics are not readily ascertained (Williamson 1985a). Close to the Economics and Management view, though differing in some important respects, is the position found within Computer Science and Software Engineering. Here, the major focus revolves around not how all of the systems compare with each other, but how they compare with the needs of the adopting organisation: how, in other words, they might ‘fit’ organisational requirements. The term attempts to categorise the commensurability between organisational requirements and the properties of the software packages on offer; i.e. the software which is most commensurable with organisational needs is said to have the best fit (Finkelstein et al. 1996; Alves and Finkelstein 2002).
Indeterminacy of properties and measures
While the quantitative approaches have portrayed procurement as organised around a narrow set of criteria, this view has been the subject of criticism. IS, STS, and Organisational scholars interested in technical change, by
contrast, have shown how technology choices are the result of a more complicated social and political process. Various accounts describe how there are often multiple, competing and contradictory assessments of the character and capacities of technologies - perhaps because they are new, complex or controversial. In much of this work, the choice of one technology over another is seen not to reside in the properties of the artefact as revealed by a formal technical or economic assessment but necessarily refracted through, or in some accounts driven by, the micro-politics of the organisation, the commitments of the various actors, prevalent supplier rhetorics, managerial fads, etc. (Pettigrew 1973; Swan and Clark 1992; Knights and Murray 1994; Koch 2000, 2001; Tingling and Parent 2002, 2004; Howcroft and Light 2006). Many from a discourse theoretic position end up following a position close to Steve Woolgar (Grint and Woolgar 1997) in his insistence that the material properties of artefacts are essentially unknowable and thus that the role of analysis is to reveal the discursive practices through which one interpretation wins out over another (Bloomfield and Danieli 1995; Joerges and Czarniawska 1998; Rappert 2003).
Similarly, just as technological choice is not dictated by the properties of an artefact, nor, in this position, is there seen to be a determinate relationship between formal assessment criteria and the purchase decision. Neyland and Woolgar discuss the rationale behind the decision to purchase a database within a university, and argue that formal criteria like ‘value for money’ figured as a ‘background relevancy throughout...[but] only informed participants’ activities in a highly indeterminate sense’ (Neyland and Woolgar 2002: 271). Having a much greater bearing was the ability of the procurement team to ‘persuade’ their colleagues that they had taken all such measures into account:
the ‘value’ in our value for money story was constructed in the process of convincing those connected to particular circuit flows and receiving verification from those connectors that we had accounted correctly, that we had judged value correctly and that the university should spend this money.
(Neyland and Woolgar 2002: 272)
The form of analysis advanced by Woolgar and co-workers suggests that we should question and unpack unexplicated assumptions about technologies as well as the measures used to assess them. However, in doing so this reduces the particularities of the decision-making setting to a political or rhetorical struggle. The decision criteria are seen as marginal to the decision, and, if not entirely removed from the equation altogether, are seen as indetermi-nate.2 While reading Woolgar’s previous work we learn that assessment measures should be seen as the outcome and not simply the cause of the procurement (Grint and Woolgar 1997). We agree. However, we need to go further than this. In our view, it would be difficult to understand fully the procurement decision - how comparison across the different technologies
was achieved - if we did not give some role to assessment criteria. How could properties be produced if it were not for these measures? The fact that numerous measures can be established to help produce and compare the characteristics of technologies and the lengths certain actors go to establish these measures is more interesting and important than Woolgar suggests.
Underpinning our own analysis is the view that analysis should not reduce phenomena to a rhetorical struggle. Rather, we see technology choice as the ‘co-production’ of both the assessment measures and the assessment. The argument is that it is not so much the properties of the systems that are important for establishing differences and similarities between the various software packages but the performance of various assessment criteria that give a form to those properties. These measures are not just descriptive: they are actively engaged in the constitution of the reality. Through working with these assessment criteria, the procurement team produce the properties of the different systems (through this cycle of disentangling, framing and overflowing). This view of co-production moves us away from the Woolgarian idea that any assessment is possible, that the properties of the systems are arbitrary, or that the assessment might become uncoupled from organisational reality. Our approach shares elements of, but at the same time differentiates itself from, the analytical outcomes of relativist Sociology (it is neither relativist nor realist - see Latour (1999) for an elaboration of this point). In this respect we need tools that are honed to identify the particularities of decision-making in what we call this ‘twilight’ or ‘grey’ spaces (Tierney and Williams 1991). The strength of such an approach rests on its ability to produce analyses of assessment and decision-making that go beyond the gulf between technocratic, ‘rational decision-making’ models and the emphasis on ‘discursive practices’ advanced by more critical approaches.
While we have highlighted those studies of technology selection on either side of the schism, there is in fact existing work that has looked to bridge these dimensions. Tingling and Parent (2004), for instance, describe a procurement decision that is the outcome of a ‘judgement’ and which is only later justified through mechanisms that are more formal. Since the decision appears divorced from the formal process, they describe these in terms of ‘ritual’ and ‘ceremony’. Their argument, which we in principle support, is that ‘rationality and ceremony’ need not be in ‘conflict’ but could in fact be ‘complementary’ (ibid.: 349). The weakness in their paper is that, rather than continue to develop this line of thinking, they conclude in a fairly conventional way, reducing the discussion down to one side of the schism, stating: ‘Decision processes and, therefore, the decisions themselves, may be socially constructed’ (ibid.: 349).
The social life of software packages
Interestingly, recent practitioner-based literatures, arising from specialist bodies providing expert advice to adopters, such as industry analysts, have
presented a more sophisticated view of software procurement than either the economic or sociological ones - capturing something of the ‘grey space’ that is being described here. They point to how criteria over and above price and product features are important when making procurement decisions. Not only is there an increasing number of (non-price as well as price) variables to consider, but there is often uncertainty about which are the important ones (Stefanou 2001; Herschel and Collins 2005). This literature - as well as some writing within the IS tradition (Oliver and Romm 2000) - undermines the notion that technical characteristics and fit can be ascertained through observation of the product or even ‘testing’, pointing out that a software package can only really be tested through actually buying the package and installing it (Bansler and Havn 1996). A common way of emulating the latter is for potential adopters to visit ‘reference sites’ where the software is being used. However, this can bring its own set of problems (as we will show). There may be few if any reference sites in the adopter’s specific domain. Even where reference sites exist, there may be uncertainties about how similar the demonstrator organisation is to the potential adopter. Likewise, given difficulties in assessing such a complex, non-material product as software, as well as its malleability (the scope to adapt and customise software), there are questions as to whether the system being demonstrated is the same or sufficiently similar to the one being procured.
Software packages are often conceptualised as a product or commodity and discussions of their commodification tend to focus on their stabilisation and standardisation (Brady et al. 1992; Quintas 1994b). We seek to conceptualise them in more fluid terms, however; as we have said, rather than simple ‘snapshots’ we argue that researchers should study the lifecycle of software packages and their longer-term evolution across multiple cycles of development and implementation. We note how many of the organisational software systems constructed nowadays are constantly being ‘recycled’ for use in different industrial sectors and are thus explicitly designed with the idea that they will be applied beyond the place(s) for which they were originally designed. Many CRM systems, for instance, are of this kind - described by one set of practitioners as ‘retooled version[s] of generic functionality’ (Herschel and Collins 2005: 2). The upshot is that, when choosing between systems, adopters may be assessing software that exists in a domain very different from their own. In this case, what is being procured is not simply the software but also crucially the supplier’s future intention and ability to modify the system to meet the specific requirements of intended adopter. This throws up longer-term questions as to whether the supplier is committed to the new sector, whether they will continue to develop software for this industry (or whether they are simply entering the market opportunistically [ibid.]).3 In these cases, what might the adopters assess - the reputation of the software supplier (as indirect evidence of their competence and future behaviour), or something else altogether?
Arriving after the event
There are surprisingly few systematic studies of software package procurement within IS research or STS (though see, for instance, Koch 2001; Tingling and Parent 2002, 2004; Howcroft and Light 2006). For this reason, we provide more detail than we have done in other chapters about just how we were able to do a procurement study. This oversight is somewhat paradoxical given the latter’s concern to subject to critical scrutiny the claims and capabilities of technology supply. In most cases, procurement is briefly introduced prior to the ‘implementation story’ (McLaughlin et al. 1999; Fleck 1993). The process by which a potential customer engages with technologies, selects between them and then commits to a particular supplier would seem to be of particular importance. One, perhaps prosaic, reason for this lack of attention is related to its timing and the consequent methodological difficulties in capturing procurement. Information system procurement is conducted infrequently; the earliest stages may be conducted over short periods of time and in a fairly ad hoc manner, with decision-making restricted to senior managerial and technical elites. By the time a technology selection process is identified and access negotiated by social science researchers, many of the key decisions will already have been made. As some researchers have noted, the large body of ‘technology implementation’ studies have typically been carried out retrospectively at some remove from the initial procurement decision, the completion of which underpins the identification and selection of the case. Thomas (1994) argues that many researchers take the formal procurement decision as the starting point for beginning their study, even though many of the issues, relationships and alliances that shape later phases will already have been formed during the procurement process itself (the same point is made by Tingling and Parent 2004). The consequence, as McLaughlin et al. (1999: 104) observe, is that choice is described by informants as having occurred in a ‘carefully managed’ and ‘rational’ manner, with researchers thus left to rely upon these ‘managerial accounts’. While we agree with these authors about the need for more research into procurement, their view perhaps exemplifies the dominant sociological view of formal procurement as merely rationalising decisions made by powerful players elsewhere.
Research design
The development of this study of procurement was fortuitous. One of the authors, along with a group of management and computer scientists, was investigating the changing nature of information systems within the public sector. The research team had negotiated access to Melchester Council with the goal of observing a ‘joint venture’ partnership the council was attempting to set up with a large IT company (see below). At the same time, the council were also hoping to purchase a Customer Relationship Management
(CRM) system; a form of system that allows organisations to capture and manipulate greater amounts of information about their customers; as well as ‘integrating’ that information across the enterprise. The procurement was seen by many members of the council as the first ‘real test’ of how and whether the partnership would work. Whatever the outcome, it represented an exceptional opportunity for studying software package procurement, particularly in terms of early and extensive access.
Data collection and analysis
One of us conducted a participant observation at the council during the period of the selection (for almost a year). During this time he sat in on and observed meetings of the Procurement Team. There were approximately a dozen of these, during which he would sit taking notes. These meetings were often conducted within the council chambers, involving the participation of a wide range of people who would sit around a large wooden table. For reasons we describe in more detail below, these meetings were often tense and involved heated exchanges. There was much uncertainty and confusion among the various actors about what the best solution might be. Often these meetings would be dominated by the more technical members present, but the Chair of the group would attempt to counteract this through soliciting the views of others (by ‘going around the table’). During these sessions, the fieldworker would sometimes be asked for his opinion of what he thought of the various solutions on offer and he would answer as best he could.
The fieldworker also attended several vendor presentations conducted on the council premises. These were highly interesting affairs and are described in more detail below. On one occasion, he travelled with the Procurement Team by train when they visited another council in the south of England (‘Bingham’) to view a demonstration of their CRM system. Once there, and along with the Team, he also watched a demo of the Bingham software and participated in a visit to their call centre where the software was being used. While at the call centre, he sat with an operator and watched the taking of calls and use of the system. Various written materials that were passed between the Team members were also collected, including testimonials from existing users of the potential systems, printed out email correspondence between Melchester and the industry analysts Gartner, and the typed-up notes of telephone conversations with these analysts.
More formal interviews were also conducted with members of the Team at different stages throughout the procurement. These usually lasted an hour or so and were tape-recorded. Discussions tended to begin by the researcher asking the interviewee to bring him up to date on the unfolding procurement story and any meetings or important discussions that he might have missed. Subsequent questions were aimed at understanding which of the solutions were preferred by the interviewee. Usually the interviewer would ask for reasons for particular preferences. Some of the Team members
were re-interviewed at a later stage in the process, and during these sessions questions tended to focus on whether preferences had changed. We were able to interview one of the suppliers, the Sales Director of the firm that eventually won the contract (but only some time after the initial procurement process). During this meeting he was asked to reflect on the process and why he thought his company had been successful. We were unable to talk to the other vendors. Despite requests for meetings, many pointed to logistical difficulties (in their work they would be travelling to Melchester Council from different parts of the country to deliver presentations, meet with key personnel, and then leave quickly the same day, with potential customers to visit in other parts of the country).
As we began to make sense of and analyse the rich and voluminous amounts of data that we had collected, rather than be guided by prior theoretical knowledge, we paid particular attention to the terms and concepts used at our sites. We did not enter the field hoping to observe the performance of comparative measures. Rather, the framework and ideas that are reported here were arrived at much later. This is perhaps one of the most appealing features of ethnography: that it promises access to the issues, concepts and categories that are deemed relevant by those in the field, not those imposed a priori by the researcher. The ‘new Sociology of Technology’ has further applied the ‘symmetry principle’ from the Sociology of Science to the analysis of technology. This regards the need to suspend belief about ‘truth’ and ‘falsity’ in scientific disputes, claims about the success of a technology, or the ‘social’ and ‘technical’ dimensions of a technology, if the nature of these phenomena is still being negotiated by those under study (Callon 1986b). We employed similar sorts of criteria when thinking about assessment measures.
Public procurement
The public procurement process and how it shaped the selection context
The procurement process had a number of distinctive features rooted in the governance of public procurement (the requirement for transparency; the emphasis on price) and the trans-organisational character of the system being acquired. Typically, public agencies have well-established and highly regulated processes for major procurements including new computer systems and other infrastructural technologies. Normally, the process would start through the issuing of what is called an ‘OJEC’; meaning that a ‘note’ is placed in the Official Journal of the European Communities describing the organisation’s requirements and inviting interested suppliers to submit comprehensive tender documents. This is a complicated administrative and legal framework which seeks to eradicate discriminatory purchasing through opening up purchases to broader international competition (Martin et al.: 1999). One outcome of the OJEC process is that there is a general
requirement to maintain a certain level of transparency during procurement. This was particularly important as past decisions were often subject to internal and external audits. A further feature shaping public sector procurements was ‘price’: Melchester Council was obliged, like other public organisations, to achieve ‘best value’ when making purchases. Finally, the composition of the Procurement Team was noteworthy as it was made up of representatives from across the organisation and not simply technologists. The major groupings were the Customer Services staff, who would be the primary end-users of the system, and the IT personnel. There were also a project manager, a chairperson and two from other parts of the council (the Housing Benefits section and the Environmental Department).
Bringing options to the table
Offering 1
Melchester Council were, as noted, in the process of establishing a joint venture partnership with a major firm we call ‘JV Partner’, to assist it in technology acquisition and systems integration. JV Partner’s first task was to advise Melchester on the most appropriate CRM solution, which they did, putting forward a large US-owned software supplier which we will call ‘BigVendor’. JV Partner described in its documents how it had something of a ‘unique relationship’ with BigVendor and therefore had no hesitation in recommending them and their systems to Melchester. In addition, as one of the world leaders in the provision of this kind of packaged application software, BigVendor had recently announced they had set their sights on becoming the leader in providing CRM to UK local government. Everything appeared to suggest a good future working relationship. BigVendor began to visit the council to carry out initial scoping work and interview staff about their requirements. However, shortly after these initial visits, JV Partner suddenly announced that it was no longer recommending BigVendor but a Latvian/American software house that we are calling ‘MiddleVendor’. No formal explanations were given for the about-turn, so people in the council just assumed the two firms had simply ‘fallen out’.
Offering 2
MiddleVendor, which up till then was unknown to the council, visited Melchester and conducted a two-week ‘discovery session’. After this, they made a full-day presentation to council staff, including giving a software demonstration (described below). However, during the question and answer session following the presentation, some questions arose regarding the exact cost of MiddleVendor’s system and in particular, what part of that fee would be passed to JV Partner. Presented on a PowerPoint slide was a ‘consultancy fee’ in excess of one million pounds that could not be adequately explained
by the presenters. Despite attempts by MiddleVendor to clarify the issue, various members of the Procurement Team left the room highly suspicious about whether the council would be getting value for its money. These suspicions were not resolved, only heightened, in a later meeting, and it was thus decided to invite a number of other suppliers to come to the council so that MiddleVendor’s prices could at least be compared.
Offering 3 (and Offering 1 again)
A small software house from Northern Ireland, which we call ‘SmallVendor’, had also tendered for the project and they too were now invited to visit the council offices. This company was also unknown to the council, having previously developed software for the telecommunications sector, but had recently implemented its system in one of the largest UK local authorities. Also in the same week, and to everyone’s surprise, BigVendor contacted the council to ask if they could have a second chance to present their offering. Despite much scepticism (and even some bewilderment), they were invited in once more. In summary, then, there were now three possible options on the table among which the Team had to decide.
Dividing possibilities
From the beginning of the procurement process, it was clear that each of the options was attracting supporters. For instance, one issue the Team spent time discussing was the ‘type’ of system they should procure and its implications for organisational change. The choice was between what was described as a ‘highly prescriptive’ mature solution (BigVendor) and the newer, more flexible software packages (MiddleVendor and SmallVendor). It was widely understood that BigVendor’s solution would mean applying ‘standard templates’ to the council’s business processes and that this would require reengineering much of the organisation. For Barry, who was chairing the Procurement Team, this was seen as a good opportunity for the council to ‘update its processes’. In one meeting he describes how ‘there is the pressure to adopt new and better practices’; BigVendor’s system templates were presumed to embody best (or at least better) practice. And BigVendor’s system ‘will provide us with a focus for organisational change’. A similar set of points was made by the manager of the Housing Benefits section as she too listed the advantages BigVendor’s solution might bring.
Customer Services staff were less impressed by the BigVendor solution. They particularly problematised the generic nature of the package. In stark contrast, however, they were openly describing the MiddleVendor solution as ‘brilliant’. The reason for this enthusiasm was that, unlike BigVendor’s ‘out of the box’ solution, MiddleVendor would be building a software package specifically for Melchester. MiddleVendor did not yet have a local government CRM system (their solutions were being used within the financial services
sector only) and they were thus proposing to use Melchester as the ‘pilot site’ for this redevelopment. This was seen in a positive light by the Customer Services staff:
From a Customer Service point of view, it is brilliant. What they are going to offer us is everything that we could possible want because they are going to build it around our existing systems.
(Author interview with Kerry, Customer Services)
From a Customer Service point of view, we strongly believe that MiddleVendor could deliver what we want.
(Author interview with Christine, Customer Services)
However, the IT staff within the Procurement Team appeared to favour the third option, SmallVendor. From their initial presentation, it was said to be the ‘cheapest’ option. When pressed to give further reasons they stated that they liked the fact that SmallVendor had already developed a version of the CRM system they were promising to deliver to Melchester at another council in the UK (‘Bingham Council’). In addition, this was seen to give the solution, to use their term, ‘added credibility’. One of the IT managers describes how:
My personal view is that SmallVendor is the least expensive; and delivers what we are looking for; comprehensive in looking to the future; and has credibility of working with the largest council in the UK.
(Fieldwork diary, Procurement Team meeting)
It also meant that Melchester could form some kind of partnership with Bingham Council to develop the CRM system further. Such partnerships (for example, for joint procurement by local authorities) were being heavily pushed by government and this appeared to offer the possibility of ‘killing two birds with one stone’ (comment by the Chair of the Team during a Procurement Team meeting).
In summary, after the first round of discussions there was no clear choice emerging; to the contrary, various sections of the Team were becoming closely wedded to different solutions. Each had their own criteria for evaluating the packages. One group was attracted to the way their favoured system would ‘fit’ with existing processes, another thought a standardised system would help modernise and ‘update’ the Council’s business processes, and another group saw a third system as attractive because of ‘price’ and the fact it had an existing local government customer. In what follows, we will show how the Team attempted to unlock this situation through disentangling the systems from their existing contexts as a precursor towards achieving the realignment needed to shift technology preferences. They did this through introducing new measures by which to compare the systems.
Narratives of success
Evidence about the software packages was collected in the following ways: observing supplier presentations and demonstrations of their systems; soliciting and reading testimonials from existing customers (so called ‘reference sites’); and seeking advice from third party experts. In this section, we will focus on the testimonies and return to the other elements later in the chapter. The testimonies were made up mostly of written replies to questions sent out by the Team concerning the nature of the system and the level of service and support provided by the supplier. Telephone interviews were also conducted. Completed questionnaires and typed-up summaries of telephone conversations were passed around members of the Team. These testimonies can be divided into different categories. First, many simply described the effective operation of the system without any qualification:
[Our internal people] rave about the savings in man-hours since we implemented workflow queues to our operational sections and they can query and print documents without having to dig through rolls of microfilm. We have also given query ability to the court judges and their staff. They were very pleased and complimentary.
(US County District Clerk talking about MiddleVendor)
Agents can now perform much better. For example, with ‘Case Management’, agents can now report complicated customer complaints electronically through Frontline for resolution at the back-end.
(SlowLink talking about SmallVendor)
[BigVendor’s] CRM solution has transformed the way the city council thinks about IT. We are well on target for meeting the government’s deadline for e-enabling all our services by 2005.
(Hill Council talking about BigVendor)
There were also those that focused on the role of the suppliers themselves. Like the systems descriptions, these were usually unconditional in their support:
To date they have delivered what has been promised.
(Island Telecom talking about SmallVendor)
As a telecom we work with many vendors within our company, and the solutions we develop with [MiddleVendor] are more of a partnership than a client/vendor relationship. The expertise offered in development, the support provided in implementation, and the confidence we have in the resulting end production/solution, make working with [MiddleVendor] a pleasant and productive experience.
(TelCo talking about MiddleVendor)
Intriguingly, when we were able to find discussions of failings, these were almost always addressed in a somewhat indirect manner. For instance, authors were careful to show how responsibility for problems was distributed among a range of actors:
We have experienced some complications that have resulted in the dissatisfaction of the staff at various periods, but again, I think the department’s administrators share some of this responsibility.
(NewState Department of Insurance talking about MiddleVendor)
We did experience some downtime but the majority was our own internal issues, i.e. network problems, personnel turnover, and some of it could be directly attributed to problems with the custom application written by [a third party vendor].
(US County District Clerk talking about MiddleVendor)
most service interruptions have been attributable to our own network and software change management.
(TelCo talking about MiddleVendor)
We would describe the bulk of the documents and references received as ‘narratives of success’ (Fincham 2002). There were very few negative comments made about the suppliers and their software systems.4 We can also interpret the insertion of these (negative) statements and the counterstatements that qualify or explain them as ‘modalities’, in Latour and Woolgar’s (1986) sense, that they are used to emphasise the problematic elements (and perhaps also to add a sense of realism and credibility to these testimonials).5 To sum up, this phase of procurement was organised around the collection and reading of testimonials. The interesting issue about these narratives is that they were uniformly positive and allowed the Team little scope for comparing the various suppliers. It appeared that the Team would have to find alternative measures if disentangling and framing were to occur.
The provenance and status of the software packages
One set of issues that dominated Team meetings concerned the nature of the system MiddleVendor was promising (where it came from, and the kind of reference sites that they were offering). As we have said, many generic software packages are ‘recycled’ across different industrial sectors. The assumption is that because a system has been made to work in one class of organisation it can also be reconfigured for use in other settings (or the setting itself can be reshaped to more closely match the newly arrived technology, or, as more usually occurs, there is a combination of both). However, this recycling presents difficulties for procurement teams, for they may be
assessing software in settings that are drastically different to their own, as well as software that is not yet in the form that has been promised to them. One member of the Team describes this difficulty:
[MiddleVendor] haven’t got any system anywhere in the world with the mix they are proposing to give to us. They have only one site which isn’t yet live in the UK. All the sites they mentioned to us in the United States, people visited some of them and had lengthy discussions with them, all had bits and bobs of the stuff they were going to give to us but no one had much of it working together.
(Author interview with Project Manager)
Reference sites are interesting in that they seemingly give prospective adopters the reassurance that the software does work in some places at least. However, this reassurance is only as good as the intellectual comparisons that adopters are able to draw between the reference site and their own setting. To the extent that such parallels cannot be established - or a recognisable system cannot be demonstrated - the adopter is then required to ‘imagine’ what the finished system might look like and how it might operate in their environment. In other words, this requires disentangling these ‘bits and bobs’ from the specificity of the reference site, imagining them in a different format, and then projecting this image on to the Melchester setting. In this case, when presented with a number of MiddleVendor’s references sites it appears that the Team were unable or unwilling to make these necessary mental leaps, to envisage technologies that did not yet exist, and to disentangle the software from the sites in which it was presented (we return to this issue later in the chapter).
Interestingly, Ken, who is an IT manager, describes a similar set of concerns about MiddleVendor but also emphasises the danger of selecting a system that did not yet exist:
Problem with [MiddleVendor] is high cost, no track record in Europe, in UK. We spent time getting references. The people we talked to were very impressive, though their presentation was only on paper and there was nothing to see. They would be developing a bespoke product for [Melchester]. We would be guinea-pigs.
(Fieldwork diary, Customer Service workstream meeting)
Here, concerns were raised that MiddleVendor would be building a software package specifically for Melchester. Somewhat paradoxically, this had initially been seen as a highly positive feature of MiddleVendor’s proposal by the Customer Services staff, but it was now recast or reframed as a significant weakness. The Melchester team were unable to disentangle the system from the supplier. Would MiddleVendor stick to its promise of adequately reconfiguring its systems so that it was useful in this new setting? There appeared
to be an easy way to resolve these questions - get the suppliers to demonstrate their capability!
Credibility contests: providing evidence of competence6
Establishing credibility is said to be important for software suppliers, especially where they are attempting to build new systems or recycle existing ones.7 Indeed, it was though using the language of credibility that members of the Team sought to reframe the procurement and in so doing pro-blematise MiddleVendor: ‘there was nothing to see’ and everything they had was ‘only on paper’. In response, during MiddleVendor’s visit to the council they spent time attempting to demonstrate their credibility. They did this most explicitly by undertaking a ‘Proof of Concept’ (POC) project which was presented as nothing out of the ordinary, as if the supplier was required to demonstrate its competence for every potential customer. The supplier’s documents describe how:
Many engagements involve a Proof of Concept. The Proof of Concept generally involves confirming that the solution will, in fact, solve the most pressing business problems. In such cases, the back-end systems are absolutely unchanged during the project scope. Capacity is typically strictly limited so that capacity engineering is not a risk (p.42).
Once the POC was established as a ‘normal’ part of the procurement process, there was an explicit attempt to signal that it was capable of standing in for a whole range of things: the not yet established skill and competence of staff and the credibility of the supplier.8
The objective of the Proof of Concept is to demonstrate to the [Melchester] IT community that [MiddleVendor] can integrate to [Melchester] ICL mainframe applications and can navigate the [Melchester] Oracle databases (p.23).
POC system should demonstrate the ability of [MiddleVendor] the Integrator component to access existing [Melchester] data systems infrastructure (idem).
POC should show the capabilities of the [MiddleVendor] Portal component (idem).
It appeared that MiddleVendor, having flown in programmers from Latvia, sought to demonstrate its competence in a dramatic manner through attempting the hardest possible case:
Specifically, POC examines the operations required to establish a customer direct debit (‘an arrangement’) for council tax payments. The scenario that the POC is trying to cover has been discussed with
[Melchester] users and was agreed would be a good test case of the technology due to the complexity and inconvenience of the procedure using the mainframe application (idem).
We say ‘hard case’ because it required MiddleVendor to achieve integration between several discrete systems. When the day of the POC arrived, there were gathered in the room the Procurement Team, other members of the council involved in setting up the wider joint venture, and several JV Partner staff. MiddleVendor staff introduced the event by announcing that because ‘we are a new company’ and because no one had yet ‘heard of us’ that they wanted to ‘demonstrate their expertise’. This is what they then attempted to do. Apart from some initial problems in getting the system to run, the POC appeared to proceed as planned. Through a process they described as ‘screen-scraping’ they were able to access a number of Melchester databases and run a simulation of how a council officer would deal with a typical customer enquiry. Afterwards there appeared to be a wide consensus among those gathered that the MiddleVendor presentation, the POC demo and the expertise of their staff were ‘excellent’. This view of MiddleVendor was repeated throughout the council. For instance, one member of the Team stated:
I	have to say that the level of expertise of these people who came in was absolutely superb. In fact, some of the questions they came back and asked you wouldn’t expect a council officer who had been there for several weeks to ask.
(Interview with Christine, Customer Services Manager)
Significantly, however, not all of the Team appeared as convinced. In a meeting between the Team and other council staff, one IT manager casts some doubt on whether the POC did in fact tackle a hard case:
There is proof that they can use tools [to access data] but other tools were already in place. I still can’t comment on the robustness of their solution. We have followed up references such as the [US State Department of Insurance]. But references are limited. I would like to see a site where integration has been done.
(Fieldwork diary, Procurement Team meeting)
In other words, this manager was unwilling to allow the POC to stand in for a demonstration. He questioned the ability of this measure to test the supplier significantly. For him ‘real proof’ would be to go to a site where the software package was already integrated with legacy systems. Another IT manager makes a similar point and also questions the long-term viability of MiddleVendor:
I’m impressed with [MiddleVendor] but we have given them access to our processes, which we didn’t give to the others. Brian’s [from JV
Partner] suggestion that [MiddleVendor] is the best for us is bollocks. There is more comfort in going with [BigVendor] as we know they’ll [still] be there in several years.
(Fieldwork diary, Procurement Team meeting)
Despite the fact that MiddleVendor were able to demonstrate their expertise, there still appeared to be concerns about their competence and now, according to this latter comment, their viability to survive as an organisation. One way of resolving these concerns appeared to be to solicit the opinion of a ‘credible’ other.
Have the experts heard of them?
During the procurement, Melchester employed IT analysts the Gartner Group. Analysts like Gartner present themselves as providing ‘impartial’ information about particular software systems and the technical and financial standing of suppliers. They are experts in the classification and positioning of vendors (see Chapter 7). Having sought their opinion in the past, Melchester thus turned to them once more in order to resolve the questions about MiddleVendor’s longer-term viability. Several lengthy telephone calls were conducted and the results of each were typed up and circulated among the Team. In the first of these calls, Ron, an IT manager, asks Gartner for their opinion of MiddleVendor and is somewhat surprised to be told by a Gartner consultant (‘Ed’) that:
[Gartner] has a list of some 500 vendors of CRM, many of which [Ed] meets on a regular basis to track the development of their products. [MiddleVendor] is not on the list; he had not heard of them.
(Ron’s circulated notes)
The consultant said he would cross-check with a colleague based in America and call back. A week later Ron reports how:
[Ed] has been in touch with his colleague in the USA, but [MiddleVendor] were unknown to him as well. Gartner can therefore not provide any research papers into the company or its products.
(Ron’s circulated notes).
What we see here is that, through the introduction of these various modalities (they are not on Gartner’s list, Gartner had not heard of them, and there are no available research papers), Gartner begins to cast doubt on MiddleVendor’s credibility. The episode becomes more interesting still when Melchester reports this to MiddleVendor and they in return seek to play down its significance, suggesting that it is the result of a simple ‘categorisation’ difficulty:
Their comment, when it was pointed out that they were unknown to Gartner, was that in the two years the company has been in existence it has not spent any time or effort in making itself known to industry analysts. This is because at present these companies do not have a category for what they are offering (the integrated framework approach).
(Ron’s circulated notes)
Here, rather than simply accept Gartner’s assessment, MiddleVendor responds by casting doubt upon Gartner itself and its methods of compiling briefing documents. In other words, MiddleVendor insert their own modality (‘these companies do not have a category’) in an attempt to diminish the significance of these judgements. The matter does not end there. Gartner, on being informed of these objections, attempts to clarify (defend?) its position by discussing the provenance of its own information:
Gartner gets 80-90 per cent of its information directly from Gartner clients talking about their experiences and technologies and not from being briefed by technology vendors. Nonetheless, Gartner hosted 150 CRM vendor briefings in Europe last year, of which thirty or so were instigated at Gartner’s request due to client calls. No client has asked us to ask for a briefing from [MiddleVendor]. It does not mean that [MiddleVendor] is a bad solution - it just surprises us that we have not had a request.
(Gartner’s email circulated among the Team)
Here, Gartner appear to backtrack somewhat on their initial statement by arguing for a lighter form of modality (‘It does not mean that MiddleVendor is a bad solution’). Another Gartner consultant makes a similar point a week or two later during a conference call. The note on the conversation describes how:
She advised [Melchester] not to read too much into the fact that they were not known to Gartner. It was in [MiddleVendor’s] interest not to be classified with other CRM vendors as they offer broader services. They did not want to be seen as simply a software vendor. They had perhaps failed to take a more pragmatic approach to this.
(Ron’s circulated notes)
This exchange is interesting, for at stake here was the issue of who was qualified to pass judgements on suppliers and their technologies. In this sense, Gartner are the knowledgeable specialists who analyse the biography and careers of objects: they know where the software originated and which other organisations are using it, and they build up extensive knowledge through their contact with these user organisations, ‘talking about their experiences’ with the technology. The existence of firms like Gartner is a symptom of the problems user organisations face in assessing claims about
software packages (and of having to operate in the ‘twilight’ or ‘grey’ space between formal and informal knowledge about technologies). This prompts two questions: what makes Gartner’s assessments credible? And what does Gartner’s contribution add to the assessment of the MiddleVendor system? A credible witness is seen to be a source who could be believed - one who was ‘impartial’ and had no particular axe to grind (Shapin 1994: 212), inspiring ‘a just confidence’ by applying genteel virtues of ‘integrity and disinterestedness’. In this respect, Gartner do not make public assessments about particular suppliers but merely comment on the availability of third party information, presenting themselves as the neutral collector and collator of that information. In other words, Gartner do not so much produce technical claims as act as the carriers of ‘community knowledge’.
However, Gartner’s assessments do not close the outcome. There is space for discussion, for example, about whether their methods may discriminate against MiddleVendor. Gartner make their assessments accountable.9 However, the ambiguity opens up space for doubt. One reading of this exchange is that Gartner are passing the ultimate judgement on the status of each supplier back to the Team. It is the Team who must now decide on how to interpret this information. It is they who have the ability to leave the issue open or to ‘say what is to be seen’ (Munro 1995: 441). For the IT staff within the Team, however, there is little doubt about how to read this report. In one meeting, Neil describes how: ‘Gartner said that with MiddleVendor we would be taking a risk’. Similarly, Fred echoes this comment by asking: ‘Who would sign up to a company that no one has heard of?’10
It becomes clear that MiddleVendor will be removed from the table when its initial supporters, the Customer Services team, begin to articulate and repeat similar views. Kerry, who had previously articulated the benefits of MiddleVendor, now presents a somewhat different view:
what we were always doing was chasing a concept. We hadn’t yet seen, and we still haven’t seen anything to prove they can do what they say they can. In my mind I have no doubt about their professional expertise and ability to deliver something, but until we can see something physical and some kind of evidence. The authority has put too much into this kind of project to be seen as a pilot for [MiddleVendor] really within local authorities and within the UK even. From what I can gather they have not delivered a CRM system in the world, never mind the UK, so we were always going to be guinea-pigs. That was unsettling a lot of people.
(Author interview with Kerry, Customer Services)
To sum up, we have seen how the procurement was initially framed and then reframed as various comparative measures were introduced and some were recast. While MiddleVendor were keen to demonstrate their capability at the outset of the process, Team members sought more concrete ‘physical’
evidence of this capacity. Having once enthused about the expertise of MiddleVendor, the Customer Services people now too sought more evidence. Rather than seen themselves positively as a ‘pilot site’ (as before), this was now recast in more negative terms as their potentially being the vendor’s ‘guinea-pig’. In the end, the result was that the Team came to the conclusion that MiddleVendor lacked both an actual software package and, through Gartner’s intervention, credibility.
Shifting technology preferences
Several months have passed by now and there is the realisation among the Team that they will have to simplify the process significantly if they are going to reach a decision. Barry, the senior officer chairing the meetings, attempts to get the group to identify their preferred option:
We need to make a decision on who we are to go with. The three suppliers, are they any good? What do you think of the three products offered so far? We need the technology and value for money.
Going around the table, it is the IT experts who speak first, and it immediately becomes clear that SmallVendor is their favoured option:
Ron, IT Manager: SmallVendor has all the technology required. It is the cheapest, with ‘transparent costs’. It is the best value for [Melchester] City Council.
Ken, IT Manager: SmallVendor looked an excellent solution.
This is followed by the remainder of the Team who, because of the strength of opinion of the IT staff, are now divided into three separate categories of individuals:
1	those who are convinced by the capabilities of the SmallVendor system and who will subscribe to it and support it:
Kath, Customer Services: Err on side of [SmallVendor]. [MiddleVendor] is US and Latvian. [SmallVendor] is closer to home.
2	those who will reject it outright because they favour another solution:
Kerry, Customer Services Manager: [BigVendor] are arrogant. [MiddleVendor] are people- and service-focused. [SmallVendor] is technology-driven.
Ed, EnviroCall Manager: [MiddleVendor] would do job. I have issues with [SmallVendor].
Dianne, Housing Benefits Manager: [MiddleVendor].
3	those who are neither for nor against SmallVendor but who seemingly could be convinced of the merits of the system:
Helen, Customer Services Director: [SmallVendor] appeared committed and had enthusiasm, were new to market, and were pleasant. But from a service perspective they would need to demonstrate.
Christine, Customer Services Manager: [MiddleVendor] provides more of what we want. [SmallVendor], I didn’t know enough about. They didn’t refer to our priority areas at all. We would need to have more conversations with them.
Richard, Project Manager: [SmallVendor] haven’t demonstrated that they could do it.
Barry attempts to move the meeting forward by focusing not on the criticisms raised by the middle group but on the possibility of convincing the third group of the merits of the SmallVendor system. In so doing, he avoids the need for the whole Team to discuss the issues raised by those hostile to the choice. There is clearly as yet no consensus in the group regarding which system is the better, and they continue to evaluate the systems according to a wide variety of measures. Rather than going on discussing each difference for several more hours, Barry attempts a form of rhetorical closure on the process by suggesting that they select SmallVendor as the preferred solution. However - and he spends considerable time emphasising this point - only if it can be shown to be the best system:
Is that it: [SmallVendor] is what we need? [There are some nods of agreement from around the room.] Our proposition then is to go ahead subject to further checking. Our view is of ‘caution’. All of this must stack up.
(Fieldwork diary, Procurement Team meeting)
We describe below how SmallVendor is shown to be the best system, but first we see how JV Partner further complicates the procurement through attempting to introduce other assessment measures.
‘We are duty bound to get to the facts’
It has been scheduled that JV Partner staff should join the above meeting, where they will be briefed on the progress of the procurement decision. Within the Team there is a fear of upsetting the development of the joint venture partnership and this is especially troubling since they are now proposing to reject JV Partner’s proposal (of MiddleVendor as the best solution for Melchester). The JV Partner people enter the room and are invited to present their evaluation of the various solutions. They do so through pointing out how difficult it was to compare each of the packages: ‘It was a bit like comparing apples and pears,’ describes one member. Despite these
difficulties, they had managed to put together a matrix, choosing to rank the packages according to the ‘risks’ they posed to the council. As expected, they identified MiddleVendor as the least risky solution, then BigVendor as clearly not a ‘good solution’ for the council, and then SmallVendor who they ‘didn’t really know much about’ but despite this still considered posed the ‘biggest risks’ to the council.
The majority of these risks were linked to the technical capacity and competence of the supplier and also to an extent on JV Partner’s analysis of the future of the CRM software package market. It was generally agreed that the SmallVendor system would require large amounts of ‘back-end integration’ before it could be made to interface with numerous other legacy systems within the council. This work would be carried out by SmallVendor and Melchester staff and, as JV Partner saw it, this was a significant risk:
[SmallVendor] have no experience of mainframe integration; [in their presentation] they said they had ‘two buddies’ from ICL who can help them out. There will have to be bespoke work, and the risk shoots up if [Melchester] staff do it. We want to ‘de-risk’ it and move towards packaged solutions. I wouldn’t be happy with [Melchester] staff doing the integration...Got to push risk back on to the supplier.
(Fieldwork diary, Procurement Team meeting)
Another member of JV Partner adds:
The [SmallVendor] proposal doesn’t have ‘middleware’. No enterprise is doing [back-end] integration these days. At [Bingham] City Council they are using [a company called] Cavendish to do the middleware.
(Fieldwork diary, Procurement Team meeting)
The extent to which SmallVendor was a risk was disputed by members of the Team and this led to further uncertainties concerning just what each of the vendors and their packages could do. For instance, in response to the comment about the use of middleware as opposed to carrying out back-end integration, one Melchester IT manager (Ken) describes how ‘this is what we are talking about doing [with SmallVendor]’, only to be told by JV Partner: ‘No, you’re not suggesting corporate middleware. Only [MiddleVendor] has ‘integrator” which is corporate middleware.’ There was further uncertainty regarding how much of the work the Melchester IT personnel would be carrying out themselves. While JV Partner thought SmallVendor was risky because it would mean using the in-house staff - ‘we don’t want to rely on in-house staff, we have to use packages’ - Ken describes how MiddleVendor actually were proposing to use ‘our COBOL skills anyway’ and that ‘we [the in-house personnel] are involved in whichever solution we go for’.
These points highlight the various uncertainties and ambiguities regarding the capacities of the technologies and the capabilities and standing of
suppliers, and the introduction of a further comparative measure (risk) has not lessened but only heightened this. There were also a number of tensions developing between the two groups, which Barry attempts to deflect by taking the discussion on to a topic where surely they can all find some agreement - the stated price of each package. Barry makes the following comment:
Superficially [SmallVendor] looks cheap. It is worthwhile having further dialogue. We had [MiddleVendor] here for two weeks who were credible but superficially expensive. Is it worthwhile having further discussion with [SmallVendor]? We are duty bound from a local authority point of view because of price.
(Fieldwork diary, Procurement Team meeting)
Yet JV Partner does not agree that SmallVendor are cheaper. They argue that the Team are not making a correct comparison:
You are not comparing like with like here. The cost of the integration is not added in with [BigVendor] or [SmallVendor]. Moreover, [SmallVendor] are new to it and back-end integration costs money.
(Fieldwork diary, Procurement Team meeting)
It seems that the uncertainty around each of the packages continues to grow, and the possibility of comparison and putting the systems on a common plane becomes more distant. To cool the conversation down, Barry argues for further research to be done: ‘Why don’t we find out what the position ‘is” and not what we think?’ In contrast, Brian thinks further studies will only lengthen the process and that it will ‘go on for another six months’. Barry describes how:
We are duty bound to get to the facts...Need for further work, to do it properly with some accuracy.we have more time than we thought we had. If it happens, it will happen in the context of the Joint Venture. We need CRM but we don’t need to make that decision before we know we have the Joint Venture. [Melchester] will have to make that decision on its own, even if the Joint Venture doesn’t come off. I have no choice but to do further evaluation. And it won’t take six months.Have to show it’s properly evaluated. That work needs to be done jointly.Need ‘like for like’ evaluation. No good squabbling; should be doing further evaluation jointly. At the end of the day we have to decide, even if these tensions exist.
(Fieldwork diary, Procurement Team meeting)
Through further research, Barry imagines a process whereby the differences between each system could be clarified so that all the uncertainty could be
reduced. Moreover, the downside to these delays was that the Team were finding it increasingly difficult to sift between the various technologies and their suppliers. While they had previously agreed (among themselves) that they would conduct further work on just one system (SmallVendor), JV Partner assume that they are considering looking at two systems. On top of that, one IT manager within the Team (Fred) adds that if they continue to evaluate SmallVendor and MiddleVendor, then, for the sake of due process, they should also allow BigVendor back to the table. His argument is that they might ‘cry foul on the grounds of not having had the same access’. In other words, rather than remove suppliers from the table, they were forced to re-admit and review all three for a third time!
Seeing is believing
We have discussed how those within the Team were laboriously attempting to shift the boundary concerning which assessment criteria were relevant to the selection. For instance, we have shown how much time was spent collecting and interpreting testimonies from reference sites, observing displays of competence, deciphering the provenance and status of the various systems, and questioning the standing of the suppliers. However, these measures were not sufficient to assess suppliers like BigVendor, who have working packages in several local government sites and, as one of the largest software suppliers in the world, have a substantial standing among users and industry analysts alike. Thus, given there were many reference sites available, the Team began to invest time in visiting these sites. Below, we show how attending and observing a successful demonstration became a further comparative measure.
What, down again!
BigVendor appeared to have a major advantage over the other suppliers as its systems were already installed and working within many UK local authorities. However, when members of the Team travelled to see one of these systems in use they were told that ‘unfortunately the system was down today’. Strikingly, the same thing happened at a number of other BigVendor sites; and even when they made return trips, they found the systems down once again! Christine, incredulous about the situation, describes this in one interview:
I visited a number of [BigVendor] sites and I haven’t yet seen their CRM solution working anywhere [NP: Where were those sites?]. I visited [Rochester], and we have been to [Lichester], and then there was a party of colleagues from the Customer Service Centre who went back to [Lichester] a second time eight weeks later and there was nothing working then!
(Author interview with Christine, Customer Services)
The difficulties were reported by the reference sites to be the result of rare (?) technical glitches, and despite their reassurances that they were indeed satisfied with the BigVendor solution, the Team were now beginning to express concerns. After one such visit, Ron, the IT manager, describes how he thinks:
Technically, [BigVendor] can provide a solution, I am sure of that. I don’t think I can sit and say there is a technical problem with their solution, I don’t think there is. I think in terms of their track record, which worries people, we have been to two [BigVendor] sites and haven’t seen the application running properly yet. I think there is a feeling that, yes, they might become the leading suppliers and, yes, they are one of the companies that the government is talking to about producing standard solutions for local government. But right at this moment in time there is a high chance of anything produced for Melchester not working first time.
(Author interview with Ron, IT manager)
To counter these rising concerns, BigVendor tried to persuade the Team to make another visit to one of their reference sites but, unsurprisingly, this was met with some reluctance. In a last-ditch effort, BigVendor offered to run their demo via a broadband link, but as there was no such link at Melchester the Team were invited to travel some 100 miles to one of BigVendor’s offices. Richard, the Project Manager, describes the Team’s reaction to this suggestion:
They wanted us to go their office in [Fine City] and we said frankly that ‘If we tried to tell people that idea they would just [inaudible]... We suggested that that wasn’t appropriate [he laughs].
(Author interview with Richard, Project Manager)
The story did not finish there. There were some further negotiations about how and with what technology the demonstration would take place before eventually it went ahead in Melchester’s own offices. As the Project Manager describes, this too did not end in the way BigVendor would have liked: ‘It was over a phone line, and it worked perfectly well over a phone line, [but] unfortunately they were showing us the wrong thing!’ We never actually learnt just which system the Team had been shown but whatever it was, or whatever had gone wrong, it was sufficiently embarrassing for BigVendor to withdraw immediately from the procurement.
‘That is better...’
Given these difficulties, much importance was now placed on actually seeing a system in operation. Thus, a few weeks later, when the Team were offered the opportunity to go and view the SmallVendor system that had recently been installed within Bingham Council they enthusiastically agreed to the
trip. While in previous visits only one or two people had actually made the journey, on this occasion the entire Team was travelling the 200 miles to visit Bingham Council. If the selection of a system depended so crucially on its capabilities being observed, then any witnessing would have to be very much a ‘collective endeavour’.11
Upon arrival at Bingham, there was a round-table session where Bingham staff talked about their experiences working with the supplier. This was followed by a demonstration of the software and time spent watching operators in a live situation within a call centre. Everyone broke into pairs and sat with an operator, watching them take calls and enter and retrieve data. This demo and visit appeared to go extremely well. On the train back to Melchester, the Team chatted excitedly about what they had seen. Some weeks later, in an interview, Christine from Customer Services still spoke highly of the visit:
We also visited [Bingham] and saw their solution actually working, and it was very simple, the agents in the call centre were all delighted with it and it had made their lives a lot easier and it was really fast and responsive.
(Author interview with Christine, Customer Services)
What was obvious on the train journey back and then in subsequent meetings was that the attachments various groups had made to particular solutions, and the case for prioritising these, were now sufficiently loosened such that all other solutions could be sifted out. Of course, there was still the awkward job of notifying JV Partner, but it was now clear that a decision had finally been taken and the procurement was all but over.12 This raises a final question in relation to the demonstration: why had it taken on such importance, which was surely incommensurate to the amount of information it provided?
As we have said, the comparative measures operated as a kind of scaffolding which was being erected around the systems in order to understand their shape and boundaries (i.e. to whom they were connected and on what they depended). What had been unclear at the outset but was now evident through the performance of the various comparative measures was that MiddleVendor did not yet have a finished version of what they were offering Melchester. Moreover, though they were able to provide reference sites, the Team were unwilling to imagine the software seen there (in rather different settings) working in their own setting. Melchester were unable to disentangle the technology from these sites; unable to envisage the whole solution, they recast MiddleVendor’s proposal as containing only ‘bits and bobs’ of software. Similarly, while MiddleVendor had offered a ‘Proof of Concept’ in lieu of an actual demonstration of a finished system, this was not seen to be sufficient proof that they were capable of turning these bits and bobs into a package that might suit Melchester. As ‘guinea-pigs’ Melchester would be entirely reliant on MiddleVendor’s word.
Alternatively, BigVendor did have many local government reference sites in the UK (and parallels between Melchester and these could easily be drawn) but, surprisingly, was unable to demonstrate its software (or at least the correct software). As Shapin and Schaffer (1989) suggest, there is much to be gained but also much to be lost during demonstrations. Similarly, writing about public experiments in the scientific domain, Collins (1988: 728) suggests that demonstrations ‘have the power to convince because of the smoothness of performance, distancing the audience from the untidy craft of the scientist’. In this sense, demonstrations are rarely spontaneous events: adopters wish to observe the technology in everyday use, and will seemingly give much importance to these events, even though they know that the demonstration has been ‘staged’ specifically for their benefit. The demonstration is both an evaluation of the systems and the supplier. Thus when things went wrong, the question the Team might have asked themselves was: if the supplier cannot stage that which is expected to be staged in the safe internal environment of their customer’s site, how might they deal with systems out there in the real world?
This left SmallVendor, which was able to offer only the one site but one seemingly similar enough to provide sufficient parallels. What was specifically interesting about the demo at SmallVendor was not simply that the Team could see a software package in action but that this was also one of the few times the Team were able to establish an uncontested comparison between the different systems. We say ‘uncontested’ because every other measure up until then (price, risk, reputation, availability of a package, etc.) was the basis of further disagreement and uncertainty. In the case of the demonstration, however, things appeared much less equivocal. The Team had successfully translated the assessment process into a specific requirement - indeed, necessity - that the software is demonstrated and this demonstration is witnessed (see Figure 6.1).
In other words, to return to our earlier discussion of the performativity of procurement, they had finally established a place where the differences between systems and suppliers could be actualised. The demonstration became the means by which the systems were finally put on a common plane; the procurement was no longer simply an intellectual exercise, but the entire Team could witness directly with their own eyes a ‘visual’ comparison between the systems. Particular, and perhaps disproportionate, weight was given to this most direct form of evidence of system performance; even though it did not per se resolve uncertainties about system properties, it seemed to have particular impact in aligning opinion.
Conclusions: comparative measures can be performed
From the point of view of Economics, Management and Engineering accounts, the procurement of technologies is seen to be the result of a formal process in which information about the properties of objects is assessed against a narrow set of pre-specified decision criteria. By contrast, critical
Figure 6.1 The demonstration: the demonstration has become one of the key events in the selection of a software package. Here we witness others witnessing the demonstration of a leading ERP system at a tradeshow in Orlando.
Source: Photograph by authors.
interpretations, informed by constructivist and cultural Sociology, reject this view, portraying technology selection as the outcome of more informal social processes in which the micro-politics of the organisation overshadow the substance of the selection procedure. In particular, many follow a ‘Woolgarian’-type view in which the technical properties of the different systems and the decision criteria, if not entirely removed from the equation altogether, are seen as ‘indeterminate’ to that decision. While constructivist tools are well honed for unpicking the political moves underlying technical discourse and seek to demonstrate how micro-politics are in command and decisions are divorced from formal assessment, we argue that technology selection cannot be fully understood unless we more fully consider the role of assessment criteria.
In the case presented here, we have shown that the procurement decision was not a purely political device in the ways that radical constructivism might suggest (even though it had an important ritual and dramatic element). The decision occurred in a context characterised by high levels of uncertainty, where the properties of the different offerings and the decision criteria were negotiable and indeed openly and covertly contested. Added to
this, the relationship between the council and its joint venture partner was teetering on the brink of failure. However, utilising Callon’s concept of the performativity of economic concepts and tools, we have shown how the Procurement Team sought to ‘frame’ the procurement, and in doing so to push away arguments that came from outside the boundaries of the choice, to edge around controversies, and to drag the procurement from the informal domain on to a more formal, accountable plane. In other words, they attempted to draw a boundary around those things they considered central to the decision and those that belonged elsewhere. These kinds of overflows included the ambiguity and uncertainty that was introduced late on by JV Partner when they attempted to include ‘risk’ as a feature of the choice.
Their efforts involved the laborious construction of a ‘like for like’ comparison: that is, attempts to draw out and compare those properties that were deemed to be significant, and to lay them on a common plane. To this effect, we saw how the Team wove into the process a number of ‘comparative measures’ or ‘comparative scaffolding’. These included elements set prior to and outside of the array of actors involved in the procurement, along with others put in place in the process of reaching a decision. First, there were attempts to collect and interpret testimonies from reference sites, - though the evidence obtained was often uniformly positive and thus provided little opportunity to differentiate suppliers. Second, much time went into establishing the provenance and status of the software packages and there were discussions of what kinds of objects with what kind of biographies and careers the council were willing to accept (a package built from scratch, one constructed for other industries, or a partially formed local government package). Third, where the Team were unable (or simply unwilling) to ‘imagine’ a finished system from the ‘bits and bobs’ they were shown, or to make the necessary parallels between their setting and the reference sites, there was a requirement for suppliers to show that they could also make their systems work within the council. In other words, they were being asked to provide evidence of technical competence. Fourth, there were attempts to assess the standing of suppliers, as a measure of their current and future performance, by asking external experts to comment on and investigate that standing. Finally, it became increasingly important for the suppliers to demonstrate their systems and that the Team could bear direct witness to these demonstrations.
We describe these comparative measures as stabilised forms of accountability; albeit a loosely coupled form of accountability that left considerable discretion for actors (they are a resource rather than a constraint). We suggested that these operate as a kind of ‘scaffolding’ erected during the move towards the procurement decision. These measures - this scaffolding - as they are put into place gradually give a shape to assessments of the various systems (i.e. their boundaries are mapped out, and it is shown what they depend on and who they are connected to). Our argument is that it was not so much the properties of the systems that were important for establishing
differences and similarities between the various software packages, but the enactment of these various assessment criteria within the procurement team that give a form to those properties.
When there were difficulties and uncertainties these have to be interpreted not as uncertainties about directly ascertained properties but, rather, as uncertainties surrounding the measures that calculate properties. Each of the different comparative measures had different explanatory power (and thus there was a constant need to move between measures in order to compare the artefacts). We saw a shift from the notion of procurement as an exercise in imagination or even enchantment (i.e. where the Team were asked to ‘imagine’ how a system in a reference site would work within their own setting) to an exercise in trust based upon testimony (where the team were asked to assess testimonies and expert judgements), and finally to a more concrete ‘visual’ exercise (where the Team could witness demonstrations). It was by shifting between measures that the technologies were eventually ranked and sifted, that they could be disentangled from the reference sites and the supplier, and that their differences could be shown.
What was also interesting about our case is that most members of the Team appeared to have a hand in the decision, to be relatively satisfied with the process, to think that not all but some of their views had been included, and to agree that the outcome (although it meant giving up their initial preference) was the correct one for the council. This latter aspect (the way the Team members shifted vendor preferences) is the most interesting. Callon and Muniesa (2005) have discussed how measures may be imposed by specialist groups. In our case the Team ended up evaluating the packages (not through the measures that were initially touted - such as fit, price, the packages’ potential for updating processes, etc.) but through sets of measures that were proposed to them - maybe in some cases imposed on them - by their (more technical) colleagues. However, perhaps the notion ‘impose’ is too strong. No one member of the Team was able to frame the process completely: new sites of tension were continuously opening up; measures were subject to continuous ‘overflowing’. This was most evident in instances where assessment criteria were recast: these included MiddleVendor attempting to sidestep their lack of a demonstrator by offering a ‘Proof of Concept’; MiddleVendor’s benign proposal that Melchester be a ‘pilot site’, which was subsequently reinterpreted by hostile local actors as meaning that they were actually being used as a ‘guinea-pig’; and JV Partner’s ultimately unsuccessful attempt to introduce the notion of ‘risk’ as a further assessment criterion. The scaffolding metaphor addresses, on the one hand, the continued spaces for negotiation and discretion about what new criteria and methods should be introduced and how they should be applied - and as we saw, these reconfigurations could give rise to ‘surprising’ outcomes.13 On the other hand, the outcomes were not wholly open but were structured, subject to various kinds of local and broader accountability. For instance, we saw how the assessment process became increasingly constrained as different
planks of the scaffold were put into place and the parties moved towards a decision. Procurement, of course, takes place in a range of contexts, with more or less well-established evaluation criteria and subject to different levels and types of accountability. In addition, as we saw in this case, accountability may change. The actors’ awareness of formal requirements such as the fair trading legislation changed the parameters of this discretion, enacting a tighter form of accountability. In other words, the transparency that was required within the OJEC had a determinate effect on the conduct of decisions.
To conclude, technology choice and purchase should not be reduced to one single dimension (either the outcome of rational decision-making or the result of discursive struggles). Rather, it is the tension between these two positions that is interesting and should be explored. What our story highlights is that, though laborious, comparison is possible - it is performed through this cycle of disentangling, framing and overflowing. In this respect, we need to understand in greater detail the ‘grey space’ that exists between rationalistic accounts of technology and cultural sociological accounts. Theorising these grey spaces as well as the actors who inhabit and are able to speak in them is crucial for analysing technical change. In terms of the former, there is a need to accept that the comparative measures identified here constitute a form of assessment (they are not simply a ‘rhetorical ploy’). In terms of the latter, we point to the emergence of new kinds of experts, like industry analysts, who accept and work with this more amorphous kind of knowledge (see particularly Herschel and Collins 2005) and argue that these experts and their organisations need to be studied further. In other words, the challenge for researchers addressing technology choice, within Information Systems research or Technology Studies, is to develop tools honed for understanding the space established between techno-economic accounts and more cultural sociological approaches. It is to this issue we now turn.
7	Industry analysts and the labour of comparison
Equation
Message from SoftCo to customer
I am attaching the most recent Gartner Group Magic Quadrant and also last year’s. As you can see, our rating has improved quite dramatically. Over recent years, our position on the ‘execution’ axis has slowly improved, but 2005-6 has been a Great Leap Forward. As you can see, this is because we have continued to ‘execute’ but also have done a much better job of communicating our company’s ‘vision’ (the other axis of the Magic Quadrant.)
An executive from the large global software supplier SoftCo emails one of his most loyal and influential customers to report to him the publication of some recent industry research. The message is brief and written in an informal style and it is clear the author is happy to convey the information. The email describes the release of a new ‘Magic Quadrant’, which is a form of market analysis produced by one of the largest information and communication technology research analysts. It is an attempt to compare and rank software vendors and their offerings according to a number of predefined measures. It comes in the form of a box with an X and Y axis (labelled as ‘completeness of vision’ and ‘ability to execute’). Inside the box, there are a further four squares into which are placed the names of several vendors. These vendors are not randomly placed - each of the squares is individually labelled (niche player, challenger, visionary and leader) to show that the position of a vendor in a particular square signifies something (i.e. that a vendor is perceived to be performing well or not). The reason for the email message is that this particular software vendor has jumped from one box to another. Whereas in the previous year it was described as a ‘niche player’, it is now said to be a ‘challenger’. In addition, as the email message describes, this is thought to be good news, very good news indeed.
An alternative view on simple market tools?
This chapter builds on the last through considering the issue of ‘comparison’. As we have seen, comparing enterprise solutions so that a procurement choice can be made turns out to be rather more complex than anticipated. Indeed, the issue of how similarities and differences between ‘commodities’ more generally are established has attracted attention from scholars in Science and Technology Studies (STS), Economic Sociology and elsewhere. Some have gone as far as to conceptualise the economy in a way which puts the work of comparison as central to its organisation (Callon et al. 2002). Within this view, the experts who measure and classify the properties of technologies and products are increasingly central in what Michel Callon and colleagues have termed an ‘economy of qualities’ (ibid.). Lacking the devices to establish equivalences, then, how can different objects be brought together and compared in the same space? Without the practices to produce differences, how can similar, if not identical, commodities be ‘sorted out’? This work - what might be described as the labour of comparison - is, of course, not without its politics (Porter 1995) or complexities (Mol 2002a; Barry 2006). This is particularly around the status and detachment of the experts who perform the work of comparison, the devices they use in their craft, the identification and ‘revealing’ of the properties to be measured, and the calculations and actions that stem from these practices (Callon and Muniesa 2005).
The labour of comparison and its related politics and complexities should be of potential interest to those in the Social Study of Information Systems. How else, if not through comparison, are organisations to choose between highly complex artefacts like packaged enterprise systems? How are the differences and similarities between solutions found or, more importantly for suppliers, ‘created’? What kind of work and devices are required to produce comparisons between system suppliers? What controversies lead from these labours? And how are they dealt with? We attempt partial answers to these questions through investigating the expertise and tools of a body of professionals whose work is centrally concerned with these issues - the research of ‘industry analysts’.
Many would argue that the industry analysts who inhabit and comment on the IT marketplace offer a relatively ‘low-status’ form of knowledge. This is unlike similar groups of professionals who analyse and make predictions in other areas and sectors (such as financial markets, say) where their opinions are seen to be of an altogether different order (Preda 2007). Yet, despite this, few can deny the influence the former wield (which equals, if not at times surpassing, that of other groups). It is our argument that industry analysts are highly influential in that they seek to construct boundaries around the technology field through generating assessments of the relative position of software suppliers within product markets. In this chapter, we discuss one particular set of industry analysts and their attempt to analyse and position
the vendors supplying packaged enterprise systems. This is the Gartner Group, which produces various forms of prediction and assessment, and there is perhaps none as influential or contested than the ‘Magic Quadrant’ (MQ).1
In the words of its authors, MQs are ‘graphical portrayals of vendor performance in a market segment’ which ‘summarizes a given market and its significant vendors at a point in time’. The device turns out to be highly interesting, but also potentially difficult to study. One of the reasons - and we have already alluded to this - is that it is a ‘dividing object’: it enjoys an extensive diffusion and influence, being widely acknowledged as ‘one of the most referenced research tools in the IT sector’, but at the same time it is also seen as ‘highly simplistic’ and in certain respects ‘flawed’.2 Intriguingly, these contrasting views are not the opinions of different communities but often the same group. The people who appear to use it are also seemingly among its biggest critics. How are we to make sense of this form of market analysis which is seen as problematic but still widely used, which is controversial but also said to be effective in comparing the properties of vendors?
There are three possible ways of analysing the MQ, only one of which we think provides insight into the labour of comparison. A first strategy, perhaps the one favoured by critical social scientists, would be to debunk the tool. It is, after all, a version of the classic two-by-two matrix much beloved of European and American business schools. In this sense it would be relatively easy to reveal its limitations and imperfections, of which there are doubtlessly many (not least of all that it ‘flattens the world’ through hiding its complexity). However, we do not think this wholly productive. A second strategy, more analytical than the first, might be to treat the MQ as a ‘convention’. This would be to explain its success through the fact that it enjoys widespread diffusion. Indeed, social scientists have used these arguments to good effect in the domain of Science and Technology Policy, for instance, where Arie Rip among others (Rip 2006; Borup et al. 2006) has described the extension of ‘practitioner-based concepts’ according to these terms. However, while we agree that the MQ is a ‘convention’, it not correct to say that all conventions are completely arbitrary (which is the reading one finds in Rip’s article). An alternative strategy, the one favoured here, might be to attempt to open up this particular ‘black box’ to study the actual production of this tool and its content. This is to investigate how vendor positions and rankings are ‘calculated’ as well as the fate of the MQ once it leaves its place of development. In doing this, we can study both what the tool creates as well as what it hides and simplifies.3
More specifically, this chapter is organised around the following question: how do vendor rankings emerge from this contested socio-technical arrangement? To attempt an answer we set in train a particular line of inquiry. We want to show how the MQ is ‘performative’. That is, it does not describe a state of affairs that already exists, nor does it simply offer a new means of representing and positioning vendors; rather, it is a device that
interacts with, and modifies its object of study. Our principal contention is that through this interaction the MQ is making itself a more ‘successful’ description of the technology field.4 The chapter is organised as follows: before introducing our empirical material we discuss briefly the emergence of industry analysts as a body of experts; we then focus on recent debates with Economic Sociology and STS on the ‘performativity’ of theories and models.5
The growing influence of industry analysts
Industry analysts provide organisational consumers with research on the nature of the IT market. Some (like the ones discussed in this book) have an international reputation and a large audience for their work. There are various reasons as to why these kinds of experts have experienced a growing influence, and we review two of the principal arguments here.
Incommensurable artefacts
As has already been suggested, the IT sector is widely acknowledged to be among the most complex of terrains for organisational consumers attempting to acquire new information systems. It is typified by accelerated rates of technical change involving the constant development and proliferation of new solutions on to the market. These are rarely ‘similar’ systems insofar as vendors continuously attempt to differentiate their technologies from those of their rivals, new systems from previous versions, niche specific offerings from generic ones, and so on.6 Added to this the characteristics of information systems are ‘informational products’ (Williamson 1985a), and as a result their characteristics are not readily ascertainable. User organisations thus typically face difficulties choosing between technologies during procurement phases. Uncertainties are compounded by the fact many user organisations lack knowledge about the current state of the market (most buy in systems only infrequently - see Webster and Williams [1993]). And whereas in the past ‘personal networks’ would once have offered advice, these avenues no longer seem to match up to the challenge of assessing today’s systems (in terms of comparing the full range of offerings available and also of the ‘robustness’ of assessments). We mentioned earlier the importance of sectoral and professional networks for sharing information about vendor offerings but these too are seen to be increasingly limited, for similar reasons, in the face of the growing range of available products and their rapid evolution (Fincham et al. 1994; Swan and Newell 1995). It seems the lack of reliable knowledge about the capacity and behaviour of vendors and the efficacy of their products in particular contexts has forced buyers to resort to more systematic and impersonal ‘reputational’ indices of vendor behaviour (Gluckler and Armbruster 2003). Industry analysts have been quick to seize upon the opportunities here and
have produced numerous decision-making tools. And, as we have said, there appears to be a large audience for their work. However, this kind of explanation itself does not provide much insight into how this form of knowledge - one that is also surrounded by much uncertainty and scepticism - has become influential.
How does low-status knowledge spread?
Scholars have noted similar characteristics in the work of IT consultants and management consultants more generally (Alvesson 1993, 2001; Jones 2003). Gluckler and Armbruster, for instance, describe it as something of a paradox that there is much uncertainty surrounding the work of management consultants but also a consistently high demand for their services. To explain these seemingly contradictory impulses they argue that there must be some ‘other mechanisms’ which work to ‘bridge these uncertainties’ (Gluckler and Armbruster 2003: 290). One of the mechanisms they focus on is the means by which people evaluate such knowledge - and they put forward a spectrum containing three possibilities. They identify the existence of experience-based trust which they say might be used by users of consultancy to judge its value. They also note, however, that this form of experience is not widely spread (since it takes years to acquire). Thus, they point to the other end of the spectrum to what they call public reputation. However, reputation as a measure is not that useful, they say, since it is of mixed reliability and not very responsive to changes or current events. They thus point to an intermediate form of bridging which they describe as networked reputation. This is where assessments of consultants are drawn from interactions within informal social institutions and networks. This seems a useful conception and one that we shall build on throughout the rest of this discussion.
There are other (more actor-centred) views which also might help us to consider the spread of this kind of expertise. Turner (2001), for instance, distinguishes between those experts for whom there exists a predefined audience for their knowledge and those who actively have to create a following. Among the former he identifies physicists, who he describes as ‘possessing’ authority in the sense that their views are widely accepted. This is because their expertise is subject to institutionalised verification and can be ‘validated’. In the latter case he identifies those who ‘provide services’, and here the authority of these experts is accepted merely by some and only then because they have actively established and demonstrated their competence to this audience. To exemplify these kinds of experts, Turner cites the example of authors who write self-help books and how they are seen as authoritative specifically (and only) by those who find their work of value (i.e. the book does something for them).
Preda (2007) offers one of the few studies that deals specifically with the birth and rise of these new kinds of professionals. His discussion, which is of early financial analysis and the birth of the Chartist movement, focuses on
how the Chartists persuaded stock market traders that the new kinds of ‘forecasts’ about the price of stocks they were producing would be a useful addition to current working practices.
This work is interesting as it focuses on how these experts, in the face of questions about the benefits and provenance of this kind of information, slowly began to establish their ‘credibility’. Jones (2003) also discusses a similar example through focusing on the work of IT consultants and how they do not unproblematically ‘possess’ expertise but have to (a la Turner) continually validate their knowledge with clients. This they do, he argues, through routinely demonstrating their competence and knowledge of specific areas. Jones suggests, as does Preda, that these experts are ultimately successful because they actively shape their users’ perceptions of what kinds of knowledge and help are needed. Indeed, much of the research on consultancy portrays these actors as actively ‘selling’ solutions/knowledge to organisations (Hislop 2002). In other words, they configure users to appreciate, accept and incorporate this new knowledge into their activities (see also Bloomfield and Danieli [1995] who deploy a similar argument).
These studies all have their merits but our case is perhaps more complicated and leads us to a somewhat different conclusion. The clients of industry analysts we observed were not simply ‘trusting’ of this kind of research (though they did appear to hold many of the individual analysts in high regard). Nor were they simply ‘configured’ to appreciate and accept assessments. If anything, they were sophisticated and wary consumers of this kind of knowledge. They often joked, for instance, about the possibility that the MQ might be ‘flawed’. Yet despite this, people treated the tool as ‘real’ even though they knew it to be a simplified convention. Thus, what we found was a rather strange situation where the research was viewed with scepticism but used in practice. This suggests we need to look beyond notions of ‘trust’, ‘reputation’, ‘configuring’, etc., to understand the influence of industry analysts. To do this, we turn to review how these kinds of tools have been conceptualised within other parts of the critical social sciences. We identify at least three perspectives, where the objects are treated as ‘oversimplifications’, as ‘folk theories’ and as ‘performative’.
Theorising simple tools
Powerful oversimplification
Despite the fact that the MQ and similar objects have been a feature of business settings for over two decades now, they (strangely, in our view) still attract relatively little attention from scholars interested in the social analysis of technology. There is nowhere near an adequate social science language, for instance, to describe the success/failure of these devices. Maybe it is because some of them are intertwined with organisational life that they
appear unworthy of attention: they have become ‘successful’ such that the world now seemingly acts according to them (Barry and Slater 2005). The few studies that do discuss them only do so in order to demonstrate their flaws.7 We appear more interested in highlighting their ‘overflows’ as opposed to their ability to ‘frame’ (Callon 1998). Whatever the reason, it is clear that there are too few decent accounts of the genesis of these kinds of tools. There are exceptions, of course.
Within the sub-discipline of Business History, for instance, there is discussion of the genesis of the ‘Boston Matrix’, the portfolio analysis tool developed by the Boston Consulting Group (BCG). Interestingly, and according to Ghemawhat (2002), the Boston Matrix drew heavily on academic work and in particular the outputs of the Harvard Business School during the 1960s. According to Ghemawhat, academic work on ‘strategic thinking’ had quite an influence on Bruce Henderson, the founder of both BCG and the Boson Matrix, who purportedly designed the tool to reflect the ideas coming out of Harvard during that time. Decision-making within corporations was becoming so complex, as Henderson saw it, that he wanted managers to think ‘strategically’ rather than ‘functionally’. The only way for managers to sort and sift through possible choices in where to guide their firms was through the application of a simple tool which provided more ‘strategic guidance’ (ibid.: 45). As Ghemawhat describes, the tool offered a ‘frame of reference’ to help bring order during problem-solving. It was Henderson himself who described the Boston Matrix as an oversimplification, albeit a ‘powerful oversimplification’ (ibid.: 45).
Folk theory?
At first glance, STS appears well-equipped to understand the nature and influence of these devices. It has, after all, a long-standing interest in the models and simplifications produced by scientists and engineers and how these relate to the development of techno-science (see Morgan and Morrison 1999). Yet oddly enough, there are very few studies of the models used outside the lab and within, say, the Science and Technology Policy arena. Some exceptions are Rip (2006) and Borup et al. (2006) who have recently conducted work on another of Gartner’s tools, the ‘hype cycle’. The hype cycle was developed by Gartner to map the rise and fall of expectations surrounding new technologies and is widely used not only by IT practitioners but increasingly by engineers and technologists working in other domains. Rip (2006) characterises the hype cycle as a ‘folk theory’ and he works up this term to analyse more thoroughly what he calls ‘practitioner-based concepts’. Folk theories like the hype cycle, according to Rip, are often deployed with the explicit aim of mobilising support and funnelling the direction of future action. They evolve out of actual practice rather than academia and their veracity comes not from their accuracy per se but from the fact they are widely taken up:
They are a form of expectations, based in some experience, but not necessarily systematically checked. Their robustness derives from their being generally accepted, and thus part of a repertoire current in a group or in our culture more generally.
(ibid.: 349)
Folk theory is a useful conception, but we see two related problems with the concept if applied more generally.8 Primarily, this is that it places emphasis on the acceptance of these tools as opposed to their conception and production. Thus, using this form of analysis there would be little interest in investigating the thinking that exists behind the tool or its actual content (lending weight to the suggestion that these devices could be more or less ‘arbitrary’). Indeed, when reading Rip’s work, one gets the impression he views the work of organisations like Gartner solely in these terms. For instance, he describes how the hype cycle ‘is not an object of systematic research’ (ibid.: 353) and that while folk theories are ‘forceful repertoires’ some are ‘arguably wrong’ (ibid.: 361). We disagree with Rip here and suggest that we need a more fine-tuned way to analyse such tools (and particularly the difference between tools). In terms of the MQ, for instance, while we agree it is a convention it is not completely injudicious. Rather, as we want to demonstrate, it is part of a wider movement to redefine decisionmaking within the IT sector. To use the language we describe more fully in the next section, it is attempting to set a ‘new world’ in motion.
The second problematic aspect concerns another reading that can be made of Rip’s article, which is that, even though a folk theory might initially bear little relation to the current pathway a technology is taking, it is because people come to believe in it that the tool begins to have some effect (and actually influences the object’s evolution in some way). The problem we have with this conception is that everything appears to take place in the ‘heads of actors’ - and this form of explanation does not help in our case. Indeed, we would suggest the MQ pushes or nudges the IT market in certain directions without necessarily being believed. As we have said, many people regard the MQ to be flawed but, crucially, this turns out to be relatively unimportant because in practice they still use it. This suggests a need for a mode of enquiry based not within the human realm (what people ‘believe’) but focusing on the actual doing of assessment, what is sometimes described as a move from a representational to performative idiom (Pickering 1995).
Performativity
We base our analysis of the Magic Quadrant on the recent and provocative work in Economic Sociology and the Sociology of Financial Markets, which we also sought to apply in the previous discussion of procurement. This was Michel Callon’s and Donald MacKenzie’s thesis on performativity, where they have argued that economic and financial theories do not simply describe
markets but also do things within this domain (Callon 1998; MacKenzie 2006a, 2006b). Having already introduced some of the principal ideas in Chapter 6, we will not go over the same ground but selectively highlight those parts relevant for our current discussion.
The actual notion of ‘performativity’ stems from the work of the linguistic philosopher J.L. Austin (1962) who wrote that a statement was performative when it did more than just describe a reality; instead, it was actively engaged in the constitution of that reality (cf. Barnes 1983). This prompts the question (and here we highlight the limitations of the notion of ‘folk theory’ and the possible benefits of the performative approach) as to whether any kind of assessment is possible. Could industry analysts make whatever judgement they choose? In Austin’s original discussion, he was careful to avoid discussing the veracity of performatives. What was important was not whether statements were true or false but how, in actually making statements, the speaker was ‘setting something in motion’ (Callon 2007: 320). Callon has built on this argument in two ways: through suggesting we replace the concept of truth and falsity with ‘success’ and ‘failure’; and then through setting out a partial framework to study whether performatives have ‘successfully’ (or not) brought about that which they previously set in motion.
This first point, especially for those familiar with the pragmatism of Actor Network Theory, is relatively straightforward enough, but the second is less so. What Callon intends is that performatives do not exist in isolation; they have meaning and effect only in the world they create for themselves. Callon describes theories and their world as a socio-technical agencement.9 He depicts the performativity of theories as the binding together of various elements (typically people, texts, objects, machines, theories and models), all of which constitute and make up the actor in a socio-technical agencement. Callon is clear that a theory is performative (successful) only when it can create this socio-technical agencement.10 One other aspect is the assertion that no one element (human or non-human) is assumed, a priori, to be more important than any other; they all, methodologically at least, have equal status, and in this sense they all can act. It is because of the implied symmetry here that Callon can argue that theories also set worlds in motion.
For those sympathetic to Actor Network Theory this argument is perhaps no longer controversial. What has upset Economic Sociologists in particular (see, for instance, the volume edited by Barry and Slater [2005]), however, is to what the argument has been applied. As mentioned previously, Callon (1998, 1999) has suggested that the ‘market’ and ‘homo economicus’ are no longer ideas that exist simply in economic textbooks but are continuously enacted within the economy. If people trade and purchase goods in a ‘market’ (as opposed to any of the other ways the exchange of goods might occur) then this is because economic notions of the market have successfully constructed a socio-technical agencement. Within this agencement buyers and sellers are continuously constructed as ‘autonomous agencies’ and the objects to be sold as stable and ‘commodity-like’ (Holm 2007). Callon is
clear that the mechanisms that enable this are not part of human nature (actors in Callon’s view have a variable ontology) but are actively constructed, and that academic economics has played a role in this construction. Actors and objects are so thoroughly entangled in other (competing) socio-technical agencements that there have to be processes of ‘framing’ and ‘disentanglement’ if economic man is to exist. If such framing is successful, according to Callon, then it is the particular socio-technical agencements an actor belongs to that gives him or her the ability to act (as well as giving meaning to that action (Hardie and MacKenzie 2007). In other words, whether an actor is calculating or not (selfish or moral) depends on the socio-technical agencement by which that actor is constituted (ibid.).
We find this work highly instructive and take it as a point of departure for our own discussion. However, before turning to our empirical material we wish to make one further point and a qualification about the notion of per-formativity as it has been recently applied. First, Callon has argued that ‘theoretical knowledge’ can have effects in the market because of various actors attempting to create the world to which the knowledge points (i.e. ideas embedded in textbooks are extended out into the marketplace). We have little problem with this but simply note the distance that exists between the place where the research is created (the academy) and the setting which is reshaped (the market), as well as the possibility of translation. If ideas are to become reality then they must, as Latour (1987) tells us, pass through and be changed by many hands before finally constructing the thing they have described. This perhaps then raises questions about what exactly is being performed or who precisely is doing the performing - and suggests a rather ‘weak’ form of performativity (see below). Another way to study the phenomena may be to look at those areas where the distance is much shorter or, perhaps, to study ‘intermediaries’. Industry analysts are an example of actors who bridge different worlds. The distance between them and the settings they attempt to describe is small (and in some cases non-existent - as evidenced by the fact their views are often seized upon by IT practitioners). They are often described, and we have used the term ourselves throughout the book, as ‘intermediaries’ - i.e. groups who bring other worlds into relation (cf. Henion 1989). Thus it seems to us more likely that the research of certain kinds of actors is prone to have effects on the thing it is describing (this is also an argument found in some of the chapters in the edited collections by MacKenzie et al. [2007], Callon et al. [2007] and Barry and Slater [2005]).
Second, in terms of the qualification, we are not suggesting that all Gartner research is successful. Indeed, during our research we found their assessments and predictions to have a variety of effects (ranging from strong to weak). We find MacKenzie (2006a: 16-18) useful here as he has characterised how theories can have a range of effects. There are those that when applied have no observable effect (what he calls ‘generic performativity’). Others, once applied, ‘make a difference’ in some small way (‘effective per-formativity’). Then there are those which bring about the ‘state of affairs’ for
which they are a good ‘empirical description’ (which he describes, after the sociologist of science Barry Barnes, as ‘Barnesian performativity’). Finally, there are those that change economic processes so they conform less well to their depiction by theories (what he calls ‘counterperformativity’).
To summarise, what MacKenzie has done here is to acknowledge that not everything is performative. Alternatively, if it is performative, that this may be so weak so as not to warrant attention. In this respect, the performative approach appears to offer greater precision than a more generalised constructivist analysis (see Mol [2002b] on this point). It opens up the possibility, for instance, that rather than simply dismissing the work of industry analysts, which is the temptation when viewed through a purely constructivist lens (cf. Rip, 2006), we can show how such tools have a range of effects. This is exactly the kind of precision needed. Indeed, elsewhere we have attempted to construct a typology of the different types of research that Gartner produces (as well as its effects). So far, we have characterised three types. We describe them here since it might be instructive in terms of our clarifying our analysis (Pollock and Williams 2007).
There is what might be thought of as infrastructural knowledge. These are typically but not exclusively classifications buried deep in the analysts’ organisation. These, we would argue, exhibit a strong and ‘enduring’ influence on a particular technological market. They endure because they are mostly invisible (in the way that Bowker and Star [1999] describe ‘infrastructure’ - i.e. as something that becomes visible only upon breakdown). Indeed, we might recharacterise the discussion in Chapter 6 as just such an episode.11 There is also what might be thought of as visions let loose. These are statements or predictions about the market that are simply ‘launched into the ether’, so to speak. The result is that these assessments have some influence but this is typically short-lived.12 This takes us to the final type of knowledge, statements and their world. This is research - and the Magic Quadrant is a clear example of this, we would argue - where industry analysts actively attempt to bring their assessments of the market into being. As such, because of the work they do, these statements have a relatively strong but - and this is important - contested influence on the market. In short, our argument is that, while there are all sorts of actors attempting to colonise the market, only a small number of these produce research that is seen to be accurate - only some of them end up being performative. Organisations like Gartner can do this form of prediction work only, as we will show, because they do their ‘homework’ and build their accountability.13
Our chapter is organised as follows. We first make the case that the tool is not simply describing but transforming some aspects of the nature of decision-making. It is attempting to shift thinking within information system procurement from issues to do with local commensurability (the specific ‘fit’ of systems) to more ‘strategic’ and ‘ordinal’ factors. We conceptualise this as the ‘world’ the tool is attempting to put in motion. We then examine the research process that lies behind the tool and how analysts construct various
‘calculative networks’ and attempt to objectify what might be thought of as ‘community knowledge’. Finally, we discuss what happens when the tools are sent out into the world, how they interact with various settings and how they help construct ‘reflexive’ actors.
The Magic Quadrant
The MQ is authored by the Gartner Group, one of the largest and most influential industry analysts, and widely acknowledged leaders in the field (Firth and Swanson 2002; Burks 2006). Founded by Gideon Gartner in 1979, the Gartner Group has its headquarters in Stamford, Connecticut, and offices in over eighty places around the world. It has 4,300 associates, of whom 1,400 are described as ‘expert analysts’ and ‘consultants’. Gartner provides its clients with a range of services which include access to research reports and analysis, briefings on specific vendors, and in some cases ‘mentoring’. They produce two streams of research: the first, more future-orientated like the ‘hype cycle’, attempts to predict emerging technologies and trends within the ICT sector; the second, more concerned with the present, characterises and assesses the position of vendors and offerings. While these two streams interrelate, it is the latter type that is mostly of concern here.
The history of the Magic Quadrant
The genesis of the MQ appears to be shrouded in some mystery. Articles in the practitioner-focused press have attempted to discuss its development but always reach the same conclusion - ‘no one is really sure’ (Whitehorn 2007). From our own discussions with Gartner we know that it first appeared around the mid-1980s but, interestingly (and something that helps sustain the mythology surrounding this device) our informant was also uncertain about how it was first developed. She identified the tool as stemming from the work of two particular analysts but was unclear as to when exactly it was first used (and even suspected it to have begun its life with a different name):
We believe the first presentation use of the quadrant (though it wasn’t called that at the time) was in 1986 at Gartner’s Scenario conference... We looked through our Scenario conference binders from 1985 to 1987 -did not find any MQs in the 1985 binder, one in 1986 and 1987. The analysts who used it at that conference were Mike Braude and Peter Levine in their Software Management Strategies Scenario - again, though, it wasn’t formally called an MQ. Given our rigid discipline back in the 1980s of limiting Research Notes to two pages, we suspect that the MQ appearance in presentations most likely predates their appearance in a Research Note, but are uncertain. Nor can we be certain that it wasn’t used at another ‘theme’ conference earlier in 1986.
(Correspondence between Gartner and authors)
Despite continuing to ask, we were unable to uncover the MQ’s original name (all Gartner’s early research is housed in a storage facility to which our informant did not have easy access; its inventors have long since left Gartner for new positions, and one has since retired) so can throw no further light on the issue. However, we were fortunate in being able to observe one senior Gartner analyst discuss early thinking on decision-making within the information systems domain (and specifically how they were attempting to change the nature of technological assessment).
A transformative tool?
We initially approached our study of the MQ using fairly conventional forms of analysis. We too had initially conceived of the tool as a ‘convention’ that was mostly ‘arbitrary’ and had become successful through widespread diffusion, and considered that this take-up was helped by Gartner’s reputation. Thus, we were genuinely surprised to find ourselves sitting listening to a talk which pointed to a rather different story. To give some indication of this we present a lengthy extract from a presentation given by a Gartner analyst to a large audience of IT professionals and practitioners working primarily in the higher education sector. Typically, he delivers one of the keynote speeches each year at this particular conference, and the theme he has decided to reflect on this time around is the history of decision-making with information systems procurement. The analyst begins by discussing what was, more than a decade ago, the means by which people assessed systems prior to purchase:
we put together [in the 1990s] an outline of how you should evaluate administrative applications...And we looked at functionality, costs, service, support, technology, vision of the company and ability to execute. And what we said was that in a stable environment you would look at ‘functionality’.That was pretty much what we were looking at. Why? Well a mainframe is a mainframe so technology wasn’t that different from one to another, it was basically a vendor’s box that you were buying but it was built around a common architecture. When you looked in ter ms of cost, that was the driving factor for us; And service and support. We really didn’t think much about vision of the company or their ability to execute, we just bought what they had to offer.So, we had some need but it was kind of focusing on functionality and cost. What we said in ‘97 was change. You need to look at functionality but most vendor packages are mature enough to where there is at least common functionality, so it is a matter of goodness of fit that you are looking at. And we started seeing that trend in the early 80s.that said we had ageing of systems, people were using these systems.whether they were proprietary or home-grown for 15, 20, 25 years.And, the point is that you had to look at buying software as being a partnership
with a vendor, and that’s a long-term relationship. It’s not something short-term. And so, the vision of the company - do they understand the business of Higher Ed? Do they know where you were going? - and the ability to execute, those are still crucial. We still say it is about half of what your criteria should be. Now, if I am a...Chief Financial Officer...I am probably going to look at functionality as being crucial. That’s fine. But somebody better look out for the good of the university as a whole. Because your institutional perspective is the one that we’re responsible to look out for in IT. [our emphasis]
There are at least three moves in this long extract. First, we see the pro-blematisation of the traditional means by which people assess information systems. His critique focuses on the assessment criteria people currently use (‘functionality’, ‘cost’, ‘service’, etc.) which he suggests are no longer effective in sorting vendors out. How can you select between vendors using criteria of ‘technology’ when systems are no longer significantly ‘different from one another’? How effective is ‘functionality’ when vendors increasingly offer ‘common functionality’? The analyst also thinks it has now become necessary to replace current assessment measures, as user organisations tend to use the same solution for longer and thus, nowadays, have ‘partnerships’ with suppliers, the implications being that organisational consumers need to assess not only systems but also increasingly vendors themselves. In other words, he is suggesting a shift in decision-making from the evaluation of functional and local concerns to more ‘strategic’ ones. In addition, in order to do this he mentions how a consumer might apply Gartner’s own evaluation criteria from the MQ, which they term as ‘ability to execute’ and ‘completeness of vision’, when evaluating vendors.
A second move is that Gartner are proposing to reframe decision-making through bringing into being new kinds of actors. We do not think we are overstating the point by talking about the MQ in this way, to think of Gartner attempting to produce a way for vendors ‘to be’. A vendor’s ‘ability to execute’ or their ‘completeness of vision’ did not exist prior to Gartner’s intervention; they are ways of seeing vendors, we would argue, primarily established by Gartner. This is not to say that others have never conceived of vendors in strategic terms (of course they have). Our argument is that Gartner, having picked them up, are attempting to extend them, through reframing decision-making and ‘remaking’ vendors in this strategic guise. Moreover, they are successfully enacting the world to which the MQ points. Their intervention, as we will demonstrate, is making a difference to vendors, who increasingly think of themselves in these ways, and the users of Gartner research, where ‘ability to execute’ and ‘completeness of vision’ are increasingly seen as unproblematic, assessable, vendor properties.
The third move is that these strategic criteria prioritise comparative forms of assessment rather than local accuracy. That is, they give form to ‘ordinal’ characteristics as opposed to those that establish commensurability with
local sites. In this respect, Theodore Porter has argued that there are strong incentives in both the sciences and the economy for precise and standardisable measures rather than highly ‘accurate’ ones. He writes ‘[f]or most purposes, accuracy is meaningless if the same operations and measurements cannot be performed at other sites’ (1995: 29). In the earlier decision-making frame, suppliers were assessed on measures that were effective in detailing how a potential system related to the needs and shape of a specific user (i.e. they were ‘accurate’) but provided little purchase on how vendors compared in catering for such requirements (i.e. they were not ordinal measures). By contrast, the new frame renders vendors commensurable with each other, as was Gartner’s intent. This is described in the documents that accompany the MQ: ‘The evaluation results give the strengths of the vendors relative to each other.’ Thus we can say that the MQ generates comparisons that do not exist elsewhere: it brings vendors together in the same space (through disentangling them from existing worlds); and through producing new relationships between them (entangling them with the new world set in motion) (Callon and Muniesa 2005). We might therefore describe the MQ as a technology of comparison as opposed to one that delivers accuracy.
We are arguing that the MQ is transformative and that, in producing the tool, Gartner are also reconstituting the ‘technical field’ from one where people were concerned with local and functional issues to more strategic ones. However, the world that Gartner are attempting to set out also requires a research process - a method by which information about vendors can be collected. This turns out to be one of the most controversial aspects of the tool, and it is to this we now turn.
Shaping the Magic Quadrant
Gartner do not entirely calculate MQs within the boundaries of their own organisation. They are partially the product of interactions analysts have with the vendors themselves and a geographically dispersed network of vendor customers. In this section, we discuss these groups through conceptualising how the former respond to MQ and then with how the latter are organised into what might be thought of as ‘calculative networks’. We describe the information flowing within these networks as ‘community knowledge’ and discuss Gartner’s attempt at objectifying this knowledge.
The research process
Researching and observing industry analysts are very difficult indeed. Gartner are ‘cautious’ when talking about their work (and understandably so since their work has been the subject of much criticism). When one of our research team was able to ask a senior analyst about the construction of the tools, he would say very little about them except to emphasise that they were the result of a ‘long period of careful research’, that they were put
together over the period of ‘several months’, that they involved the work of different Gartner analysts, and that these analysts met regularly with vendors and their customers. This is all he would say. However, we were able to find more detail through reading their documentation. One document describes how:
During the research process, we may ask for new information and briefings from vendors. We often gather information from vendor-provided references, from industry contacts, from unnamed clients, from public sources...and from other Gartner analysts.
(Burton and Aston 2004: 4)
While conducting fieldwork we were able to focus on two of the groups mentioned here: we interviewed a number of vendors subject to Gartner’s assessment; and we talked with some of these so-called ‘unnamed clients’, as well as observing Gartner’s interactions with these people.
Vendors are on the move
We spoke to a number of vendors about their relationship with Gartner. SoleSys (a pseudonym) is a US-based software package vendor supplying information systems to a number of sectors. SoleSys have been consistently well-placed on the Magic Quadrant in the particular sector with which we were concerned. They are identified as a ‘leader’ and make every effort to publicise the fact. For example, after we contacted their Marketing Director to arrange an interview, initially about a different issue, he sent us a recently published MQ in which they had maintained their position. When we met him, we took the opportunity to ask him about just how they achieved this highly positive ranking. We broached the subject rather simply, asking whether they ‘marketed themselves’ to Gartner. He responded:
It takes a lot of work, actually [laughing]. And, you don’t really market yourself to Gartner as they are very focused on the communications they have with corporations. So what they do, if you want to be considered for coverage on the Magic Quadrant, they send out a questionnaire in advance of the Quadrant. And it ends up being like a 50-page response that is required from a vendor, from, you know, the high-level product strategy down to the feature and functionality and architecture. So we make an investment to respond to that as thoroughly as possible. And that’s how, where our placement in the Quadrant comes from.
(Author interview with Marketing Director, SoleSys)
While polite enough to laugh at our question he did, however, chastise us for the suggestion that they just simply ‘market themselves’ to Gartner. Interestingly, other vendors made similar points, often explicitly refuting claims that they did anything other than provide ‘real information’. This can
be seen in an email exchange between a different vendor (SoftCo, the vendor from previous chapters) and one of its closest customers:
We have spent quite a lot of time bringing [the Gartner analyst responsible for their sector] up to speed on what we have achieved in terms of development and successful projects. I don’t mean just ‘marketing’ to him - I mean real information on real achievements, which have not been visible to him.
(Email from SoftCo to customer)
These two extracts are instructive. Our reading of them is that if a vendor is to be well-positioned on the MQ then it is far from a simple marketing exercise. The first respondent from SoleSys is replying to a tacit derogatory definition of marketing as ‘selling’ something irrespective of its quality. Instead, he makes the point that responding to Gartner takes much internal ‘investment’ and ‘work’. He insists there needs to be real substance behind the claim (even though his description does look like fairly straightforward self-promotion and positioning). We thus imagine a dual process whereby a vendor has first to disentangle itself from the existing (functional) ways it currently conceives of itself and then to reframe these according to more strategic measures. This suggests that the subjects of Gartner’s research are ‘on the move’ so to speak: vendors are remaking themselves in terms of the new world Gartner is attempting to set out.
This resonated with Ian Hacking’s (1999) argument, where he makes the insightful observation that new classification schemes rarely simply stabilise settings. Instead, they encourage newly sorted actors to act in different ways (often either conforming to, or rebelling from, the classification). Presumably, and this is what the Marketing Director is suggesting, it is because SoleSys are able to move towards the new realities set out by Gartner that they achieve their current ranking. The case of SoftCo, however, is more interesting still: either they are unable to conform, or are ‘rebelling’, or it is simply that Gartner remain unconvinced by their efforts. Whatever the reason, SoftCo have consistently underachieved on the MQ in this sector (see below).
Community knowledge
The second group from which MQs are derived are ‘unnamed clients’. These are (as far as we can gather) people who are customers of these vendors and, in most cases but not always, subscribers to Gartner research. Gartner’s relationship with this group is particularly interesting. We observed how one particular analyst had built up and was managing a large network of people with whom he interacted on a regular basis. These people would continuously feed back ‘judgements’ to him on the particular vendors they were working with. Based on our fieldwork we observed how a vendor ranking is
enacted within these interactions - which constitute what might be thought of as a ‘calculative network’ (Callon and Muniesa 2005).
We describe this calculative network and how it works more fully below, but for now we simply sketch some of its features. It is ‘selective’ in that analysts keep themselves close to certain people and exclude others. It is ‘tactical’ in that people recognise the importance of these interactions and may use them to further goals. Moreover, finally, interactions in the network are often highly ‘informal’ - being typically based on telephone calls or quick chats, conferences, etc. We might term these users who continuously feed back information to the analysts as ‘satellites’ and Gartner, who, in turn, translate these judgements into positions on the MQ, as a ‘centre of calculation’ (Latour 1987). Further, we might characterise the information within these networks as ‘community knowledge’ to emphasise both its informal and its distributed status. When pressed, for instance, Gartner will often deny that it is in fact the firm acting but that they are merely representing, within the tool, knowledge originated by others elsewhere. There are parallels with science: both seek to make their knowledge claims ‘objective’; though scientists tend to validate their claims in terms of objective nature (Shapin 1994) whereas Gartner continuously point to the community of vendor customers from where the claims originate, and, as we will now describe, a number of research protocols and ‘qualitative rules’ that sit between this community knowledge and final assessments.
The objectification of community knowledge
What we are arguing is that Gartner is shaping the world so that ‘community knowledge’ is no longer a highly particular and local form of knowledge but one that can travel the world. This is to say that this informal knowledge can be commodified. However, these kinds of judgements are not easily objectified (as Porter [1995] argues, judgements do not fit straightforwardly into quantification). For instance, during fieldwork we noted how Gartner often struggled to account for the provenance of community knowledge and how there was a certain amount of ambiguity surrounding the methodological status of the tool. Let us look at the latter aspect before returning to the former. For instance, in its early life we found the more ‘quantitative’ aspects of the MQ were highlighted; and only some years later was it described as resulting from ‘qualitative research’. Today it is typically described as having a mix of both these aspects: ‘Gartner analysts use a combination of objective and subjective criteria to evaluate individual vendors’ (Soejarto and Karamouzis 2005: 5).
When Gartner say that the tool includes ‘subjective criteria’, we take it to mean it is shaped through analyst interactions with clients. Indeed, one might think that incorporating this kind of knowledge increases the tool’s credibility, for instance giving weight to the argument that Gartner are ‘close to the action’, so to speak.14 It is this community knowledge that Gartner
are attempting to objectify, to bring into the calculation these customer judgements (seen as important but having till now remained outside the frame). Yet this is also seen as one of the weaknesses of the tool (leading to accusations of ‘partiality’ and ‘bias’).
Partiality and bias
One issue appears to be the obfuscation that exists around these calculative networks and community knowledge. The fact that Gartner refuse to make the names of their sources public, for instance, is a cause of much concern. There is also little information on how specific customers are chosen, as well as with the weight given to their views. During fieldwork, for instance, we spoke to one IT manager who was critical of how, despite Gartner’s well-advertised claim that they consult widely when conducting their research, they had never solicited his views. He was until recently the IT director of a large US university and very active in the wider software community, having until recently served as president of a SoftCo user-group. We interviewed him initially about this presidency, but during the conversation the topic of Gartner came up. He described how he thought the particular Gartner analyst responsible for his sector had not been completely even-handed when assessing SoftCo’s solutions:
he has been very negative to [Campus, the SoftCo module discussed earlier]. He has never called. He has never visited our site. [SoftCo] wants me to be on a conference call with him, but I really don’t want that. He just knows everything; he never listens. There are just some people you know that, I took an immediate dislike to him and that is because of that arrogance. But he does know a lot and Gartner is important...He is not against [SoftCo] he just thinks that they are a bit player and they are not serious. That is what I gather.
(Author interview)
Despite the fact he is well-informed about SoftCo and is someone who might have been expected to be contacted, this IT manager is not part of Gartner’s calculative network. It seems that in the labour of comparison Gartner actively differentiate between customers when gathering information: that access to calculative networks is ‘unevenly distributed’ (Callon and Muniesa 2005). Indeed, the issue of ‘bias’ implied in the above account was an aspect voiced several times during our fieldwork. It was, for instance, the focus of an email exchange between one SoftCo solution manager and a customer:
Up to now I perceived their.chief analyst.being pretty vain - it is hard to turn his mind around just by facts. For the last Magic Quadrant we proved him being wrong in every single sentence of his comments to
his (bad) assessment of [SoftCo], but I believe this has made him more negative about [SoftCo] than before.
(Email from SoftCo to IT manager, Big Civic)
Others at SoftCo made similar points. One of the most striking features of the various criticisms we came across were their identification of ‘attachment’ and ‘authorship’. Gartner are a large, global organisation with many hundreds of analysts, but nonetheless our informants identified one particular analyst as the source of ‘negative’ assessments. We mention this because it contrasts with the strategies Gartner are employing in an attempt to ‘objectify’ their knowledge. While certain actors highlight the ‘particularised’ nature of expertise, Gartner are pushing in the opposite direction through attempting to demonstrate how MQs result not from individual but from ‘collective expertise’. On their recently established Ombudsman Blog, for instance, a ‘code of ethics’ has been published which explicitly refutes the claim that MQs embody bias and declares that, by contrast, they result from a ‘collegiate’-style research process:
Each piece of Gartner research is subject to a rigorous peer-review process by the worldwide analyst team. Sign-off approval by research management is required prior to publication. This process is designed to surface any inconsistencies in research methodology, data collection and conclusions, as well as to use fully Gartner’s collective expertise on any research topic.
(Gartner website)15
The objectification of community knowledge includes a process of ‘purification’ (Power 2003) where Gartner are attempting to detach specific contributors from tools through emphasising the formal research protocols and ‘qualitative rules’ that exist between individuals and final assessments. MQs result not from individual but from global expertise; assessments are not simply ‘discretionary’ but analysts are strongly committed to certain ‘academic’ principles. Notions like ‘peer review’, ‘research methodologies’, ‘data collection’, etc., are an increasingly common aspect of Gartner’s vocabulary. They have also published the specific criteria by which they measure vendors; the two components of the MQ (‘ability to execute’ and ‘completeness of vision’) break down to reveal an extremely detailed list by which a vendor is measured (and they can score between one and three points on each of the particular sub-measures). This is an effort to convince others that calculation is less about ‘personal discretion’ and more about the following of qualitative rules (Porter 1995).
Extending the Magic Quadrant outside Gartner’s offices
We have focused on the process by which Gartner gathers information for MQs. In this section, we consider how the tool begins to ‘interact’ with the very thing it is attempting to describe. We do so through discussing how
Gartner’s assessments are taken up by one particular vendor customer, and then how they become a ‘resource’ shaping his activities. We also return to this notion of ‘discretion’, where we show how Gartner, having handed the ‘calculative equipment’ to others, labour furiously to wrestle it back again.
The Magic Quadrant at Big Civic
Phil is an IT manager at the university we described in previous chapters and which we have called Big Civic. He has been sent the Magic Quadrant by a SoftCo executive keen to report the ‘good news’ that their rating has finally improved. In turn, Phil circulates it among his colleagues, careful to add his own interpretation of what he thinks the MQ is actually saying:
See attached an e-mail from [SoftCo] with some positive news that Gartner have improved their rating of [SoftCo’s] products within the higher education sector. The diagrams are worth looking at because they show that [SoftCo] have improved since 2004 but also that they have a long way to go before they overtake their competitors.
(Email from IT manager to colleagues)
Although the vendor is keen to highlight a change in position, Phil qualifies the improvement through highlighting the ordinal nature of the tool and the fact that even though SoftCo have moved position, so too have the others, and thus SoftCo still lags behind its rivals.16 In a further series of emails, Phil discusses with a senior executive at the vendor what he thinks are the specific problems that Gartner find with SoftCo. He receives a reply to his email where the vendor appears to accept the assessment:
Yes, we need to move ‘North’ in the execution axis and ‘East’ in the vision section. We really need to push across the line into the ‘Leadership’ Quadrant. Implementation (speed, cost - same thing, to some extent) remains a challenge
(Email from SoftCo to IT manager, Big Civic)
At this point, we simply note how the properties of this vendor appear to be settled and adjusted to those of the MQ. The various actors present appear to accept the alternative conceptual technology set out and agree that Gartner has ‘correctly’ identified that SoftCo has a poor ‘ability to execute’. However, this is not the end of the matter. What then develops is a fascinating and quite unexpected series of events. Rather than simply accept the assessment, Phil discusses with the vendor how he might be able to improve SoftCo’s position:
I think that the [CRM] final result will help move things much further. If we can then exploit BW [Business Warehouse] to include financial and other
information then we should help to move the [SoftCo] position further in the right direction. I think that it is important for Gartner to realise that [SoftCo] are building up momentum as they move across the MQ.
(Email from IT manager to SoftCo)
The CRM project is a customer relationship management system being built by SoftCo and implemented within Phil’s organisation. It is seen as a significant flagship venture since it brings together and integrates several previously unrelated ERP modules - one of which is SoftCo’s Campus module - with other modules (such as Business Warehouse). What Phil is suggesting is that, once the CRM project is implemented, news of this could be fed back to Gartner and that this would provide evidence affecting SoftCo’s standing!
Big Civic becomes a ‘test case’
It was at this point during our fieldwork that we wondered how this might happen; how could the CRM project be linked to the MQ in this way? We watched with interest as the IT manager attempted to gain Gartner’s attention. Having recently become a Gartner client, Phil had access to their analysts and his main point of contact was someone we are calling ‘Bob’. We observed as Phil deepened this relationship: they began to conduct regular telephone conversations; to participate in lengthy email exchanges; and Phil would engineer meetings with Bob in various places around the world. Phil discusses this blossoming relationship with one of his colleagues:
He [Bob] is coming to Big Civic in early November to a...conference. I tend to speak to him approximately every two weeks. He is really interested in seeing what we have done in Big Civic. He is also watching [Kent Uni] and [Purse Uni] (?) at the moment. I think that he will also watch [West Uni] in the UK as well to see whether [SoftCo] can hit implementation dates. I am sure that we can generate some really good publicity from our CRM project.
(Email from IT manager to colleague)
According to the email, Gartner are watching a number of sites around the world, from which they will gather evidence about SoftCo’s ability to execute. Moreover, Big Civic has become part of this calculative network. This raises a number of issues, not least just why Phil might go to such efforts to improve SoftCo’s rating.
Calculating actors
During the same period, Phil was also in regular contact with a number of SoftCo executives, continuously telling them of the influence Gartner were
developing among decision-makers. The following message is typical of these kinds of interactions:
I would suggest that [SoftCo] need to be aware of quite how much influence Gartner are developing amongst the higher education community in the UK. This could actually be good news, given Gartner’s comments about [SoftCo] and [BigVendor]...Buit I suggest that your higher education team should become well aware of Gartner’s comments because they will certainly be known to the university IT Directors (though whether we would agree with them is something else!).
(Email from IT manager to SoftCo executive)17
The vendor executive replies to Phil, and appears to be grateful for the work that Phil is doing with Gartner:
I appreciate your ongoing dialogue with [Bob] of Gartner. As you know, we also have a parallel dialogue with Bob. I agree that he is looking for [SoftCo] to ‘execute’ on the ‘vision’ (in Magic Quadrant terms) in terms of key projects such as yours and [Purse Uni’s].
(Email from SoftCo executive to IT manager, Big Civic)
Phil is more explicit still in later messages, outlining the specific interest Gartner are taking in his project, as well as the work he is doing to encourage this attention:
Gartner ([Bob] especially) are following every twist with great interest. He wants to spend much time with me in Orlando before and during [a forthcoming US conference] (he’s invited me on to a User Panel on the Sunday HE Symposium to discuss the question ‘What message would I like to give to my ERP vendor?’!!). He also intends to visit [Big Civic] during his trip to [UK conference] (being held in the [Big Civic] area at the beginning of November). I am giving him very positive messages -he is very interested in the timescales of the project - possibly, because he is looking for evidence that [SoftCo] can implement good/solid implementations in a short time-scale. He is looking for similar evidence from [Kent Uni] and some other critical US implementations.
(Email from IT manager to SoftCo)
Phil outlines to the vendor how their position on the MQ is now linked to their performance at Big Civic. What Phil hopes to achieve is to exert pressure on SoftCo to continue to devote resources to his CRM project (the development started well but has floundered in recent months). SoftCo need to improve (not worsen) their ranking and he thus anticipates that Gartner’s interest will have a positive effect on the vendor. In another email to a colleague, Phil describes his overall aims:
Things are getting ever more interesting for me and the [SoftCo] relationship. They are really moving into a ‘partnership’ role -throwing in highly competent resources to ensure that we go live on 10th October. Though I guess it helps that they realise that [a senior Gartner analyst] has told them that Gartner are watching [SoftCo’s] ability to implement at each of three universities in the world ([Big Civic], [Kent Uni] and [Purse Uni]) and that their results will materially affect whether [SoftCo] move from the lower-left quadrant to the topright!
(Email from IT manager to colleague)
To summarise this section, we can say that the MQ has two principal effects. First, it has framed the setting so that the means by which vendor rankings can be improved have been defined. No longer an abstract or diffi-cult-to-measure notion vendor performance is translated into the most tangible of things: to repeat Phil’s words from above, the implementation of its systems in the three organisations ‘will materially affect whether SoftCo move from the lower-left quadrant to the top-right’. Second, the fact that vendor rankings are intertwined with the success of these projects opens up the possibility of new kinds of action. In particular, the MQ has created actors with the ability to calculate and to think and act in different ways (Miller 2001).
‘We can’t delay the go-live’
Let us now turn to the CRM project, for if SoftCo was to improve its position, it was essential the implementation continues smoothly. Indeed, as it approached its ‘go-live’ date, all appeared to be going very well. Despite initial problems, SoftCo had now ‘pulled out all the stops’ to ensure everything was a success.18 Overnight, however, serious problems began to emerge. Some among the internal IT team at Big Civic were asking Phil to postpone the go-live till later in the month (as they feared it was too early to implement since some problems had been found).19 Yet Phil was reluctant to move the date, seeing any delay as damaging. It was exactly the kind of evidence that would underwrite Gartner’s existing assessment of SoftCo. This presented Phil with something of a dilemma: to follow the advice of his team and postpone the go-live date; or to soldier on as planned and hope things would work out. He spells out the nature of the problem in a message to his internal IT team, suggesting to them that they should carry on:
I’m trying everything to ensure that we do not delay the go-live. It critically depends upon [SoftCo] resource availability. Gartner are watching closely because they have severe questions about [SoftCo’s] ‘ability to execute’ within the [sector-specific] environment. They have no problem with [SoftCo’s] ‘Vision’. Their views of these two parameters result in
[SoftCo’s] position in the Magic Quadrant. They are currently NOT in the ‘top-right’ quadrant.
(Email from IT manager to his IT team)
Phil knows that a delay will be potentially ruinous for SoftCo; not only will their position on the MQ be affected but, he suspects, further Gartner criticism will negatively influence SoftCo’s decision on whether to continue investing in his particular industry sector.20 Thus, he decides to push ahead with the go-live, fully aware that the software is not tested properly and that it will introduce risks for his own organisation:
Apart from the immediate [CRM] project team, I am now also passing the message around the rest of the ISS that we will have to go live on 10th October - ‘cold turkey’ techniques may well be required. This will introduce risks to several other areas (e.g. inadequate testing of the ‘common desktop’ image across all 10,000...PCs, etc.)...However, we will need to prepare detail cover/support plans for the days/weeks after 10th October because we know that we will be going live without adequate testing/training.[our emphasis]
Nevertheless, and despite his efforts, further problems mount up and several days later the realisation dawns that they are not going to meet the go-live date. The project therefore has to be postponed. However, a second go-live target is quickly set and when the new date arrives, and despite the fact that many problems have still not been resolved, the system is implemented. A few days pass and it becomes apparent that in the rush to implement, the system is not working as it should; there are numerous difficulties and it is eventually decided to shut down the live system while problems are rectified. For over a week Big Civic is left without an external facing system in one of its busiest periods of the year! The genesis of the problem is contested as both the vendor and Big Civic each blame the other for the debacle. A complex (and heated) debate ensues.
In the meantime, this provides Phil with a difficult issue: how does he break the news to Bob? The implementation did not go as planned, there have been issues with the vendor and Big Civic was left without an external facing system for several days. In an email to Bob about a different issue, he adds the following postscript:
The [CRM] project at [Big Civic] is continuing to go really well. I have decided NOT to risk going live on 10th October but to delay until later in the month. We will still have succeeded in going from project mobilisation to go live on a raft of [SoftCo] modules in 8 months - I just don’t want to risk things by implementing without exhaustive user testing. However, I will be able to demonstrate to you what we have done in a ‘QA’ environment, if you wish to see it.
(IT manager’s email to Gartner)
What Phil does is to put to one side the problems that followed the implementation in favour of the more positive message. Gartner will not therefore be told of the ‘chaos’ that ensued at Big Civic. How are we to understand this? This is also a kind of labouring that makes vendors comparable, though it may not necessarily be typically phrased in that way (as a labour of comparison, since it could just as easily be described as a ploy). However, there is more to this than the notion of a ‘ploy’ suggests (Callon and Law 2005). The MQ is supposed to simply describe and analyse vendors but, as we can see, it interacts with these entities. It is the presence of the MQ that has partially caused the problems through ‘encouraging’ Phil to stick as closely as possible to the original implementation strategy. The MQ invites them to conform to an ideal (a demonstrated ability to execute) and to provide evidence that would refute the tool’s assumptions. However, these kinds of projects are, as those who study them know very well, highly problematic. There are numerous difficulties that can be attributed to both sides. What Phil does is to draw a boundary around the things that will go forward to Gartner and those that will not. SoftCo’s failings, as he sees it, should not be taken into Gartner’s account.
How satellites report back to Gartner
We have argued that, in compiling MQs, Gartner hand discretion over to others - as Gartner are keen to emphasise, it is not them but the ‘community’ who provide the judgements on vendors. In effect, it is these others who have the power to say whether a vendor can execute or has vision. We describe this process through analysing how one satellite reports back to Gartner and in doing so negotiates this discretion. The particular episode took place in the US, where Gartner were organising a symposium to coincide with a major IT conference. Phil travels to the conference (as does one of the authors), one of his aims being to advertise the ‘success’ of his CRM project to other potential customers. However, he also hopes to meet and update Bob on progress (see Figure 7.1)
At the conference, one of the authors is sitting talking to Phil when Bob comes over. Paul introduces the fieldworker as ‘an academic conducting research on [SoftCo]’ to which Bob responds in a jocular fashion ‘I sure would like to look at some of your research.’ He then straight away begins to discuss SoftCo, turning to Phil and telling him how he has just heard that they are having difficulties with one of the other organisations Gartner is watching (West Uni):
Bob: Chris [from West Uni] and I were just talking, she has put some ultimatums out with them [SoftCo].
Phil: Yeah, the real problem with them, [West Uni], is that they have always written their own systems and they have gone for BoB [best of breed] but when they start hitting sort of a [GenteSys] or a [SoftCo] they think that it is going to be straightforward...So, so she has got problems?
Figure 7.1 The calculative network. Industry analysts will often attend large tradeshows like the one shown here to meet vendors and their customers. These are the kinds of spaces where community knowledge is produced and exchanged.
Source: Photograph by authors.
Bob: She said that they are two million pounds over budget and they haven’t even started implementation.
Phil: Oh, I think that a lot of that is going be, the guys from [SoftCo], the ones that I have been talking to, it is just that the account manager of the [nationality] is bloody useless.
Bob: But that is a key...
Phil:...what’s absolutely critical, what [SoftCo] have been doing, is that in the UK, they have been recruiting, and they have been recruiting some really good people. But those guys, I don’t see them at [West Uni] yet.
This interchange is interesting because Bob begins the conversation by highlighting SoftCo’s failings and doing so through invoking the ‘community’ view (it’s not him but Chris from West Uni criticising SoftCo). In contrast, Phil attempts to defend SoftCo through shifting the focus back on to West Uni’s lack of experience with these kinds of generic systems. He also suggests that things are improving since SoftCo have just recruited ‘some really good people’. This exchange goes on for some time in this manner, with both providing contrasting evidence. Phil is forcing Bob to both explain and defend his assessment of SoftCo, which Bob appears able to do - and in
a robust manner. This confrontation continues and eventually Bob is forced to be less guarded, telling Phil what he thinks are the real problems with SoftCo:
I told them [SoftCo] seven or eight years ago that they needed to start investing in the Higher Ed sector. We have a saying: ‘Do something or get off the pot’. Have you ever heard that? [Phil: Yeah.] In essence what I told them, it’s like ‘You put your toe in Higher Ed but you really haven’t committed.’ They said: ‘We just hired! We got ten people writing the student system’ [Phil: Gosh.] I said: ‘Are you kidding me?’ I said: ‘How can you? I mean, that’s embarrassing!’ I said: ‘The smallest software companies in the US. would have fifty or sixty.’ I mean, [DataSys] have got fifty, sixty people. [GenteSys] have 100, 150. [BigVendor] have 150. You know ten people is just nothing! They are up to, I don’t know, twenty, twenty-five now, but still it is not what I would call for the size of the company, I mean they have the resources to be a global leader in Higher Ed if they want to be. It is just that they have just never made the commitment. And that is what you are saying?
What we see here are two actors opposing each other through offering contrasting accounts of the qualities of a vendor. Phil openly challenges Gartner’s assessment of SoftCo and Bob is forced to defend the position. While Phil states that SoftCo is improving, it is clear to Bob that they are not sufficiently committed to the particular market. As he sees it, they are being opportunistic in this market (‘they could be the global leader in Higher Ed if only they wanted to be’). This particular thread of conversation ends when Phil is forced to fall into line with Gartner’s assessment. Finally, and despite all previous efforts, Phil has to concede territory to Gartner and he appears to accept Bob’s assessment. Before they part, however, there is one more exchange that we mention since it provides an important insight into how Bob views Phil.
Phil tells Bob that while he has been in the US he has been attempting to demonstrate to SoftCo that there is a potential wider market for the CRM system that SoftCo has developed for them. He had been encouraging various other conference participants (including the fieldworker) to go and visit the vendor display hall to ask for a demo at the SoftCo desk. He then reveals to Bob that, even though it is a SoftCo product, his organisation (through an administrative failure within SoftCo) actually owns some of the intellectual property. In addition, he goes on to describe how, even if his efforts do lead to potential sales, he is not going to simply allow SoftCo to sell the system -at least without first negotiating a deal:
So Neil’s gone along.and they have been demonstrating this to say, ‘It’s real.’ What I’m after is to create a demand.I am trying to create a demand and say: ‘Ok, right. Now, you [SoftCo] can’t sell it: what we want to do is this’.21
Upon hearing this, Bob laughs wildly, turns to the fieldworker, and announces, ‘I don’t want to play poker with this guy.’ This comment is highly instructive. What we are arguing is that the MQ is a collaborative venture: Gartner rely on their satellites to keep them ‘close to the action’. To do this they hand them the calculating equipment (Callon and Muniesa 2005) such that it is the satellites that have the discretion to say whether a vendor is doing well or not. However, no matter how strong the calculative agency of these satellites might be, it is relatively weak when contrasted with the cal-culative power that Gartner has constructed for itself. As Latour (1987) has convincingly argued, for instance, if facts are to be overturned, this requires the mobilisation of at least an equivalent size network of actors and resources. In front of Phil is a vast organisation of experts equipped with research protocols, networks of users, and various other equipments - with which Phil cannot compete. The final nail in the coffin, however, is that Bob also has the power to characterise (and thus draw a boundary around) Phil’s input. There is an understanding among industry analysts that people like Phil are actors in their own right and that they will have particular interests to pursue. In his comment (‘I don’t want to play poker with this guy’) Bob acknowledges this fact and effectively limits his ability to intervene within the frame.
Conclusions: reconstituting the technical field
Michel Callon and colleagues (2002) suggest that the economy can be understood in terms of the regimes of expertise, devices and practices that measure, classify and draw boundaries around the properties of products and technologies. In this view ‘comparison’ is both central to the organisation of the economy and the ranking of products like software but also, at times, highly controversial. The suggestion in the work of Callon and others, and what we have attempted to build on here, is that, in order to understand what they term the ‘economy of qualities’, we must analyse how equivalences between vendors are made and resultant controversies dealt with -what we have described as the labour of comparison. We have focused on the Magic Quadrant (MQ), which is a contested object but also one that has developed influence. Our aim has been to understand both the appeal and the controversy it has generated: more precisely, to answer the question of how it is that this contested object can wield significant influence within the marketplace. We have done this through treating the MQ not according to a representational idiom but according to a ‘performative’ one, deploying recent ideas developed by Callon (1998, 1999, 2007) and MacKenzie (2003, 2006a, 2006b) where they suggest that theories and models play a crucial role in the doing of the economy.
However, we need to be clear. We are not suggesting that all of Gartner’s research is performative - indeed, they regularly produce predictions that have little, or perhaps only short-term, influence on the market. As scholars, we need to be more precise about the different types of research produced by
these kinds of intermediaries (which has arguably not been the case to date). We have characterised Gartner’s research according to the following typology: (1) as infrastructural knowledge; (2) as visions let loose; and (3) as statements and their world, all of which have different effects on the market (see Pollock and Williams 2007). The MQ fits into this latter category. We argue that Gartner do not describe a state of affairs that already exists. Nor do they simply offer a new means of representing and positioning vendors. Rather, the MQ is a device interacting with, and modifying, its object of study. The MQ is playing a hand in remaking aspects of the IT sector - it is reconstituting the ‘technical field’ and how people make decisions about information systems and other IT.
It has done this principally in two important ways. First, this is through offering an alternative conceptual machinery. We have shown how the MQ has set in motion a new world, where it is attempting to shift how people conceive of vendors and systems through offering different assessment criteria. This equipment prioritises ‘strategic’ and ‘ordinal’ criteria over ones that measure ‘accuracy’. Instead of evaluating local commensurability, how a potential system relates to specific customer needs, it has created various new kinds of ordinal relationships that privilege equivalences between vendors. Through bringing vendors together in the same space, it has put previously incommensurable technologies and systems on a scale; it has defined the two dimensions of this scale and created the possibility of ordinal assessment of suppliers/their offerings. Second, and this is where the tool has raised much controversy, it has also attempted to provide a mechanism to collect and analyse information based on this new conceptual machinery. Gartner have set up an extensive ‘calculative network’ where their analysts draw on the views and opinions of those who have purchased, and are working with, the systems of suppliers under analysis. This knowledge has an unusual status (being informal and highly subjective). In other words, Gartner are attempting to find a way where ‘community knowledge’ is no longer the highly local form of knowledge it once was but can be turned into a form of more robust - even commodified - knowledge that can then travel the world.
What we have also shown is that the MQ is not a stable but a contested object. It divides opinion between those who appear tightly wedded to it (for various reasons) and those who are opposed to it (for reasons to do with its status and potential bias). What is interesting about the MQ, however, is that this seemingly makes little difference. It does not have to be accepted across an entire community to be effective - which is in contrast to many accounts of the operation of scientific communities, for example. Despite the fact that people regard the device as flawed, in practice many still use it. Thus, the MQ inhabits a rather strange place where it is viewed with scepticism but trusted in practice. How do we understand this - and to return to our question - how do contested objects wield such influence?
We are suggesting that while it divides opinion it also has been able to establish a number of new realities that are less contested (less irregular and
more ‘prescription-like’), all of which have begun to nudge people across the boundary from theoretical opposition to practical use. These new realities were enacted because the MQ is an ‘interactive object’. It does not sit outside the arena but has come inside to modify its target setting. This was visible in at least two ways. First, the vendors being assessed were clearly ‘on the move’ (Hacking 1999); they worked hard to adjust themselves to the new forms of assessment and classification offered. They increasingly conceive of themselves, as do many of their customers, according to this new analytical machinery. The terms set out by the MQ are more and more the focus for decision-making: in some places, a vendor by its very nature is able to ‘execute’ or does not have ‘vision’.
Second, the device was a resource shaping not only the activities of vendors but also the practices of its users. We saw the example where one IT manager gained access to the ‘calculative networks’ of the industry analysts and modified his implementation strategy so that his project might serve as positive evidence of one vendor’s improving performance. Even though his intervention did not work as anticipated, the episode demonstrates how the domain has been framed. A boundary has been drawn around those things that are seen to count as an improvement in performance as well as the means by which the evidence is gathered. This suggests people are increasingly able to see the effects of their own actions in relation to these kinds of tools. The IT manager was able to place himself within the calculative network and knew what might happen if the industry analysts were to get wind of the near-disaster that had occurred in his organisation, but he also knew how a positive outcome might lead to an improved ranking. This ‘ability’ to act is not something that has come naturally but rather is the product of this new calculative equipment - or the particular socio-technical agencements to which an actor belongs (Hardie and MacKenzie 2007).
In summary, what we have attempted to do is identify the important role being played by this new kind of intermediary in technology procurement and the operation of technology markets. Gartner, by enabling systematic, commodified access to community knowledge, help would-be adopters who, given the incompleteness of information and the importance of intangible issues, would otherwise be hard-placed to assess vendors and their offerings. This includes assessments, for example, regarding the future reliability of the vendor and its commitment to that technological field. Gartner and other analysts also help shape community sentiment about the boundaries of technical fields and their future direction of innovation. We return to Gartner once more in Chapter 9 where we argue that their work - this commodification of networked reputation - is growing in importance.
8	Passing the user
Searching for expertise in globalised technical support
Neil Pollock, Christine Grimm and Robin Williams
Equation
Urgent message sent to the SoftCo support portal
We have recently gone live with a portal solution at [Big Civic University]...The project deadlines did not allow sufficient time for testing all operating system/internet browser combinations that we need to support given that our portal is externally facing and can be accessed by any internet user. Since go-live we have identified two serious problems with the Firefox browser which prohibits the submission of an online application to our portal by prospective applicants -hence we need a resolution as soon as possible.
This message is a call for help sent by a software package user to the vendor of their recently installed Customer Relationship Management (CRM) system. Having just gone live, it has recently developed serious problems, leaving the user organisation without the means to interface with potential customers and the outside world. This is especially crucial as it comes at one of the adopting organisation’s busiest periods of the year. Moreover, the system downtime is seen by many within the user organisation as a ‘disaster’.
Crashes and downtime are an unwanted but familiar event within the daily life of software package vendors. The largest of vendors might receive hundreds of thousands of requests for help from users each year. This is partially because of the sheer number of organisations these vendors service, but also because of the complexity involved in implementing packaged enterprise solutions at user sites. These crashes can be damaging not only for the organisations experiencing them but also for vendors themselves, where failures can have irreparable implications for reputation and standing. As we saw in the previous chapter, such episodes raise questions about the ability
and performance of vendors, and raise concerns surrounding the reliability of their products. The Gartner Group has estimated that most implementations are dogged by serious problems and that, for instance, nearly two-thirds of all ERP systems are not fully implemented even as long as three years after they were initiated (Gillooly 1998; Gargeya and Brady 2005). Thus, with software system disasters and difficulties attracting much (practitioner and scholarly) attention, vendors are taking the technical support of their products very seriously (Gable 2001; Light 2001).
We would observe that technical support has become one of the main activities of large package vendors. This is because, as mentioned in the introductory chapters, many vendors have now delegated other important aspects of the software package lifecycle (implementation, training, etc.) to networks of partners, leaving them free to concentrate their attention on only a few key activities (of which technical support is one, sales, marketing and new product development being others). As evidence of the growing importance of the support function, many of the larger vendors have developed highly sophisticated organisational and technical networks to deliver help. As yet, however, not much is known about these networks: what they look like, what they mean for the relationship between vendors and their users or for the nature of technical support itself. We still find a marked neglect in the literature of system maintenance and customer support (with noteworthy exceptions - such as Gable [2001] and Light [2001]). We might suggest that Social Study of Information Systems here unintentionally reflects the knowledge hierarchy that surrounds the commercial supply of packaged organisational technologies, whereby within the vendor organisation, for example, the client will first encounter key vendor staff responsible for development and sales, followed by those specialising in implementation and training, and finally with lower-status staff involved in maintenance and customer support.
We would also suggest that, in principle, technical support is unlike other phases in the package lifecycle which are characterised by processes whereby suppliers actively detach themselves from customers. During technical support, however, there is seemingly a necessity to interact directly with users in a more substantial way. We return to this point throughout the chapter, primarily because there is a very influential strand of work in Technology Studies and the Sociology of Technology more broadly that has argued that repair is an irredeemably ‘situated’ and ‘territorial’ activity. Julian Orr (1996, 1998), for instance, wrote one of the most comprehensive sociological studies of technical support, discussing in glorious detail the work of engineers repairing photocopiers, paving the way for what we might describe as ‘Orr-type’ studies. Drawing inspiration from the growing influence of interac-tionist and situated perspectives at the time (e.g. Suchman [1987]), as well as an associated enthusiasm for local ethnographic studies, Orr showed how repair work demands not simply theoretical knowledge of an object but also understanding of the social and technical context in which that machine is
used. His work has been highly influential (see Bechky [2006], for instance, where the importance of this work is forcibly defended) and we return to review it more fully below. However, for now we simply note how, if correct, his arguments could be seen as a test of our earlier discussion, since it appears to challenge the vendor strategy of ‘generification’. Here, at last, in the process of technical support, vendors are forced to deal not with generic but with individual particular sites, not with aggregated or ‘smoothed’ demands but with highly specific and ‘unique’ technical problems. And there are, it seems, good reasons for doing so.
At first glance, the technical support of software packages appears to be one of those occasions where, by necessity, there should be close physical interactions between vendors and users. It is a domain where Orr-type forms of support should, in principle, be visible. This is because standard software solutions are highly complicated organisational technologies (much more so than the relatively simple equipment discussed by Orr) and users experience no end of problems when attempting to adapt and integrate them with their organisational environment and legacy systems. This is evidenced by the now vast, and still growing, academic and practitioner literature on implementation difficulties (only a small fraction of which we are able to review in Chapter 2). Thus, reading these studies it might be assumed that problem-solving requires physical proximity and face-to-face interactions (see Wagner and Newell [2006], where a version of this argument is discussed in the case of ERP).
Yet this appears increasingly not to be the case, at least not at the vendor where we conducted our study. This organisation, which we have called SoftCo throughout the book, is one of the largest and perhaps most successful vendors of organisational software packages in the world. At SoftCo, we found that when users experience crashes and downtime, even in the most serious and ruinous of cases, they no longer engage in Orr-type forms of support. Instead, help is delivered remotely through what might be described as a globalised face-to-portal model. That is, problems are ‘lifted out’ from their local settings and brought back to be worked on within one of the vendor’s numerous labs. In this model, vendor programmers rarely meet with users and have relatively little specific knowledge of their local sites.11n short, it seems the technical support of software packages no longer requires the kind of proximity/intimacy it once did, and that Orr-type forms of support are no longer practised (or, if they are, have been significantly reshaped).
As we have argued throughout the book, micro-sociological and ‘localist’ conceptions of technology are no longer by themselves adequate to study software packages. While effective as a research methodology in producing rich local pictures, in isolation they tend to provide rather unbalanced accounts. While technical support is ultimately delivered in a local context of immediate action, it is also patterned by other things. In this case it is the fact that support is increasingly being stretched out (Nicolini 2007) across a global network of laboratories and technologies. This includes a distributed computer-based organisational form that allows the vendor to coordinate
and manage technical support in novel ways. Where a system crash cannot be resolved by one vendor lab, for instance, either because programmers there lack specific competence or because they are finishing work for the day, problems can be passed around the vendor’s geographically distributed sites in search of ‘expertise’ and support staff who are awake and at work. This means user problems can, in principle, receive immediate and constant attention wherever and whenever they arise. Indeed, this ability to pass problems around has become a taken-for-granted feature of repair work here. At SoftCo it is increasingly common for problems to have travelled the world and been worked on by many different software teams before a resolution can be found (see Pentland [1992] for an early discussion of this kind of problem distribution).
Clearly, the kinds of technical help delivered by software package vendors today have to be distinguished from earlier kinds of repair, starting with the obvious point that what is at issue is no longer mechanical repair but the operation of an information infrastructure operating through extremely complex suites of software. The problems as well as the means to resolve them are increasingly ‘virtualised’ and ‘distributed’. Moreover, the kinds of relationships maintained within networks that span different geographical and time zones are seldom captured by prevailing localist accounts. It seems that an important shift in how (as well as where and when) the user and their computer problems are managed and administered within vendor organisations is underway. We identify a number of interrelated features for study. First, how vendors distribute what was traditionally a highly situated and territorial practice across time and space. Second, how this allows for new ways of providing technical support across an organisation and also reconstitutes the nature and practice of support (i.e. the vendor has to find alternative ways of fostering the interactions and knowledge flows necessary to enact repair). Third, the consequences for different actors who participate in the support process and whose relationship to each other is significantly reconfigured (this includes the amount of discretion participants can exercise within the process). Fourth, this increasingly virtualised and distributed community has unexpected and perverse consequences, forcing vendors to develop new technologies and regulation in an effort to organise support more effectively. Finally, and unsurprisingly perhaps, the globalised face-to-portal model has itself a tendency to break down, and users find themselves having to escalate their problems beyond the narrow, restricted and virtualised world of the portal.
The organisation of technical support
Orr-type forms of support
Much of the sociological literature on technical support has focused on the topic through the lens of situated and territorial-based expertise. For
instance, Julian Orr, in his (1996) study of photocopy repair engineers, argues that to have expertise in this context demands not simply theoretical knowledge of actual machines but also knowledge of the social and technical context in which they are embedded. His work is discussed in the context of efforts by management to standardise - that is, to increasingly map out and script - the process of photocopier repair so that it might be subject to increased rationalisation. Orr’s thesis, however, usefully shows how managers found documenting the full range of steps required to effect repair difficult, if not impossible, since many errors and photocopiers were ‘peculiar’, requiring forms of knowledge and expertise not easily captured in documented processes. The problem was that, while management assumed photocopiers and their usage to be similar, meaning they could codify the processes involved in their repair, this was usually not the case. Photocopiers would apparently behave quite idiosyncratically. Orr writes: ‘Work in such circumstances is resistant to rationalisation, since the expertise vital to such contingent and extemporaneous practice cannot be easily codified’ (ibid.: 2). The upshot was that, despite rationalisation attempts, technical problems could not be approached abstractly but only through building a sensitivity to the local settings in which machines were used. Orr also goes on to describe not only how technicians had concrete interactions with social contexts but also that the ‘territory’ in which users were located made a difference to support. ‘The social arena of service is defined spatially by the territorial divisions of the service world, and these territories are a fundamental category of concern for the technicians’ (ibid.: 62). According to Orr, the technician ‘owned’ a particular territory and therefore had particularly close relationships with users in ‘their’ area. This meant the kinds and levels of help they provided depended on whether they were working with their own or someone else’s users: ‘Whether a machine is interritory or out-of-territory changes one’s responsibilities to the machine... and it crucially affects one’s relations with the customer’ (ibid.: 65).
Situated and territorial forms of explanation have been extremely productive both in Technology Studies and Organisation Studies and not just in terms of technical support but in discussions of work more generally (see Barley 1996; Pentland 1997; Star and Strauss 1999; Bechky 2006). As such, they were a useful corrective to earlier assumptions, mostly from Management scholars and Computer Scientists, that most forms of technical work were amenable to codification and systemisation and that, once mapped, this knowledge was free from its context (Hammer and Champy 1993). In contrast, Orr-type studies usefully highlight how computer system failures, for instance, are shot through with ‘sticky’ local context, thus weighing them down (in the sense they are not easily lifted out of their setting). While providing an important corrective to (once?) dominant discourses that presumed codification and rationalisation of work, analysis cannot stop with this initial critique. We need to go further. There remains a prioritisation of ‘propinquity’-type arguments in discussions of technical work where there is a still a strong assumption that actually ‘being there’ can
deliver forms of knowledge, or manage relationships, not obtainable in other ways.2 Arguably, this somewhat overemphasises obstacles to the delocalisation of technical expertise which, coming at a time when various forms of expertise have been disembedded from local settings (see, for example, Berg [1997]), is demonstrably problematic.
Current situated and territorial views on technical support are increasingly inadequate in a context in which globalised software package vendors look for novel ways to provide help to users. In the new form of technical support, proximity to users is obviously no longer tied simply to geographical proximity but can be enacted in other ways. Thus what is necessary is to show how earlier assumptions about ‘being there’ and ‘propinquity’ are extended in this new globalised model. Many of the features that Orr took as being crucial for technical support are still there, albeit stretched out across a global network of support labs and user organisations. The insight that motivates this discussion is the vendor’s attempts to configure the portal in relation to the issues of ownership and control over technical problems.
‘Ownership’ and ‘control’ were also key themes within Orr’s book. While reading his work it is not difficult to draw the conclusion that these were some of the very reasons support work could not be scripted or rationalised. Technicians had little control over problems or the places where the problems occurred; they simply had to react to what they found when they visited sites. He writes:
[t]he technicians are responsible in a world in which they have very little control. Their job is to respond to trouble and to anticipate and avoid trouble when they can, but the setting in which they perform is largely constructed by other people, inhabited by other people and inherently unpredictable.
(Orr 1996: 158).
Much of his book, therefore, is a description of how technicians handle problems (and work in settings) that are not their own. It is clear that certain Orr-type issues - for instance, the careful management of problems and settings - can be extended into the globalised face-to-portal environment. What is required here is to recast the ways ownership and control are played out when support is delivered at a distance. In Orr’s case, he details how face-to-face interactions played an important role. However, in our case, the vendor has attempted to mediate this issue through applying a particularly interesting strategy (what we call ‘endowing the user’). Before we describe this, we want to discuss in more detail what we mean when we talk about globalised face-to-portal support.
Distributed forms of support
We are describing a form of support where problems, in order to be resolved, are lifted out of their contexts and then distributed. Once delocalised, user
problems are effectively ‘mobile’ and thus can be managed and co-ordinated in novel ways. While there has been a growing interest, in Sociology more generally (Urry 2002) and IS research in particular, on discussions of ‘mobility’ this has primarily focused on the mobility of people and the various technologies that enable, for example, IT workers to move back and forth between various places (D’Mello and Sahay 2007). However, there has been little detailed focus on how technical knowledge or work itself is distributed. One important exception from the field of Organisational Studies is Brian Pentland’s (1992, but see also [1991] and [1997]) wonderful description of the functioning of telephone ‘hotlines’, which, while sharing some of Orr’s concerns, arrives at a very different account of technical support work. At the two vendors he studied, they operated what was essentially a ‘telephone model’ of support. That is, expertise was almost entirely delivered remotely from specialised offices and by dedicated workers manning phones. The strengths of the article are the various conceptualisations he offers to describe this primarily technology-mediated and distributed activity. When faced with difficult calls, for instance, support workers embark on what Pentland (drawing on Goffman) calls ‘organising moves’. That is, technicians do one of two things: they move the solution through getting help or they move the problem through giving calls away. In terms of the former, this involves the ‘quick question’ aimed at colleagues, asking them to ‘take a look’, and in terms of the latter Pentland writes:
There were four generic kinds of moves through which this was accomplished: assign, refer, transfer, and escalate. These moves resembled the ways to get help, except that they involved assigning the responsibility for a call to someone else. The responsibility could shift back to the originator at a later time, but for the time being, the call would be someone else’s problem.
(Pentland 1992: 539, our emphasis)
What he characterises are the practices found within distributed forms of support (where others are drawn into the problem-solving process) and problem distribution (where problems are pushed out to experts elsewhere). The telephone model requires alternative terminologies than those found in Orr-type studies, and thus Pentland emphasises mobility concepts rather than proximity ones. We would further suggest that the forms of distribution described in Pentland’s paper are increasingly common and are becoming a feature of how technical support and other forms of work are done.3 They were certainly present at SoftCo and, if anything, were highly amplified. It was, for instance, a strategy and resource within SoftCo to shift problems around in order to make more effective use of distributed expertise as well as to provide round-the-clock forms of support. We attempt an initial characterisation of this effort through suggesting we are witnessing the birth of what might be thought of as a ‘passing regime’. This is an organisational
form that routinely and prodigiously uproots problems from their context, packages them up so that they are mobile, and then shifts them around its geographically distributed sites in search of the expertise that can provide a solution. Thus, building on Pentland, we can say that support workers rather than ‘move solutions’ (i.e. ask for help from the physical community in which they themselves are located) now increasingly ‘move problems’ elsewhere within a virtualised community (i.e. distribute calls for help over distances to where there is expertise).
In our case, the passing regime was clearly encouraged by various socio-technical phenomena at SoftCo (a portal, the division of expert labour, and various policies and practices). Pentland does not develop this line of enquiry but it can be seen how the distribution of caller problems in his hotlines paper were similarly facilitated by the phones themselves (where callers could easily be transferred elsewhere) and the division of expert labour (beside every worker, for instance, there were lists of telephone numbers pointing to other experts scattered throughout the organisation). This is vividly demonstrated in the following quote, where a caller complains to a technician about the propensity for problems to be passed around the organisation: ‘Boy, you guys don’t have a lot of information, but you sure do have a lot of phone numbers’ (1992: 540).
Setting and research methods
Software package vendors have moved technical support online in order to both ‘improve’ and ‘rationalise’ the support processes. These, as we will show, are contradictory impulses that provide for various tensions within the support process.
The user in the portal
At first sight, the portal does appear to be a highly effective means to resolve problems. As we have said, SoftCo receives hundreds of thousands of requests for help each year and only a small percentage of these now require a site visit. Programmers increasingly interact not directly with users but mediated through a portal - and we see the enactment of what we will describe as the ‘user in a portal’ (cf. Hughes et al. 1999). This is a move that appears inconceivable from an Orr-type perspective but is perhaps more easily understood when one considers that the portal is not completely new but builds on previous infrastructures and attempts to delocalise technical support. For instance, in the mid-1990s SoftCo established what was widely thought to be the first system to allow software package problems to be reported and resolved remotely, the ‘Online Support System’ (SoftCo documentation, November 2005).
Going back further, however, the vendor operated what Orr would describe as a ‘territorial model’ of support. That is, user organisations
interacted with a specific office that had responsibility for the region in which they were located. In this model, the user may not have close relationships with individual programmers but certainly would have with the ‘office’ to which they were assigned, and, likewise, in turn these dedicated support offices knew which users ‘belonged’ to them. In the territorial model, however, the kinds of help a user might receive differed between countries and according to physical proximity and communication links to SoftCo offices. Maintaining territorial forms of support were not only resource-intensive but - because of the complexity of the various products - regional offices would be unable to house all necessary expertise. As a result, particularly complex problems could mean literally hours on the phone chasing expertise in offices elsewhere. Moreover, if problems could not be repaired through this means, they would require a visit where a support programmer would travel from a different territory, sit down with users, listen to their problems, work through their systems and finally suggest a fix.
However, packaged enterprise-wide systems have a number of features that make Orr-type and territorial forms of support difficult to sustain. Clearly, one important difference from earlier kinds of computer systems is the huge numbers of users serviced by these vendors. With many thousands of users, a vendor can potentially receive several hundred calls for help each day. The vast majority will be deemed ‘urgent’, where the user is experiencing downtime. Users may also be exerting various kinds of pressure on a vendor to affect an immediate response (this may not simply be contractual but might come in other novel forms, see below). Users will also be highly distributed and thus the vendor will have to deliver support to far-flung places. Moreover, the problems users present with will come in varying degrees of complexity and will often be intertwined with ‘sticky local context’. Turning ‘unruly problems’ into manageable ones that can be distributed therefore presents vendors with non-trivial issues.
The result is that SoftCo has moved from earlier Orr-type and territorial forms of support to a distributed online system. Through the implementation of the Online Support System, the previously dispersed activities of the different support labs were more closely integrated and the processes of organising support throughout SoftCo were increasingly standardised. The breaking up of support into territories was done away with. The result is that now at SoftCo the vast majority of requests for help are handled almost entirely at a distance by any one of a number of labs around the globe. Today, in order to enact this more global form of support, the labs are equipped with various technologies and tools allowing the vendor to integrate and co-ordinate its responses to users. The primary technology around which the labs and indeed the entire support process is organised is the ‘support portal’. The support portal is crucial for all parties involved: it is the medium by which users report and present technical problems to SoftCo; it is the platform on which the vendor organises and coordinates its internal support processes; and finally it is the mechanism by which SoftCo delivers
help back to its users. Through its introduction, support has shifted from primarily face-to-face, away from the territorial model, to principally ‘face-to-portal’ (Knorr-Cetina and Bruegger 2001) interactions. Traditional links between the users and programmers within a regional support office have been severed and replaced with a system that distributes problems to where there is expertise. Once in the portal, the user can be passed to the most appropriate site, thus allowing the vendor to maintain its existing divisions of labour and provide round-the-clock support. That is, the bulk of problems are repaired by programmers who no longer meet users but interact with them mostly through an electronic interface and who know computer systems and local sites only in terms of the limited information provided via a technology.
Putting the user at the centre of the distributed organisation
While globalised face-to-portal forms of support might appear an effective means to manage large volumes of requests for help, they also have a number of perverse and unintended consequences. One issue is that interactions between vendors and users are ‘canalised’: that is, the relationship is channelled through a series of predefined pathways. In this respect, it is difficult to see how the portal allows for the kinds of intense interactions, knowledge flows and careful management that Orr-type studies describe as essential for the surfacing and repair of user problems. Even though the relationship between the user and vendor is not broken, it is attenuated. Not only is the vendor viewing the user through ICTs but the user, in turn, interacts with the ‘vendor in the portal’. This canalisation is somewhat paradoxical since the reported aim was to use the portal to provide a more effective level of support to users (through allowing them increased access to expertise and providing round-the-clock cover) as well as being more efficient (SoftCo presentation).4
Paradoxically, then, it seems that while SoftCo has the desire to build and exploit deeper relationships with users it also wishes to rationalise links with them. The vendor organisation is attempting to put the user at the centre of the organisation while simultaneously trying to distribute that organisation. To balance these tensions - and to counteract possible canalisation - SoftCo has attempted to enact a more ‘endowed user’. That is, there was seemingly a need to find some synthetic way of motivating customer orientation through a more transparent system. Modifications were necessary to atone for the geographic separation between vendor and user. Thus, a more sophisticated system has been produced to respond to user messages. Within the support portal, therefore, the user is no longer the simple generic user but one endowed with certain prostheses to ensure she remains close to the vendor. They have been handed over to the user, and this is given material form by the technology, various rights and abilities, which have the effect of giving the user a more central role in the support process. Thus ‘propinquity’ has
(potentially at least) been enacted through a complicated endowment process. The user and vendor are not near in geographical terms but can potentially become proximate in other ways.
Writers such as Akrich (1992), Latour (1992) and Woolgar (1991) have explored the ways in which technologies define spaces in which users are able to act through assigning certain competences, rights and responsibilities. This is the idea that the capacities of actors, just what they can and cannot do, are not given but constantly produced through interaction with the (material) things surrounding them. In this sense, the portal exemplifies a clear agenda to configure users (and to foster certain kinds of relationships). First, it is designed in such a way that it is the user who is able to influence the nature and pace of the vendor response to problems. Among other things, they can set the status of requests for help, so-called message ‘priority states’.5 Second, the technology also defines a moral space where, unlike generification strategies where the vendor chooses which resources go where, it is the user herself who decides. We say ‘moral’ because in this configuration, resources, at least in principle, go to those with the most pressing problems - it is a user-defined response. Finally, these endowments do not come without responsibilities. If a user is to wield influence then she must allow herself to be prompted and guided by the technologies and policies of the vendor. In fact, the more help a user demands then the more she must conform to certain (quite arduous) demands.6
To summarise, we want to show how the vendor is attempting to produce a user able to employ a certain amount of discretion, albeit located in a wider network of controlling technologies and practices. They have endowed individuals with clearly defined rights and, importantly, through the technology as well as other things, the ability to claim those rights. What it is interesting to show, now that users have been given these abilities - the fact they have been endowed with various prostheses - is how the SoftCo organisation reacts to this newly endowed user. What challenges do users as owners of failing systems, but endowed with certain prostheses, present to the vendors of generic software?
Research setting
We had excellent access to one of the largest and most successful software package vendors in the world and were thus able to observe the support process from a number of distinctive viewpoints. One of the authors of the chapter carried out an ethnographic study inside one of the vendor’s labs for a period of five months. During this time, she worked as a support employee in SoftCo’s ‘Development Support’ section, which is the highest level of technical support that a user might meet. Development Support, as we describe in more detail below, deals with all the customer messages that cannot be solved by the primary and secondary support levels. Her previous
training in programming meant that she was also able to participate in the technical work. The bulk of her day was spent responding to messages and resolving problems that were reported in the portal. Based on the period of study, we are thus able to describe the general policies and practices found within one particular lab. The chapter also follows the life of a message from inception to completion. Most of this material was gathered while one of the other authors was carrying out long-term research on the SoftCo software package implementation at an organisation in the UK. He was observing the organisation when their newly implemented CRM system crashed, which meant he was able to witness their attempts to gain support from SoftCo and their use of the support portal.
Development support
The fieldworker was based in one of the Development Support labs, where she was housed in a large open-plan office with almost two hundred programmers. The lab is spread over two floors of a multi-occupancy building located in the industrial area of a large cosmopolitan city in North America. This particular lab was responsible for developing and maintaining the vendor’s Customer Relationship Management (CRM) system (as well as Service Industry and Mobile Business Solutions). Working in the ‘support’ team, the fieldworker had her desk on the margins of the support area and close to the ‘developers’, which allowed her to move between both communities. As with any other modern work environment, the majority of the employees spent most of their day sitting at desks staring at computer screens. One of the things they were (obsessively) watching for was the arrival of ‘messages’ from around the world. These were the requests for help sent in by users through the portal.
The support programmer close to where our fieldworker sat had a number of applications open on her computers through which she could view these messages. There was, for instance, a ‘global monitor’ that presented a continually updated list of existing and new messages arriving in the vendor organisation. New messages arriving in the portal were glanced at but no more. Programmers in Development Support knew these would be dealt with by their colleagues in other parts of the Product Support Chain. They might mentally note ‘very high’ priority messages for the modules for which they were responsible, since it was likely that they would be passed to Development Support by the other support tiers within the next few hours. Having prior warning of possible incoming messages allowed the programmers a little time to organise schedules.
Globalised face-to-portal forms of technical support
We return to look at Development Support in more detail in a moment, but first we comment on the specific workings of what was called the ‘Product
Support Chain’ since it provides a quite distinctive organisational form and set of practices influencing the nature of technical support.
Distributing support across time and space
The vendor managed the large numbers of calls for help it received each day through a process of distribution. We present the journey of one specific message as it moves around the various labs to demonstrate various aspects of what we are calling a ‘passing regime’. When a user submits a message through the portal, it is received by the first link in the chain - the ‘Primary Support Centres’. There are approximately twenty of these around the world, providing a basic level of support. One vendor programmer describes what happens to a message when it is received in these centres:
The message is immediately, depending on the priority, taken by someone wherever they are in the world, whoever is awake in the Primary Support Centres...They take the messages. They have some kind of expertise depending on the component the message is assigned to. It might be the case that the customer has not read the documentation properly. And they just give them assistance on that. And then the message is solved. If Primary Support can’t handle the message, then they send it to the next level.
(Email to fieldworker)
While there are several interesting points in this quote, let us for the moment consider simply the last one. If the programmer cannot resolve the problem she receives then it is passed on to the second level of support, the Product Support Centres. While programming staff in the first level are generalists, those in the second level have specific expertise, and may be working on some of the actual application components. During fieldwork, for instance, we saw that many problems were weeded out at this first and second stage. However, if the message cannot be resolved at these two levels then it is passed to a third level, called ‘Development Support’. Development Support is the final link in the Product Support Chain and there are six such ‘labs’ around the world. We demonstrate how a curious process had developed in the context of the labs where messages went looking for expertise.
Looking for expertise
When the newly implemented CRM system crashes at one user organisation (the university we have described previously as Big Civic), they send a very high priority message within the portal. As it is of the highest classification, the message is picked up straight away by a Primary Support programmer. However, unable to help, she re-assigns the message within the portal to
Secondary Support. Having picked up the message a Secondary Support employee writes the following reply within the portal:
Please also be aware that I’m not an expert in the issue & have limited knowledge in this area...in saying this I will try to resolve the issue... However, we may need to eventually pass this msg to Development Support, they are currently out of the office until 6am CET tomorrow. Please be aware that you may have to wait until then for expert help.
Jeff, CRM Support Consultant, Active Global Support
(17.27 CET, 29 November)
The first thing the support programmer here does is to point out that he lacks expertise in the particular system mentioned, but nevertheless he says he will ‘try’ to resolve their problem. He also sets the Big Civic team up for the possibility that they may be passed to another lab in a different country (and time zone) and that they will have to ‘wait’ for help. This is indeed what happens. The message is then picked up by the Global Support Centre the next day (30th) and a programmer there attempts to fix the fault but also has to admit that neither he nor any of his colleagues in the lab are experts:
As per telephone conversation, there are no CRM-IC experts available tonight. If none of the recommended notes help resolve the problem, please return the message to SoftCo for further analysis.
Ethna, Global Support Centre (19.13 CET, 30 November)
He makes a suggestion of a possible fix and advises the Big Civic team that if this fails to work they should leave the status at ‘very high’ so that their problem will be picked up by another lab when those programmers come into work. In fact, this is what happens; the message is now taken up by Thomas at another Global Support lab and he is told by the Big Civic team that they think there is something wrong with the ‘WebClient’. Since Thomas is not an expert on this module either (and also because it is a public holiday in his country) he wants to forward the message to the WebClient developers in another lab:
I forwarded your message to our development of.WebClient. One colleague from there will get in touch with you asap. Thanks for your understanding for this procedure.
Thomas, Global Support Center II (15.40 CET, 1 Dec)
The message is picked up later that night by the ‘Priority 1 Co-ordinator’ who has responsibility for checking and following the journey of very high priority messages. He does not influence the direction of the message and appears to check only on the status of the problem (and whether or not it is in danger of ‘overflowing the portal’):
Called Graham...They are unable to save info on the CRM system which went live yesterday the 30th November. Customer needs to get this solved asap although they are willing to wait for the development team tomorrow morning.
Sean, Priority 1 Coordinator (19.26 CET, 1 December)
In this case, since the customer appears ‘willing to wait for the development team tomorrow morning’, no action is taken. In the morning, as promised, a WebClient team member contacts Big Civic. He specifically asks them to amend the status of the message from ‘very high’ to ‘high’ to allow the problem to remain with him (and not to be passed on any further). The message remains with him for nearly two weeks while he attempts a repair. However, in the meantime, back at Big Civic, there is another more serious problem in the same module and the team thus submit another very high priority message (and the whole process begins once again). This is the beginning of a long series of interactions which finishes when Big Civic remove themselves from the portal through ‘escalating’ their problem (we return to this below).
For now, we note a number of interesting features of these exchanges, and in particular how they were organised around a number of themes. First, much interchange concerned the contingency of programmer expertise. The people at the other end of the portal were always careful to state at the outset whether or not they had the competence to work on a particular problem. Indeed, the bulk of support staff had only a very general level of knowledge of the CRM package. Therefore, to ‘have’ expertise in the lab meant possessing detailed knowledge not only of the inner workings of specific products but also of how they behaved in particular social and technical contexts of use. In addition, importantly, this knowledge had to be continually exercised (this was because it was often an experience-based form of expertise).7 As a result, replies to problems, communications with users, were often quite humble (‘I’m not an expert in this issue’), with programmers seemingly willing to advance an hypothesis (through suggesting past fixes and ‘notes’ to look at) but at the same time not claiming definitive expertise. In terms of this latter point, for instance, replies often included the possibility that suggestions might not work.
Second, there was often discussion of the possibility of further distribution. Replies to messages pointed to the possibility that problems could, and probably would, be assigned to someone else, to another group of experts, sometime in the near future. In this regard, expertise was not brought to bear on a problem but had to be actively sought. This was done through the constant (re)assignment of messages that in some cases ‘chased experts’ before they went home for the night. Third, messages indicated temporal issues surrounding the search for expertise. Indeed, there were certain technical staff simply concerned with managing the ‘timeliness’ of messages (and ensuring users were aware of and content with the speed at which their problems were being addressed). Temporal issues surrounding messages formed
such an issue because of this user endowment and their ability to set priority levels. A very high priority message, for instance, had to be dealt with as soon as it arrived. This introduced a dilemma for support staff: either they maintain timeliness and distribute messages regardless of whether those who receive them have competence (‘I’m not an expert in the issue’) or they seek competence but don’t maintain timeliness (‘you may have to wait until then for expert help’).
To summarise, we have seen how technical support has been recast and inserted into a new geographical and temporal regime where users’ problems are distributed in the search for expertise or because time dictates messages should be moved on. Interestingly, while the relationship with the user is potentially attenuated (for they too interact with the ‘vendor in a portal’), we see the introduction of various strategies to counteract this. There are numerous interchanges as programmers attempt to describe the journey of problems through the portal. Perhaps ‘strategy’ is the wrong word, since it suggests a purposefulness we did not witness. It was a requirement (part of the endowment) that programmers update users who, at a distance, had potentially little idea of what was going on with their problem. The vendor assumes that once in the portal, it can present a unified view to the user, seamlessly transferring their problems around the labs, but of course the ‘vendor in the portal’ does not speak in singular but in multiple voices. This had the interesting effect of leaving support staff no choice but to draw users into, and share with them, the various contingencies and uncertainties surrounding a search for a fix within the portal. Thus, it turns out that the portal is a relatively rich space where programmers offer up all kinds of information about the journey of messages.
Reconstituting the nature of technical support
What we want to do in this section is to look in more detail at how this passing regime is made to work. We start by showing that while SoftCo need to attend to problems ‘when’ they happen, it is seemingly not concerned where they occur. This is because it has in place the means to ‘lift out’ problems from user organisations and bring them back to the labs, to be worked on by programmers away from distractions. These means include the organisation (labs), technology (portal) and also various strategies.
Transforming user problems
When a message eventually found expertise or simply came to the final link of the chain, a vendor programmer would set about investigating the problem. What does resolving the problem mean in the context of the lab? She would read descriptions of where and when the problem occurred in the user’s system; look at internal memos describing the various fixes already attempted by colleagues in other parts of the chain, and then set about resolving the problem herself.
Typically, these technologies crashed in the course of implementation because of some kind of unpredicted interaction with software components found at the user site. This meant it was often extremely difficult to establish the provenance of a bug or diagnose reasons for failure. We might say these kinds of problems were ‘sticky’ because they consistently resisted separation from local contexts (von Hippel 1988). Diagnosis in this case was not simply a narrow issue of pinpointing a fault so it could be rectified, but also involved working out who had responsibility for it. If a fault was found in SoftCo’s standard package then things were relatively straightforward and work could begin on a fix within the labs. However, if the bug was located in an area considered to be outside the standard system, then the problem was the user’s responsibility. The issue was, in other words, to transform the problem from one that was sticky to one that could be ‘distributed’ - what we might simply call a problem in the portal - or to one that could be legitimately rejected. The vendor took this kind of work seriously, for it had no wish to bring back problems other than its own, and transforming a problem required a huge effort, involving not only sophisticated strategies but also disciplined programmers to implement them. We first show an example of how a message was pushed away and then, when a complication develops, how other alternative strategies had to be deployed.
The quote below is typical of how problems were rejected, and we note how the programmer is careful not to reject the request outright but to offer truncated levels of support:
I gave you a few hints above. However, I would like to point out that I can only give support for the standard version. [Customer name] is responsible for its own customized version. Therefore, if the standard version is working, then I cannot offer support for [customer name]’s version because it shows that the problem is not coming from our side.
(Message from Development Support to user)
Even though some assistance is offered, (‘I gave you a few hints above’) it is made clear this is not sanctioned but informal help. Offering informal help turns out to be a common practice among this distributed community, as programmers attempted to make the portal a co-operative and rich environment. Although this user was not technically qualified to receive help, she was still endowed nonetheless.
Not surprisingly perhaps, the provenance of bugs, the types of help offered, and the pushing away of users were sometimes highly contested issues.8 There were, for instance, problems that resisted easy transformation and thus remained, if you like, between states. These were bugs that were not clearly identified as belonging to either party - and thus might be described as ‘liminal problems’. Liminal problems were potentially damaging for the vendor because even though there was technical uncertainty, vendor protocols dictated it was SoftCo which should assume responsibility (at least until it
could be proven otherwise). In practice, this meant ‘the clock’ in the portal ticked against the vendor, and if the issue was not quickly resolved a user could become frustrated and ‘escalate’ the matter further. According to lab gossip, one user had recently been awarded millions of dollars in lost revenues as SoftCo had failed to adequately deal with such a problem.9
The vendor was always looking for ways to avoid being drawn into such difficulties and was continuously implementing various ‘triage devices’ which could sift potentially difficult and time-consuming problems. In the case of liminal problems, for instance, it had become standard practice to ask users to run ‘two systems’ when conducting fault-finding. That is, they would run data through their localised package and the standard system at the same time and then re-perform the series of steps causing the initial error. The two-systems strategy was seemingly a highly effective triage device because through running the test the user could see with her own eyes who had responsibility for a bug. If the problem occurred in her local system only, the problem was clearly the user’s, while if it could be reproduced in the standard it was SoftCo’s.10
Yet interestingly, despite this strategy, there were still difficult cases. One team of programmers, to which the fieldworker was attached, was involved in a lengthy problem-solving exercise with one increasingly irate user. As was normal practice, they asked the user to supply information about how his problem occurred and to confirm he was running two systems. His replies were strangely ambiguous, however, and despite further questioning no clear answer was forthcoming. Therefore, the programmers had to assume the problem was occurring in their standard system. This meant that they undertook a large-scale examination of the standard system but were still unable to reproduce the error. On further questioning the user claimed the problem was ‘intermittent’ and difficult to reproduce, so the programmers continued with the investigation. The writing of messages back and forth had gone on for some several days now and the user was becoming more frustrated. Eventually he insisted the problem be given more attention, and to effect this he raised its priority to ‘very high’. The programmers working on his problem could see an internal memo had been added within the portal:
Dear Colleague, Customer called in to expedite the process of the message and the priority will raise to Very High upon customer request. The Production is not down but the system is greatly affected.
The team overseeing the problem became ‘slightly nervous’ as they were still unable to replicate the error and began to suspect an ‘escalation’ was forthcoming (which they feared, see below). Other experts were drafted in but were unable to resolve the issue. In a last-ditch effort the programmers went back to the user, asking him to confirm that he was indeed running both systems as requested and to state in which the error occurred. At this point, they receive a short reply where the user acknowledged errors were occurring
in his local customised version only! Responsibility thus immediately passed from SoftCo back to the user.
However, the days of wasted effort had caused some bitterness (since it was not the first time this particular user had attempted such a move). It was widely suspected in the labs that certain users would directly avoid responding to questions in the knowledge that the vendor would give them the benefit of the doubt. This was possible because the onus rested with the vendor to prove where a problem existed (sometimes impossible to do, especially if users withheld information, which they appeared to be doing). It seemed face-to-portal relationships created ambiguity and, on top of this, the fact that users were heavily endowed encouraged certain people to act opportunistically. Yet despite everything, the team leader advocates a ‘friendly response’, stating that even if we don’t ‘like them’ they are still ‘our customers’ (fieldwork diary).
To sum up, we described the reconstitution of technical support at this vendor and how it involves not only technology and organisation but also the transformation of user problems. Orr-type suggestions that problems are weighed down or ‘unique’ are clearly inadequate here. We have seen how bugs have gone from being ‘unruly problems’ in the local site to ones which can be distributed as ‘problems in the portal’. Problems in the portal are ‘light’, meaning they are no longer fixed to a site but have been stripped of the sticky things keeping them entangled. Achieving this, disentangling these problems from their sites, requires motivated programmers to implement various ‘triage tests’, as well as the ability to turn the distant and undisciplined user organisation into a ‘laboratory’ full of responsible actors. Users obviously participate in the work of problem resolution (it is really co-resolution) but also participate in the transformation of their own problems. It is a collective act where the user is handed part of the apparatus for demarcation: they give to - or, better, attempt to produce in - users certain responsibilities towards the self-resolution of problems (in the hope that users will rule themselves out of support). All of this is not straightforward; some users refuse to play the role allotted them - they may have other interests to fulfil - and some users attempt to exploit the geographical separation and their endowment in order to win more resources for themselves. The more senior programmers were aware of this potential and had found ways to manage users who were likely to create ‘trouble’. This is the issue we focus on in the next section, where we discuss the ways the vendor/user relationship is reconfigured. We approach this reconfiguration through focusing on how programmers manage to live with the endowed user.
Reconfiguring relationships
It was the arrival of messages that created the rhythm within the labs. The requests for help prompted programmers into action, dictated what they should work on, shaped the organisation of the support work. This rhythm
was relentless. Thus, programmers were always attentive to messages that were assigned to them. Despite the sophisticated technologies in the lab, the only way to check for new messages was to continuously open their message inbox - which they did compulsively. Once messages were received, programmers had to respond to them. The status of messages in the labs was such that it was inconceivable they might simply be ignored: very high priority messages literally challenged programmers to act on them in some way. Once a message arrived, it would be opened and the programmer would immediately add his or her name as the designated person responsible for that problem. The message was now ‘in process by the vendor’, where the portal would automatically log the time messages spent with SoftCo. This meant the programmer would have to do something with the message - either attempt to resolve it or find some other way to handle it.
Living with the endowed user
One interesting thing about priority levels was that despite being ‘set’ they were rarely set for long. Users continuously raised them when they perceived a problem wasn’t receiving the kind of attention they thought it deserved, or, just as common, programmers might attempt to negotiate them down. We say ‘negotiate’ because, once assigned, it was normally only users who could adjust a status level. SoftCo had handed this ability to the user and programmers were unable to alter levels without first seeking the user’s explicit agreement. There were some exceptions to this, of course, as when it could be demonstrated a user was flouting responsibilities towards priority levels, which meant that vendor programmers were then endowed to modify a priority. However, this endowment had the effect of drawing programmers into extensive negotiations with users about the exact nature and urgency of their problem. When Big Civic submitted their message classified as ‘very high’, they immediately received the reply:
You have logged this msg in Priority ‘Very High’...Please be aware of the Priority & its restrictions re Note 67739. You say in the msg that this is affecting the ‘go live’ of the project. Can you please tell me when this is.? Also, please provide a 24hr contact number & open all relevant connections re note 67739.
Here, the classification is queried since in the formal definition there must be an ‘absolute loss of the system’, and this is not the case at Big Civic. The programmer also makes Big Civic aware of their particular responsibilities with regard to a very high priority message and directs them towards the relevant note. As the message describes, they must now open relevant connections (i.e. make the system ready so that the problem can delocalised) and ensure that Big Civic staff are on site around the clock. When replying to the posting the Big Civic team defend their priority selection, stating:
This is creating a huge problem in our production environment and is affecting a lot of our customers. All relevant connections are open...An early response would be highly appreciated.
The programmer accepts the explanation and is therefore now unable to modify the status of the message:
I [am] aware that you have gone live with CRM this morning and that this is an urgent issue. If the message is returned, I can leave the status at ‘very high’ and assure you that one of my colleagues will review it first thing tomorrow morning. [I]s this is OK with you[?].
The fact that the message is accepted means a number of processes are set in train (the vendor is committed to issue a correction, a work-around or an action plan within the next four hours). In this instance, however, the programmer negotiates whether the response can wait until ‘first thing tomorrow morning’, upon which the user agrees without much ado. Indeed, in the face of these formal requirements, the working lives and to some extent private lives of the vendor programmers depended on these micro negotiations regarding message priorities. To show this we highlight the case where one SoftCo programmer asks the Big Civic team to amend a message status, not because he doubts the seriousness of their system failure but because it will mean that he is able take a country-specific public holiday:
Today is holiday in [specific country] and I would ask you if [there are] these problems still.can [they] be postponed [until] tomorrow [and] to set the priority on to ‘High’?
According to protocols, the problem can be ‘postponed’ for a day but only if the Big Civic team agree. It seems their explicit agreement is not enough: the level also has to be physically changed. This is because the portal will still automatically record the amount of time a message is with SoftCo and a very high priority message left overnight will attract attention from other labs.11 Interestingly, Big Civic do not respond immediately to the request (and we do not know if this programmer was able to take a holiday or not). A few days later a vendor programmer contacts Big Civic once again to ask if they can lower the message priority since the system, as far as he can see, now appears to be up and running:
Because productive use now seems to be on http, we would like to reduce the issue’s priority.
Again, as far as we understand, Big Civic does not respond. One of the vendor programmers then telephones the Big Civic team, and because she
does not find anyone there (while it is daytime for this programmer it is well into the evening for Big Civic) she writes in the portal:
Graham is gone for the day. Not reachable...I will lower the priority. As described in the note 67739 a contact person needs to be available 24h.
She points out how Big Civic are no longer maintaining their responsibility towards very high priority messages or conforming to the conventions of the portal; and she is thus endowed, according to vendor protocols, to lower the priority level (which she does).
To summarise, this episode demonstrates how the user is at the centre of this new kind of distributed support. She is able to set the pace of the help she receives. In turn, vendor programmers find themselves having to negotiate with users about the adequacy of their proposed solutions to their problems as well as timetables. All of this is encouraged by the endowment of the user and facilitated by the portal that attempts to keep the vendor to the promises and agreements it has previously set out in service-level agreements. However, programmers also work hard to ensure that the various conventions and norms of the portal are upheld. This is not to say that programmers themselves do not enjoy a certain amount of discretion (or that users are always at the centre). We have shown, for instance, how programmers give greater attention to messages with a higher priority. This prompts the question of what happens when there are several messages with the same priority.
Programmer discretion
When there is a ‘stalemate’ in the priority levels it seems programmers apply alternative logics. One programmer discussed with the fieldworker how she would deal first with those users deemed to be the ‘most important’ to the vendor.
Importance in this sense was related to whether they were operationally significant for the release of new products and modules (if the user was a ‘reference site’ or ‘ramp-up customer’, for instance) or to the more straightforward issue of hierarchy (whether the user was a prestigious firm or from a strategically significant sector). Other factors influencing speed of response were technical complexity or timing of the problem. None of the programmers liked to receive a difficult problem just before a weekend, for instance. In addition, more individual and personal factors seemed to play a role. There was the ‘mood’ of the programmer that day, or how they ‘felt’ towards a particular user and whether they ‘liked’ them. Interestingly, in this latter respect, personal knowledge of users, and their previous message practices, appeared to have a strong influence on how and when messages were dealt with.12 These included issues such as the ‘politeness of the request’, whether they had previously said ‘thank you’ to the help received,
or whether or not they had previously left positive feedback. Added to this, some users were known to ‘exaggerate’ and ‘over-classify’ messages, or, as we have seen above, to seek help for which they were not entitled. These users were usually already known and they thus received a kind of implicit deprioritisation of their problems. For instance, on receiving a high-status message just prior to the weekend, one programmer said to another:
Ah, that’s [customer name]. They always put their messages on high importance on a Friday afternoon even though it’s not. We can leave that until Monday.
(Fieldwork diary)
In a similar manner, a user who routinely over-prioritises her messages is ignored in the hope that she will learn to modify her behaviour:
She always comes last minute. Maybe we should teach her not to do so by not helping her this time.
(Fieldwork diary)
Some messages were also ‘slowed’ in the knowledge that the user in the past had themselves been slow in providing all the necessary information (i.e. they had not maintained responsibilities towards very high priority messages):
The Indians will take forever to respond anyway.
(Fieldwork diary)
A message could be slowed in a number of ways. One common practice was to ask users for clarifications and further information about problems even when the answer was already suspected or known. Slowing worked as a strategy because while a clarification was expected from the user the message was officially designated in the portal as ‘in progress by the user’ and was thus not counted as time spent with SoftCo.
Ping-pong
We have described at length the emergence of a kind of passing regime at this vendor. Allied to this, there had developed a further more informal strategy that everyone described simply as ‘ping-pong’. Faced with the relentless arrival of messages, many of which might be complex and difficult to resolve, or from users who were known to be ‘trouble’, programmers would sometimes ping-pong messages to other parts of the organisation, knowing full well that expertise might not be found there. It was simply sufficient that their problem would become someone else’s problem. Indeed, the most difficult of messages, or troublesome of users, could be ping-ponged back and forth across continents for several weeks. Below, the Priority 1 Coordinator,
who has responsibility for managing the timeliness of messages, describes how she picked up a message that had been travelling between labs for some time:
The customer message was initially forwarded to the wrong component and subsequently went back and forth between the customer and [SoftCo] support with no ownership taken for the message. Having received no information for a month, the customer requested to close the message as ‘it was taking so long that I might as well just live with the bug as [SoftCo] is obviously not concerned in fixing their own bugs’. I picked up the message while monitoring messages for the specific market and contacted the customer directly. I listened to the customer’s frustrations, understood the situation and promised to take action. While the customer appreciated the efforts he advised - ‘you are flogging a dead horse’. I took immediate action and forwarded the message to the responsible team, informing them about the customer’s negative experience. The message was subsequently resolved in less than a day. I received the following email statement from the customer: ‘I am very satisfied. I am even tempted to start using the customer message system again.’
(Email to fieldworker)
In this case, the actions of the Priority 1 Coordinator meant the ping-ponged message finally found the appropriate expertise. Indeed, the vendor was continuously attempting to reconfigure this virtualised community through putting in place new roles, policies and practices to improve the message management process and to get closer to their (attenuated) user. We identity two projects that were implemented in relation to programmer discretion and ping-pong, and we investigate both in turn.
Raising new technologies and policies
Nudging programmers in certain directions
As we have said, when a programmer’s inbox was full of messages with the same priority there was sometimes uncertainty around who programmers should respond to first. Typically, programmers would pick through messages applying their own discretion. However, SoftCo continuously sought to nudge the message-sorting process from the human realm towards a more formal and technologically led decision-making process. For instance, a new piece of software was introduced within the portal that automatically prioritised and ranked stalemates according to a number of pre-defined criteria. The vendor wanted users to be dealt with in the order it thought fit and thus used a technology to compensate for what might be described as the ‘strategic failings’ of its employees. One support programmer describes
this system in an email to the fieldworker shortly after she had completed her ethnography:
[F]or prioritizing messages, we’re now using a new monitor that prioritizes the messages for us according to a number of factors (agreement with customers, time spent at [SoftCo], priority). This way, when you have a ton of messages, it’s now much more easier to select which one to work on.
(Email to fieldworker)
What we see here is another form of technical configuration or mediation where a further technology is inserted between the programmers and users, and which the programmers were encouraged to take into account. The new monitor will take on the role, previously enjoyed by the (apparently undisciplined) programmers, of scheduling support work. If support resources are to go to those most in need then the vendor will help the programmer make up her mind about just who that is. The result is that it is now more likely that the users will receive help in the order the vendor has previously deemed appropriate. When the support worker was asked if she thought the new technology took away her ability to schedule her work, she was clear that it did not:
I don’t feel it takes freedom away from us. The messages are prioritized but we still pick the ones we want to work on. It’s just that [SoftCo’s] prioritization method is clearly shown... I just use the tool. I just don’t question their rules.
(Email to fieldworker)
For this programmer at least, the tool was not a concern as she thought it would offer her greater clarity when choosing between problems. In other words, it equipped her with the ability to make decisions that were broadly in line with her employer’s strategy and preferences for how users should be sorted and dealt with, and she saw this as a positive thing.
From wild to regulated ping-pong
The second innovation concerned ping-pong, which, while established among programmers, was now problematised within the labs. An internal communication was released stating that ping-pong was no longer to be tolerated since it ‘significantly damaged the vendor-user relationship’ (fieldwork diary). However, ping-pong was interesting and potentially difficult to control since it closely resembled the vendor’s own strategy and means of dealing with the thousands of messages it receives. Passing and ping-pong were obviously bedfellows.
The vendor response was thus not to outlaw ping-pong - for this would surely call into question the organising principle of the support process - but
to more closely regulate it. SoftCo raised a protocol that stipulated just how and when a user might be passed. Above all, it emphasised the need for a ‘justified reason’ to be given. Such reasons might include receipt of a message which clearly concerned the module of another team or lab. In this case, it would be considered legitimate to pass the message onwards. However, before this could happen, everyone involved in what was described as the ‘passing chain’ would have to agree on the necessity of redistribution. Unsurprisingly, perhaps, and despite attempts to regulate it, (‘wild forms’ of) ping-pong were still commonplace, though they did take on a more complicated, bureaucratic form (‘regulated ping-pong’). Rather than simply reassign a message to a lab or group elsewhere, the new protocol required a programmer to demonstrate how a user problem was connected to modules other than the ones for which he or she had responsibility. Once an overlap was identified (and of course, all modern code has these kinds of interdependencies) it was then relatively easy to justify to senior colleagues and management the reason a message could be ping-ponged elsewhere.
Escalating beyond the portal
What we have just seen is that this distributed and virtualised community has a number of perverse and unexpected consequences that force the vendor to raise new technologies and policies to more effectively organise the support process. Also, at times this form of support might break down in quite dramatic fashion and users would find themselves having to escalate problems. That is, problems would overflow the narrow, restricted and virtualised world of the portal.
Escalation is the word
The setting of message priority levels was not the only way users had been endowed. If a user found she was being ping-ponged around the organisation, or she was simply unhappy with the length of time her problem was taking, she could ‘escalate’ her problem. Escalations are a major event within SoftCo and were reportedly ‘feared’ by programmers. In the five months while she was at the vendor, our fieldworker never actually saw an escalation but, as described above, her team did once have a ‘near escalation’ (which in itself was a major event). It was said to the fieldworker time and time again that ‘nobody wanted an escalation on their shoulders’ since it meant ‘the board’ would from then on be closely monitoring them - and an escalation could be damaging for a programmer’s promotion and career prospects.
When the team in which the fieldworker was placed were close to an escalation, their line manager began to give the situation more attention. After inspecting the portal statistics, she found there was a total processing time of ‘ninety days’, which was considered to be extremely long by the
standards of the lab. However, the team were able to demonstrate that the message had actually only been with them (‘in process by the vendor’) for a couple of days and the majority of the time was ‘in process by the user’ (where they had been waiting for responses to questions). In this specific case, it was relatively easy to use evidence from the portal to justify how the delay had been caused by the user rather than by them.
Escalations are unlike priority levels in that they cannot be selected in a drop-down menu. Rather, the word itself has to be spoken during a phone call or written in a message. For instance, when Big Civic found their problems were not being resolved as quickly as they hoped, they performed an escalation. It had been in the portal for several weeks now with what appeared little progress, and thus an email was sent to the vendor by one of the project managers:
We are concerned that the development period is continuing to stretch and that this is compressing the already inadequate time we have for testing, UAT and training...We must meet this target of 16 September however there are a number of concerns: a) the CRM middleware ‘show-stopper’ issue remains unresolved. This is now back from [the portal] but if we need to put it back into [the portal] we would appreciate help in escalating this.
(Email from Big Civic to SoftCo, our emphasis)
Interestingly, once a user had escalated the nature of the problem, the resolution process changed quite dramatically. It was described to the fieldworker how, following an escalation, a user organisation must then receive the ‘immediate and maximum attention of the vendor until the issue had been resolved’ (fieldwork diary). One support manager describes how seriously the vendor takes an escalation:
We don’t work on anything else. That’s what escalation means. It doesn’t necessarily mean that we have to be there at three in the morning. I haven’t done it in four, five years. But it could happen that they say, sorry, you have to work until it’s solved.
(Author interview)
When an escalation is received, the following organisational processes are enacted. The programmer responsible for the problem contacts her line manager, who then sets about establishing a ‘judging team’ to investigate the issue. The first task of the judging team, made up of senior programmers, is to accept or reject the escalation. Unlike other kinds of endowment, the ability to escalate is contingent; it is the vendor’s judgement as to whether an escalation is appropriate or not. It was explained to the fieldworker that the vendor maintained this right so as to avoid users ‘overemphasising’ their problems and obtaining valuable resources for themselves.
In this respect, SoftCo had a formal definition for when an escalation could be accepted:
...a major situation that causes a substantial negative business impact for the customer or [SoftCo]. A customer project must be escalated if the customer’s [SoftCo]-based solution has or will have: system standstill; server performance issues; functional gaps; implementation issues; operational issues; public relation issues; legal or compensation issues.
(SoftCo internal document)
Once the judging team has accepted an escalation, they then classify it as either a ‘message’ or a ‘customer’ escalation. A message escalation refers to a specific instance where the user’s systems are critically under threat, but a customer escalation is considered to be more serious since the relationship with the user itself is seen to be in jeopardy. We focus on one such customer escalation below.
Customer escalation
Frustrated with the lack of attention their system was receiving, and having already attempted to escalate the matter, Big Civic now decide to bypass the support organisation altogether and bring their problem to the attention of SoftCo’s executive management. A senior manager from Big Civic writes directly to the President of Global Public Services demanding that something more substantial should be done to rescue their system implementation, where a number of interconnected problems had begun to mount up:
I feel I must draw your attention to the message below which has caused me and my colleagues considerable concern. As you may be aware we have agreed with [SoftCo] to enhance the functionality of [Campus] to support the recruitment of postgraduate students by integration with DRM, Portal, BW [Business Warehouse] etc. This is absolutely critical in relation to the highly competitive market situation we face. There was considerable pressure from within [Big Civic] to adopt a third party product [name of small vendor], but I managed to persuade my colleagues on our Executive Board to stick with [SoftCo] on the understanding that you were fully committed to the project.[SoftCo] seems to have stumbled at the first hurdle, partly it seems from a failure to prioritise your limited higher education expertise to tackle the challenge of integration. I remain personally committed to sustain the partnership with [SoftCo] on which we both share the risks. I believe the developments we are working on together are vital if the [new SoftCo] product is to take off and create a win-win situation for us both. There are several major opportunities in other universities who are watching this project carefully and I hardly need to tell you the consequences if we are forced to take another route
because of the pressures we are under in relation to the next recruitment round. I would therefore be grateful if you could personally review the situation and let me know what remedy you can propose.
(Letter from Big Civic to SoftCo)
In the letter, the implications for the vendor/user relationship are explicitly spelt out. If more resources are not made available for the Big Civic implementation, then the organisation may look elsewhere for an alternative solution. Even though it is highly unlikely that Big Civic could or would be able to find an alternative at this late stage, the vendor takes the escalation seriously. When receiving this kind of escalation, the vendor response is markedly different from previous ones. For instance, an ‘escalation manager’ is appointed who will then ‘pay special attention’ to the user and act as the intermediary between the various actors. One vendor manager from the labs describes what happens during such an escalation:
If it is a formal escalation, it is taken care of by an escalation manager. The escalation manager will be close [to] the location where the customer is. If the customer is in US east coast, we have an escalation manager for him. The escalation manager is then going to check why they are escalating...What that means, we pay special attention to this specific customer.
(Interview)
Interestingly, while in normal problem-solving circumstances geographical location or personal relationships were not thought to be relevant, this is clearly not the case with customer escalations. Here the vendor appoints a senior employee who has some knowledge of or relationship with, as well as proximity to, the user. This character turns out to be crucial: she designs a ‘de-escalation plan’, listing the ‘top issues’ to be resolved; the various programmers working on the project will keep her up to date on progress, and she will regularly relay this information back to the user. Eventually, once the issue has been rectified, the judging team will propose closure of the escalation to the customer, to which the user must agree before a problem is de-escalated. Where agreement is not given, or the judging team does not approve of the way the escalation was handled, the de-escalation plan will continue to be adjusted until the judging team and the user are satisfied with the outcome. At the end of this process a description of the escalation top issues and the action plans, as well as user details, are stored in the vendor’s own CRM system. Below, we produce part of a letter from the escalation manager appointed to look after Big Civic. Here he is describing what actions they have taken:
As you know, Tom [President of Global Public Services] asked me to track the [SoftCo] project at [Big Civic] on his behalf, and maintain
contact with you on his behalf. I have been in regular contact with [Phil, IT manager] from your side, and with the [SoftCo] team on our side. This morning I had a conference call with Rob..., the [SoftCo] representative on the project Steering Committee. He reports that at the latest meeting, [Big Civic] agreed to the blueprint and that the project is back on track. There is now clear agreement on the project staffing, scope and objectives, and both the [Big Civic] and the [SoftCo] teams are confident that these objectives can be achieved within the timeline. This independently confirms the information I received from Rob., and from the Account Executive Bob.who is doing an excellent job coordinating the [SoftCo] efforts. At Phil’s request, we have committed the involvement of the [Campus module] expert., whose involvement has been, literally, pivotal in turning this situation around. We have also ensured the direct engagement of Simon.of [SoftCo] UK as the executive sponsor from our side. Please be assured that [SoftCo] is, at all levels of the organization, absolutely committed to the success of this project, and to [Big Civic’s] overall success.
(Letter from SoftCo to Big Civic)
This episode is interesting since it demonstrates how problems sometimes overflow the portal when users do not receive the kind of resources or attention they think their systems demand. Big Civic’s message was passed around for some time before finally finding expertise; when a fix was eventually found, the same system developed another problem (meaning it had to be sent back to the portal once more, and Big Civic staff had to go through this laborious process all over again). In the meantime, as the go-live date approached, the local team became anxious and therefore escalated their problem.
There are two points to be made about this example. First, when these kinds of overflows occur, the vendor is forced to deal not with the manageable problems and disciplined users it has spent so much time enacting but once again with unruly problems and users in the wild. Interestingly, once inside the organisation the vendor has no alternative but to handle users through reverting to territorial and Orr-type forms of support (face-to-face, personal relationships, etc.) until the problem can be rectified and the user returned to the portal. In these instances, propinquity becomes clearly an issue of proximity and close interactions.
Second, the programmers in Development Support pointed out to the fieldworker how they found that escalations mostly did not occur ‘out of the blue’. Some said they could detect ‘warning lights’ where a user was giving out strong hints that she was about to escalate. If identified, these might even be documented in internal memos within the portal, so that everyone involved was aware of how critical the problem was becoming. Some even went as far as putting a small candle-like symbol next to such messages, to signal that there was a potential escalation and that programmers should
‘take care’ when communicating with the user. However, in the Big Civic case, no such sophistication or strategies appeared to be at play and programmers did not notice or deal with the growing frustration at the user site. Thus, Big Civic responded in the only way they could by escalating their problem beyond the restricted and virtualised world of the portal and back into the main vendor organisation.
Conclusions: putting the user at the centre of the distributed organisation
The technical support and resurrection of failing organisational information systems looks to be one of those areas where there should, in principle, be close relations and face-to-face interactions between vendors and users. Enterprise-wide packages are, after all, some of the most complex technologies to be found within modern organisations. The need to configure them at local sites makes errors and eventual breakdown not only possible but also highly likely. Added to this, the failure of a packaged enterprise solution is seldom straightforward but is heavy with ‘sticky local context’, making the search for a solution and untangling the various threads of responsibilities all the more difficult. This is at least what we learn from existing sociological conceptions of technical support - most of which are informed by the groundbreaking work of Julian Orr (1996), where he puts forward a ‘situated’ and ‘territorial’ account of repair. In this view it is emphasised how support requires experts to have not only abstract knowledge of objects but also the contexts in which these objects are used. Added to this, it is common for repair technicians to ‘own’ and to become attached to particular territories, such that the relationships they develop influences the kinds of help delivered. These arguments have had much success, both within STS, Organisation Studies and beyond (Barley 1996; Pentland 1997; Star and Strauss 1999; Bechky 2006).
Yet this conception and some of its key assumptions are called into question by scholarship elsewhere (Pentland 1992) and the empirical developments reported here. In our own work, for instance, we found repair and support work not to be the situated and territorial affair it is often reported to be. Nor do support workers, it seems, require the same physical proximity to, or close physical interactions with, the sites in which technologies are used. This suggests that purely micro-sociological or ‘localist’ conceptions of technical support are inadequate in the context of commodified off-the-shelf software packages. It seems that those elements discussed in Orr-type studies are either no longer necessary or that vendors in shifting technical support online have found alternative ways to enact those features deemed essential for rescuing systems. It is perhaps the latter that offers most fruitful discussion.
At the vendor we studied, technical support has moved from a primarily face-to-face territorial form to a globalised face-to-portal one. It is an activity
that has become stretched out across geographical space and is now managed by a complex and distributed technology-enabled organisational form. Some of the features that appear to be inherently situated and territorial -Orr discusses at length, for instance, the careful work necessary to manage users and their problems, which he says are beyond technicians’ control -can clearly extend to more distributed, online environments. However, these features and this work need, of course, to be further extended to account for the various exigencies of life within the portal (cf. Knorr-Cetina and Bruegger 2002). On top of this, the actual task of support has been reconfigured such that computer system problems are no longer entangled in user settings but, through the application of various technologies and strategies, can be ‘lifted out’ from local contexts and brought back to be worked on in one of many labs across the world. This, of course, does not occur straightforwardly but is the result of processes of configuration and the introduction of various ‘triage devices’. Through this effort the many thousands of users and their ‘unruly problems’ have become highly standardised interactions that can be co-ordinated and sorted within a portal according to predefined logics.
Importantly, once in the portal, user problems can be managed in novel ways. They can be distributed and passed around the vendor’s remote sites in search of expertise and programmers who are awake and at work. Arguably, in this respect we are seeing the emergence of a new organising mode - what we describe loosely as a ‘passing regime’ - where technical support is managed through its constant distribution and redistribution. This differs from Orr-type studies, which focus on the sharing of knowledge around a support community and where technicians repair machines, through moving within this community and drawing on the combined knowledge found there, to one where the vendor effects repair through making problems mobile enough such that they can be passed around its geographically distributed sites in search of expertise. This is a substantial reconfiguration of the support process and deserves further research.
However, what is the nature of the new globalised face-to-portal support process? On the one hand, it seems that the portal, while obviously not bearing the richness of earlier forms of support, is ‘rich enough’ to allow the kinds of interactions and knowledge flows that Orr describes as essential so that the bulk of problems can be surfaced and repaired, and the relationships with users managed. On the other hand, this is not to say that technical support has become a wholly delocalised (depersonalised, asocial) activity that takes place effectively on a routine basis, or that support within the portal does not break down. This is, as we have shown, clearly not the case. Some things are obviously left behind. In addition, there are cases - either because problems are too entangled or users become frustrated with the canalisation they find - where technical support overflows the portal. Interestingly, when users do leave the portal in this way, we see the vendor return to a more territorial and location-based form of help once more.
In this respect, the portal turns out to be a quite unsophisticated mechanism to manage user problems. Thus we saw attempts by the vendor to include more ‘social’ elements within the portal and to capture increased detail on specific user locations (in the case of irate users, for instance, symbols were attached to their electronic records to warn others of their waning patience and the possibility of an escalation). On top of this, the virtualised support space gave rise to various perverse and unexpected forms of usage that meant the vendor had to continually regulate this hybrid organisational form of labs and portal. As well as passing problems around the labs, programmers could also ‘ping-pong’ difficult problems elsewhere. Thus, policies and technologies were introduced to improve but also further control relationships.
From the point of view of the user, life in the portal presented various tensions. Among other things, their relationship to the vendor was increasingly attenuated. Instead of working with specific support employees, or a fixed office, they viewed the ‘vendor through a portal’ and their problems were passed around various vendor sites to where there was expertise. Intimate (face-to-face) relationships were replaced with canalised (face-to-portal) ones. Users found their problems could, and often did, travel the world before they were finally resolved. Thus in order to compensate for possible canalisation of the relationship, to enable proximity at a distance, the vendor endowed the users with various abilities and rights as well as providing them with the means to claim them. They enacted what we have called the endowed user.
The term ‘endowment’, like that of empowerment, could easily be seen in a solely positive light (who today, for instance, would argue against the idea of the ‘empowered user’?) (cf. MacKay et al. 2000). However, we think it more useful to view these endowments as a more general change in the nature of the user-vendor relationship where conventional lines of responsibility and control have been deliberately blurred as support has moved online. Moreover, interestingly, endowing the user in this way presents quite a challenge to the vendors of generic software (and we thus see a process of social learning where the vendor had to reconfigure the relationship). Part of this difficulty was that endowment encouraged opportunistic behaviour where certain users, because they were ‘trusted’ and ‘given the benefit of the doubt’, were able to gather for themselves more resources than they might otherwise have been entitled to. Programmers suspected particular users were over-exercising their endowments. They had developed methods for handling such users (slowing or ping-ponging their messages elsewhere). In some senses, then, programmers (from one part of the organisation) were attempting to wrestle this endowment (given by another part of the organisation) back from the user.
In summary, our findings contrast with critical micro-sociological accounts that give primacy to direct interpersonal interaction and implicitly portray virtualised forms of support as lacking the various kinds of
contextual knowledge and interactions to provide technical repair. This is clearly no longer true. In addition, we have shown how certain Orr-type features have been extended to an online environment. However, this does not necessarily mean that we agree with those who are arguing that on-line interactions are abolishing distance or overcoming space-time separation. This is also not the case. Instead, we find the shift towards a portal offering round-the-clock online support from across the globe to be surprisingly successful, but also posing various new challenges for vendors and users alike.
9	Discussion and conclusions
Advancing the social analysis of technology
This book seeks to contribute to what we may describe, in parallel with Collins and Evans (2002), as the ‘third wave’ in understanding about the social dimensions and implications of technology. The domain of Science and Technology Studies (STS) emerged through a critical assessment of the initial portrayals of technologies, which dominated economic, managerial and engineering discourse, as subject to instrumental control. Under these accounts, workplace technologies were presumed, on the one hand, to have properties that could readily be assessed and subjected to rational assessment (e.g. in technical or economic terms) and, on the other, they were seen as a reliably delivering organisational improvement (i.e. as drivers of organisational change). In a competitive context, globally available technologies were expected to deliver best practice solutions around the world.
Early STS writings engaged critically with this pole of research, which was seen to be influenced by what we have described as ‘the rhetorics of technology supply’, legitimating the choices favoured by managers and engineers and promoting particular technology pathways as necessary and incontestable.
In reaction against these claims of technology to be transformatory, imperative and universalistic, scholars from STS as well as other critical social science disciplines drew attention to the ways in which the outcomes of technological change often fell short of supplier promises. They highlighted the gap between the formalised representations of organisational processes embedded in supplier offerings and the diverse circumstances of the user organisation and its complex, heterogeneous and difficult to formalise practices. They also pointed to difficulties in assessing the properties of artefacts and in subjecting technology to rational assessment and control; and highlighted the negotiation and contestation of outcomes between different groups in the organisation.
These accounts have principally emphasised the importance of local action and contingency, rather than the ‘universal logics’ represented by computer systems. In so doing they have drawn attention to the need for local
discretion by user-organisation members to repair the deficiencies of computer-based systems which remain generic in comparison to the intricacy of organisational practice. As a result, many writers in what might be broadly described as the Social Study of Information Systems (McLaughlin et al. 1999; Ciborra 2000; Walsham 2001; Carmel and Agarwal 2001; Avgerou 2002) and elsewhere have problematised the claimed effectiveness of standardised (e.g. packaged) supplier offerings, stressing instead the need for extensive customisation of computer-based systems to get them to match specific organisational practices. Some have even carried this argument further, to insist that information systems must be built around the unique exigencies of particular organisations (Hartswood et al. 2002).
Others have disputed the possibility of assessing the properties of organisational computer applications in a context of necessarily imperfect information about these complex non-material products and their fit to particular organisations’ requirements. They have pointed to the contestation of assessments between different specialist groups within the user organisation, groups which deploy diverging and often incompatible criteria - and the associated salience of organisational politics in technology decision-making (Grint and Woolgar 1997). This analytical development coincided with the semiotic turn in the social sciences. In consequence, the social analyses of technology generally and the Social Study of Information Systems more specifically have, in recent years, been dominated by accounts from constructivist or interactionist perspectives (and particularly by Actor Network Theory and Ethnomethodology). These approaches have brought a widespread interest in ethnographic studies as well as the application of micro-sociological understandings. This work has been influential as a way to produce rich pictures of local settings and the resultant technological and organisational reshaping occurring during the implementation of generic solutions.
However, as we have argued throughout the book, this body of research tends to produce somewhat ‘unbalanced’ and ‘reductive’ accounts. In much of this writing, undue emphasis is given to local actors in technology implementation and use. Reading this work, we note how attention is focused almost always on immediate action, as if various actors were creating and recreating the world from scratch. This perspective is particularly problematic when one considers that we are addressing the importance of technologies which are, inevitably, largely constructed at a considerable remove in time and geographical and social space from their use (Kallinikos 2004a, 2004b). Thus the existing critical social science work on information systems both downplays the influence of technology supply and often overlooks the influence of the broader historical setting on the unfolding of the technology. The emphasis on local, short-term studies of technology adoption is particularly inadequate for exploring the development and influence of complex organisational technologies like ERP, which, typically offered as standardised, packaged solutions and increasingly supplied internationally and across
different sectors, would appear to exhibit very different dynamics to traditional software systems supplied on a bespoke basis.
In this book, we have argued that the time has come to move beyond this intial generation of STS and Information Systems research. While the shift from the first to the second wave of analyses of the technology and society relationship, which led to the formation of Technology Studies, was characterised by a sharp ‘epistemological rupture’ (Bachelard 1934), the second shift, which we are noting here, involves a more gentle weaning of Technology Studies and allied disciplines from what we may characterise as their ‘adolescent’ preoccupations. The critical project that motivated early Technology Studies (i.e. the second wave of analysis) inspired the rejection of much existing social and economic theory. In rejecting earlier frameworks and their imputed views of the character of technologies and the operation of social structures, analysts eschewed existing theory in favour of extremely simple but flexible analytic schema (concepts such as Actor Networks and Relevant Social Groups). These schema, however, provided rather weak analytical templates that run the risk of generating reduced forms of analysis. In particular, they encourage ‘atomistic studies’ which, in emphasising immediate interaction, tend both to divorce local action from its broader context and to underplay the need for a longer-term historical view both of the factors shaping local action and of their outcomes (Williams and Edge 1996; Russell and Williams 2002a).
The second wave has as a result been characterised by a somewhat dualistic analysis, reflected in a series of debates about the respective roles of:
1	local: global
2	unique: generic
3	action: structure
4	technology decision-making processes as rational versus political.
We seek to overcome the way in which analysis has thus been dichotomised along a number of dimensions. In particular, we attempt to redress the shortcomings of those approaches which seem to be underpinned by the presumed victory of ‘the local’ over ‘the global’ and of ‘the political’ over ‘the technical’. The problem with these is that they seek to collapse a complex web of techno-social relations into a single level, either by seeing the local as constitutive of broader social relations (as in Ethnomethodology) or by rejecting existing structural explanations and the idea that broader structures and local actions may require different modes of analysis (as in Actor Network Theory - see, for example, Callon and Latour 1981; Knorr-Cetina 1981a). What is at issue here is a combined epistemological and ontological error. Thus, action-centred analyses also presume that tools of interactional analysis are able to capture all the important social relations; ignoring the possibility of influences that are not visible in particular sites of local action or recognised by actors. With determinedly naive methodologies, these
analyses fall foul of problems of empiricism that have been long-debated within social sciences (Russell and Williams 1988).
Instead, we argue for the importance of paying more effective attention to issues of methodology and analytical framework. In challenging the current emphasis on the local and immediate social relations around innovation, for instance, we suggest the need for contexted and multi-level analysis. In terms of locating micro-interactions within their broader context, we argue the need for multi-local study that looks at how local interactions are framed, nested and shaped by other settings and sites of interaction. This includes those more generalised sets of social relations that we may capture at a number of different levels as constituting broader contexts of development. To convey this multi-level mode of analysis we have drawn upon the metaphor of the zoom lens, through which we are able to move between levels. Thus, our gaze may rest centrally on a particular depth and range of focus, depending on the issues under examination, this does not involve jettisoning other levels. Researchers from particular schools of analysis tend to pick a preferred granularity of analysis. For example, ethnomethodologists tend to address short-term episodes of interaction; traditional Marxist, feminist and environmentalist critiques would be more interested to address the general characteristics of a social formation’s socio-technical ensemble (Russell and Williams 1988); evolutionary economists (such as Dosi’s [1982] analysis of the operation of the selection environment or Freeman’s analysis of long waves [Dosi et al. 1988]) tend to address longer-term trajectories and patterns in a class of artefacts, a sector or an entire economy.
In our work, we have conducted a number of detailed studies of particular episodes in order to yield a rich account of particular innovation settings and processes, and have in addition carefully selected these sites to address different moments, locales and nexuses in the development and implementation of a specific supplier’s offerings and group of artefacts. Our work has involved multiple scales of analysis, ranging from particular episodes of technology development and appropriation to studies of the nexuses where the relative performance of offerings in a technological field is comparatively assessed (and the market thereby constructed), as well as seeking longitudinal insights and longer-term reviews of the coupling between technical fields and business concepts and practices.1 By adopting multiple locales and differing scales of analysis, we aim to address both the fine structures and the broader structures at play in the emergence and evolution of technological fields, combining the respective advantages of local and larger-scale analysis, and in so doing overcoming local-global and action-structure dichotomies. We commend this mode of enquiry. The multiplicity of sites of enquiry improves the span of research (with a manageable loss of intensity of study at the single sites), increasing the number and range of locales sampled. By combining results from different modes of study (interaction-focused, longitudinal and broader-scale studies) we are able to benefit from the respective strengths (and compensate for potential weaknesses and biases) of these
different modes of enquiry. In particular, it offers the basis for an integrated understanding and a detailed account that may be required for complex and intricate phenomena such as the emergence and evolution of complex artefacts and technological fields like enterprise systems. A more diverse evidence base - in terms of locales of study and modes of analysis - arguably also provides the basis for more robust explanation.
As is evident from the preceding discussion, this concern with the diverse sites of innovation and levels of generality at which they may be addressed goes hand in hand with greater attention to historical setting and process. If innovation needs to be analysed in different settings and levels of generalisation it also must be addressed at multiple historical registers. Any particular moment may be seen as involving simultaneously the unfolding of:
1	a task;
2	an individual project;
3	an occupational project (see, for example, McLaughlin et al. 1999;
Clayton et al. 1999);
4	an organisational project (Hutchins 1995; Hyysalo 2004), and,
5	the production or reproduction of an institution.
Moreover, any particular moment or event has to be seen as the intersection of a number of historical strands. Thus, we previously considered the example of the implementation of a packaged solution like ERP which can be analysed, simultaneously as a moment in the Company Social Constitution (Clausen and Koch 1999) and as a moment in the biography of a specific artefact, a supplier’s offering or a broad class of artefact, and so on.
We have identified a number of failings of interpretation that arise, for example, where studies embrace immediate interaction and neglect historical processes. For example, short-term implementation studies emphasise the constraints to adoption of technology within particular organisational settings and overlook incremental processes of restructuring as technologies become embedded in organisational practices over time. These are examples of the ways in which particular disciplines and schools of analysis have generated partial accounts (in both senses of the word ‘partial’) that address only a small fragment of the complex and interconnected relationships that constitute real-world phenomena, by means of studies that remain framed around and restricted to selective arrays of actors and settings, timeframes and issues.
We have sought to acquire historical insights and combine them with the strengths of contemporaneous study in this project by a distinctive and carefully considered research design:
1	First, our current research project involved longitudinal ethnographic research (though the resource model of current externally funded research projects typically involves relatively short timeframes - rarely exceeding two to three years).
2	Second, we developed a research design around the cyclical processes we have identified in relation to the development and implementation of packaged software, whereby we could address different phases and moments of the biography of an artefact.
3	Finally, and somewhat exceptionally, we were able to build upon earlier research, giving us a base of contemporaneous enquiry spanning two decades.
In this study, we sought to move away from naive methodologies, as exemplified by the ‘flat ethnography’ of a single site of technology production or consumption, or simple methodological nostrums such as that of ‘following the actor’ (Latour 1987). Instead, we have pursued a theoretically informed selection of different sites and moments for study. For example, the long timescales of technology development make it very difficult, in the context of typical durations of social science investigations of a few years at maximum, to address in contemporary work the technology development process for a complex product like ERP, let alone the complete cycle of technology development, implementation and use (Williams et al. 2005). We were able to overcome this constraint in this project by conducting a carefully selected array of contemporary studies within a comparative frame of a number of selected locales. This was, first, the development of emerging packaged software products compared to more mature ones, and in addition, second, by case studies that examined the different character of linkages between technology supplier and user in relation to the different phases and locales in the biography of the artefact.
The Biography of Artefacts Framework developed here in the book provides analytical tools for tracking the trajectory of a group of artefacts and associated practices over time and within a broader setting. We note the possibility of applying this frame to: the development of particular vendor systems; the broader class of ERP technology; and the technological field and a societal practice connected to the software package. Moreover these mappings can be seen as particular instances or viewpoints within a broader space of competing conceptions and practices which Kaniadakis (2006) has described as the Agora of Technical and Organisational Change. The biography framework thus provides an analytical template that we have developed and refined for understanding the historical evolution of workplace technologies. Our enquiry has encompassed Manufacturing Resource Planning/ Computer-Aided Production Management in the 1990s, and Enterprise Resource Planning and Customer Relationship Management systems today.
To carry out this work we have undertaken detailed empirical studies of linkages between technology supplier(s) and user organisation(s). Our array of studies has been designed around existing understandings of how these are coupled together and how they shape development. We have sought to examine similarities and differences in various moments within the product
lifecycle, for example, between the design and development of packaged solutions, their procurement, their implementation and post-implementation support. The biography perspective provides us with analytical cues as to the important sites and settings for investigation, encompassing the broader context as well as immediate sites of interaction, helping us to achieve an effective research design. This work, for example, sensitises us to the importance of various intermediaries that link technology supply and use, spanning locales and levels (Williams et al. 2005; Howells 2006). This in turn helped us identify an important new phenomenon - the role of industry analysts such as Gartner in technology procurement and in the operation and shaping of technology markets.
What is at issue here is also our orientation to theory. We do not see social science theory as something that should or indeed can be applied as a fixed analytical grid: a set of predetermined analytical categories encompassing pre-ordained relations with such predictable effects that theoretical generalisation can serve as an alternative to empirical research and interpretation. And this is only a slight caricature of how the use of existing social science theory was portrayed in early Actor Network Theory and constructivist critiques of structural explanations (Callon and Latour 1981; Latour 1988). Instead, we propose a more modest and arguably more ‘scientific’ understanding of the role of theory: as a set of generalisations that have been shown to be valid under certain circumstances (supported by a certain body of evidence arising under particular empirical and methodological conditions), and that can be used for further and more detailed exploration. In other words, we do not go into the analysis like newborn children, somehow innocent of theory and able to derive the world from naturalistic observation (as ANT seems to claim). However, nor do we approach the world through narrow theoretical blinkers. We can hold some prior knowledge as a provisional account of the world we are investigating: a knowledge-base that is certainly incomplete and that may prove inappropriate to the matter under examination. This, however, is just the starting point for empirical exploration, to test and refine the analytical schema in relation to particular empirical settings, selected and approached according to our theoretical understandings and analytic concerns.
Conceptualising the biography of software packages
In advancing the concept of biography, we are seeking to examine the evolution of an artefact in its historical context and across its lifecycle. We find that a variety of disciplines have applied ‘lifecycle’ concepts to analysing particular moments and aspects of technological change. Let us briefly examine these. It should be noted that the terminologies and usages of these various lifecycle perspectives, crafted for particular purposes, are not always consistent within disciplines, let alone between them, and vary in important ways. However, there is not space here for a detailed analysis of such
conceptualisation (though this is certainly worthy of further examination). The scope of this study encompasses a number of these (differently conceived, overlapping) lifecycles:
1	what from an Information Systems perspective is called the software lifecycle (or the Systems Development lifecycle) encompassing the selection, design, implementation and use of a particular application;
2	across the multiple systems development cycles that constitute what Business and Innovation Studies describe in relation to a particular vendor in terms of its product lifecycle management strategy; and
3	at the level of an industrial sector or application domain, the product lifecycle across the multiple product cycles involved in the evolution of a technology (which Abernathy and Utterback [1978] also characterised as the technology lifecycle).
Thus, various disciplines and schools of analysis have developed their own lifecycle concepts to analyse certain (temporal and institutional) settings for tracking these different cycles. These conceptualisations have focused upon the dynamics of particular parts of this complex ongoing set of processes, depending upon their concerns and analytic framework and the segment of socio-technical practice that each discipline foregrounds, often at the neglect of others. Our concerns, however, are more integrative. Our concept of biographies seeks to characterise these multi-local spaces and multiple timeframes within a broader understanding of the evolution of technical fields. We can, for example, track the evolution and shaping of packaged software at three levels (following Hyysalo 2004) as:
1	the specific: the development of a particular innovation (and the organisations and people connected with it);
2	the generic: the development of a class of technology;
3	the institutional: the coupling of a technological field and a societal practice.
Thus, we can gain insights from addressing these cyclical processes at multiple levels: across the software cycle and across multiple product cycles (in relation to both a vendor’s offerings and the evolution of the technical field more generally). We shall examine each in turn.
Across the software lifecycle
A concern with the software lifecycle can help us more fully understand the multiplicity of (temporally and socially) dispersed connections between diverse players in technology supply, implementation and use. In particular, it can help redress the ‘design bias’ that has beset both Information Systems perspectives and social scientific enquiry (Stewart and Williams 2005). These have traditionally placed system design and development at centre-stage.
More recently, social science has extended its gaze to include the implementation of systems within user organisations. However, the focus has been uneven and other moments and functions in the software lifecycle have received much less attention. Within the book, we have focused particularly on (1) design and development, (2) procurement, and (3) implementation and post-implementation support. Let us look at each of the phases in turn.
Design and development
The commodification of packaged software supply, in contrast to the traditional model of bespoke development, is often portrayed in Information Systems accounts as divorcing technology design, implementation and postimplementation support from specific customers. Various writers deploy the language of choice in describing the apparent autonomy of suppliers in relation to an impersonal market. Regnell et al. (2001: 51) talk about requirements being ‘invented for packaged software offered ‘off-the-shelf” to a mass market’ while Howcroft and Light (2006) describe the vendors’ ability to ‘decide’ which functions would be built into their products, depending on whether enough customers ask for them, as evidence of their ‘technical exercise of power’. However, our study shows that the suppliers of ERP and of other packaged organisational technologies developed sophisticated strategies that linked them and the community of current and intending adopters.
As we have shown, software package design is marked by a number of key but shifting moments. These include the search for ‘pilot sites’ upon which new modules can be based. Many of these sites participate on the assumption that they can influence the design of a generic package - and indeed, many appear to find success in wielding influence (Chapter 4). However, whereas in the early development of a packaged solution vendors could be rather flexible in taking on board requirements of new customers, something we described as ‘accumulative functionality’, quite rapidly a more selective and managed approach to user requirements emerges (Chapter 5). Package vendors have developed sophisticated strategies to create and further develop generic offerings by actively managing their relationships with their customers. They typically do this through shifting design to community forums and by actively differentiating between users, so that their response is segmented according to the strategic and commercial importance of particular pilot sites. Thus, user requests for new functionality would almost always include an assessment of the other potential users who might require it.
Procurement
We analyse procurement in terms of the configuration of two interrelated aspects: (1) the translation terrain and (2) the technology field.
Configuration of the ‘translation terrain’: the players directly involved in the procurement decision
Procurement represents something of a challenge for would-be organisational adopters. These large organisational software systems, while extremely expensive and of great strategic importance to the organisation, are as we have seen extremely complex and their properties and match with the requirements of the potential adopter hard to assess. The adoption of a package like ERP represents a reworking, if not complete replacement, of the organisation’s information infrastructure. These substantial and often business-critical decisions about what may be major strategic investments are carried out infrequently by user organisations, which thus often lack the expertise and experience needed for effective decision-making. Many user organisations therefore find it beneficial, or feel compelled, to deploy third party players such as expert consultants or system implementers. These groups on the face of it appear better placed to acquire and deploy the relevant knowledge about, for example, available products and the implementation process.2 As a result, such third party players often mediate between adopter and supplier(s). Though notionally making the selection and implementation process more straightforward, in practice the resort to external expertise tends to makes the vendor-adopter relationship even more complex (Chapter 6). The micro-politics of the decision-making process thereby takes place on a complex inter-organisational as well as intra-organisational stage. The resort to external expertise to reduce uncertainty felt by the organisational adopter about how to develop and implement its own strategy creates, in turn, new kinds of uncertainty (for example, about how to select the right consultant [Hislop 2002]).
Configuration of the broader context constituting the technology field
Packaged organisational technologies are inevitably selected under circumstances of (what transactional economists have characterised as) ‘imperfect information’. This arises from a wide range of interlocking factors. Software packages are highly complex and non-material products, meaning that their properties are not directly ascertainable. The value of a package to a particular user organisation depends upon the ‘fit’ that can be constructed with existing organisational practices. Therefore, a substantial investment must be made in implementing and adapting the package and incorporating it within organisational practices before the performance and ‘fit’ of the package with the adopting organisation, let alone its organisational outcomes, can be directly assessed. Moreover, assessing the ‘fit’ with particular user organisation structure, practices and requirements can be particularly tricky since user requirements are poorly articulated; they are in many respects ‘negotiable’ and change in the course of adoption and use as the artefact’s affordances and constraints interact with evolving organisational practices. These
features frustrate the conventional mechanisms by which purchasers might seek to scrutinise the properties of a material product. Moreover, as we saw in Chapter 6, what the adopter must assess is not just technical properties but intangible issues regarding the future performance of the vendor (will it survive?) and its behaviour (will it still operate and invest in developments in this particular market in coming years?).
Potential adopters are forced to use proxy indicators of these intangibles, such as the reputation of the vendor and their products. Vendors mobilise proxies, such as reference sites, where their products can be seen in everyday use, to convey the utility of their offerings. Gluckler and Armbruster (2003) have noted the trade-offs between different kinds of reputational evidence along a spectrum between direct local knowledge - of ascertainable provenance built upon a limited experiential base - and public reputation based on community knowledge which builds upon a broader knowledge base, the reliability of which is less readily judged. Moreover, since community knowledge is slow to diffuse, public reputation may be of limited use in rapidly changing contexts of innovation. They highlight the importance of networks in providing a modicum of timely information based on a broad base (networked reputation). Such inter-organisational networks have been seen as critical across many areas of innovation, including software procurement (Fincham et al. 1994). Networked reputation may be difficult to acquire, however. Taking this argument further, our study has drawn attention to a phenomenon that has received little academic attention but which is growing in importance: the commodification of networked reputation through the efforts of industry analysts which act as repositories for knowledge across the vendor and user communities and supply this community knowledge back to them on a commodified basis.
Gartner’s efforts become the focus of attention of vendors and adopters alike. They are primus ceterus paribus among the industry analysts in the ERP and related domains, and have been particularly influential in mobilising belief and expectations among both supplier and user communities. Their assessments, as we saw (notably in the Magic Quadrant), helped user organisations to make comparisons between the proliferation of supplier offerings and also helped vendors assess their own position within the sector. In the case of ERP, Gartner went beyond this, initially coining the term ‘ERP’ and mapping out visions of how this and related segments of the software market would develop. However, Gartner’s assessments are also an interesting space. On the one hand, they are viewed sceptically on the grounds that the industry analysts are not independent of those they assess. On the other, they seek to make their knowledge and processes open to certain kinds of accountability. Thus, while they have some features of being a ‘public good’, participants still have a sceptical orientation to Gartner’s assessments. Interestingly, the latter are contested but, as we have seen, are influential in practice.
An analysis of the shaping of ERP, even at the level of the single software lifecycle, needs to address the broader web of relations that constitute the broader context, which Koch (2003, 2005) has characterised as the ERP community. We, however, seek to enquire how this community is established and constituted. This is, first, to draw attention to the internal constitution of this community (i.e. how it is made up from various other specific arenas and how it develops through distinct sets of relationships) and, second, to examine the external constitution of this community though the operation of what Kaniadakis (2006) described as the broader agora of technology and work organisation. The ‘agora’ refers to the broad space in which ideas are circulated both about good industrial practice as well as about how this can be achieved through new technologies. This is where technological fields come to be constituted, and certain concepts achieve wide currency, in a process catalysed through the activities of certain key players - in the case of ERP, notably vendors, consultants and industry analysts - but also ultimately sustained by the activities of wider communities of organisational users and others. These concepts and broader visions provide crucial resources within which vendors and management and technology consultants can articulate their offerings.
The way in which the technological field is constituted is particularly relevant to the social study of ERP packages. This class of artefacts was itself reconstituted when the field of ERP emerged from its roots in a predecessor technology, MRP II, and again as it has absorbed other fields (currently through the convergence of ERP and CRM systems). The emerging consensus, a consensus shepherded by industry analysts and others, orients the development activities of vendors as the class of artefacts evolves and also enables potential adopters to consider the development of their information systems and to assess the range of relevant offerings.
Implementation and post-implementation support
The supply of standard packaged organisational technologies poses challenges for vendors in providing implementation and post-implementation support for differently implemented products, particularly as these become long-lived components and often the backbone, of a customer’s broader information infrastructure. Organisational information infrastructures are configurational technologies (Fleck 1988a) built from a combination of standard and custom technology components from different suppliers, selected and adapted to the user’s context and purposes (even in the case of ERP implementations in which some of the components are very large). System failures and bugs may arise through unpredicted interactions between these components, which may be variously combined and configured together in different states. The idea of modular design to limit unintended interactions and interference is a central aspect of computer systems design. However, the proliferation of technology components that may be combined
into inter-operating systems, with the fact that these come in multiple versions that themselves are being constantly updated and revised, that they may be configured in different ways, contributes to incalculably large numbers of variations in any system’s operation. As a result, it may be very difficult to diagnose the reasons for any failures.
Vendors are forced to develop increasingly novel strategies to manage these challenges. Thus, as we saw in Chapter 8, vendor contracts and their customer policies explicitly seek to police which parts of the package can be changed without prejudicing the licence and post-implementation support, as a way to regulate the way a package may be adapted in implementation (and in particular to protect the dependable operation of the core functionality of the package). We saw how a major package supplier sought to tackle the challenges involved in supporting its many thousands of customers which were geographically distributed around the world. The supplier was facing contradictory pressures. It sought, on the one hand, to rationalise system support in order to economise and gain more control over the support process, and, on the other, to give clients more rapid access to the most appropriate technical expertise. The supplier sought to address these by developing a organisational and technical solution in the shape of a complex portal allowing customers to access a network of global laboratories operating around the clock. However, as we saw, this generated a new organisational problem where the supplier sometimes struggled to coordinate and regulate its response to users. Since it had previously undermined ‘territorial’ links between customers and specific support offices, this resulted in the unforeseen problem whereby tricky customer problems could end up being ‘passed’ from one lab to the next, with no one taking responsibility for resolving them. The supplier, in turn, sought to avoid such eventualities by trying to involve customers, endowing them with rights and responsibilities (and in particular the right to escalate a problem that had not been resolved). Where this right was invoked, the customer in the exchange was given the responsibility of following certain vendor procedures.
An unusual feature of this particular study, which addressed different sites and moments in the biography of packaged software, is that we were able to look at these from both the customer and the supplier view. This was of enormous value in helping undermine views that have been articulated based on one-sided accounts: for example, the portrayal of vendors, centring around frustrations that might often be expressed in user organisations, as insensitive to customer needs. Au contraire, our research found that addressing (and importantly being seen to address) user requirements was a priority for this supplier; particularly as it tried to move into a new market in which it lacked reputation and experience.
To summarise this section, discussions among those interested in the Social Study of Information Systems concerning how the development of packaged software might differ from conventional bespoke solutions have
tended to convey the idea that package development is divorced from its users, compared to the direct links available for bespoke development. There has been, for instance, emphasis on the autonomy of designers in orienting their efforts towards an abstract market of potential adopters (Salzman and Rosenthal 1994; Sawyer 2000, 2001). Our empirical research, informed by our lifecycle perspective, has in contrast highlighted the intricacy of the relations between vendor and current and potential adopters. We saw instead a wide range of relationships linking the vendor with user organisations through, for example, adopter clubs, support desks, etc., as well as the joint effort in package implementation. These relationships are carefully managed and, as we saw, were sorted by the vendor in terms of the customer’s strategic importance. Our observation that these linkages continued throughout the software lifecycle, but differed between different moments in that cycle, confirmed the importance of a lifecycle-based perspective. The system of linkages we have uncovered is far more intricate and elaborate than existing academic literature presumes.
Moreover, these direct and indirect linkages allowed the exchange of information between vendor and adopters - and here we find what Evolutionary Economics would characterise as a strong learning economy (Lundvall 1985, 1992). However, the relationships between the vendor and user organisations are far from transparent, in the way economists seem to presume, but are canalised. The development and distribution, on a generic mass-market basis, of an extraordinarily complex organisational technology, such as ERP, imposes potentially huge challenges for vendors to manage their relationships with an ever-growing number of customers. The knowledge management and coordination costs of knowing about, and to some degree catering for, this diverse customer base potentially undermines their business model of standardised commodity supply which promises high profits through the sharing of development costs of standard components among a large number of customers. Vendors must therefore carefully manage linkages with customers and keep the cost of such linkages under control. Economising on that knowledge of customers becomes a crucial part of the business plan for generic software supply. Thus, it is channelled through an array of pathways that differ in terms of what information was exchanged and how readily (quickly, completely) it was communicated. So, even in this well-established communication context, we are struck by how uneven, incomplete and imperfect the channels of communication are between vendor and current/potential users.
We also saw how the vendor regulated the level of customer variety it would deal with and prevented users from altering elements that would impinge on the core systems. Likewise, the vendor also sought to retain a certain independence, and indeed distance, from its existing customer base to enhance its manoeuvrability in a rapidly changing market - reflected in the gap it maintained between existing product support and new product development.
The biographies perspective has helped us address this multiplicity of local linkages that surround the software lifecycle (the development, procurement and implementation and post-implementation support of packaged solutions). This work has also flagged the operation of broader structures and trends that are sustained across multiple product lifecycles, and it is to this we now turn.
Across multiple product cycles
We have undertaken a longitudinal examination of the challenges surrounding the supply of packaged organisational solutions and how these differ across multiple implementations within product cycles and between a number of product cycles. We achieved this not through a conventional longitudinal study (which would need to be extremely protracted, given that the timeframes involved are very long compared to typical research project durations) but through studies of a selected array of vendors and their products at different stages in the evolution of a product market, including the birth of a vendor offering (in a young supplier) and the move of a major supplier of a mature offering (into a market to which it was new). We were also able to explore the evolution of a product market/broader technological field (by revisiting findings from a study undertaken nearly two decades earlier). We identified common patterns in the development of packaged solutions. We saw, for example, how in the early stages of the development of their products, vendors embarked upon extensive growth. They took on board a wide variety of user requirements, as doing so meant systems could be provided to additional customers. This phase of extending functionality and growing diversification later gave way to a more controlled phase of generification as vendors were forced to tackle the knowledge management problems of an ever larger and more diverse customer base.
We saw how in developing a new product, for instance, vendors actively sorted customers (sifting, prioritising and aligning their requirements). The final stage in the vendor’s product cycle was characterised by the stabilisation of their product. Here we saw how successful vendors, having achieved increasing market penetration within their established market niches, then sought to extend or recycle their product offerings to other market segments with more or less similar technical and operational contexts. Another strategy to ensure continued growth was to expand the scope of their offerings to include additional functionality. To the extent that the additional functionality was a feature of other established technologies, this would bring the vendors into competition with established suppliers for these products (as we see today with the extension of ERP to include CRM).3 Thus, success of these systems was achieved through this careful recycling of the same (or slightly modified or even enhanced) system from sector to sector, rather than through the notion of ‘steamrollering’ (Koch 2001), to use one of the metaphors sometimes deployed.
Theorising relationships at multiple levels and multiple timescales
Multiple levels
How, then, may we theorise these complex arrays of relationships, both localised and aggregate, and the ways they unfold over different timescales? In addressing a new technology, for instance, studies might usefully focus upon varying scales. This could be upon particular levels, sets of spaces, as well as different temporal framings of analysis. Some common types of study include:
Implementation studies: this might be addressed through micro-level studies of particular work groups, or perhaps more effectively through studies of particular ‘implementation arenas’ (Fleck 1988b), which are hybrid spaces linking together players directly involved in a particular implementation. Such a study might include what we could call ‘foregrounded players’ like managers and different functional divisions of the adopter organisation, as well as the technology vendor(s) and various third party consultants. Also present could be a number of ‘background players’, such as industry analysts, policy-makers and perhaps competing suppliers, some of whom may have been initially foregrounded but, as the selection progresses, become ‘backgrounded’.
Studying the biography of a particular innovation or artefact: though this might be addressed through micro-level study (such as following a particular short-term episode in a design lab), more effective explanation might call for meso-level study addressing in addition the broader arena encompassing a vendor and its user relations. By this, we mean the interlocking array of actors that could be traced around a particular vendor, its customer relations and competitors and, perhaps, further considering how, in the background, technological fields are being reconstituted in different ways.
Studying the biography of a technological field: here a broader macro-level focus might address the multiple arrays of arenas around particular innovation/ artefact biographies constituting a particular field of technological practice, along with players operating primarily at this inter-organisational level (analysts, policy-makers, etc.). Empirical study might ‘foreground’ the latter, as well as vendors with strategic influence, but behind these would also be the decisive role of collective adopter decisions as well as broader sets of ideas about good industrial practice and the contribution of technology to achieving this.
What is clear from this brief discussion is that any exclusive framing around a particular setting or level of analysis would produce a partial and incomplete account.
When we focus on one locale or moment, we also find that important influences (actors and factors) from other levels and moments are being ignored. Indeed, micro-, meso- and macro-level phenomena are relevant, albeit with different levels of influence, in all three kinds of study. For example in our study of the procurement processes within adopting organisations, the primary empirical focus was upon the immediate level of organisational actors involved in the procurement decision, the expert intermediaries they had enlisted to assist them with procurement, and the supplier (and its competitors and collaborators), all of whom together constituted the immediate network of players directly engaged. However, out beyond these, we found the combined actions of players who constituted ‘the market’ of technology artefacts, the field of technological practice, and also intermediaries who channelled information through these collective spaces. In our study of ERP and its predecessors, these were exemplified by industry analysts and by professional organisations.
Particular schools of social scientific analysis will, however, often be associated with characteristic temporal and spatial framings of a phenomenon -motivated by the things each seeks to explain and its view of the aetiology of phenomena. To the extent that these framings focus primarily or exclusively on particular moments and settings, they can skew the conduct of research and its findings. Research and explanation does, however, need to make boundaries and distinctions, for practical reasons as well as reasons of theoretical elegance; some simplification is needed to produce manageable generalisation. And extending the scope of empirical enquiry simultaneously to an ever-increasing number of sites becomes potentially unmanageable. If we are seeking to produce integrative accounts that pay attention to multiple levels and timeframes of influence, we need tools to guide a selective, multilevel focus.
Actor Network Theory helpfully problematised rigid theorisations concerning the prioritisation of particular locales and settings (i.e. ‘localisation arguments’) (Knorr-Cetina 1981b; Callon and Latour 1981). Yet once having done this, it provides rather few cues as to how these different scales should be addressed. Indeed, ANT’s early attack on explanations in terms of social structure, and the social scientific theories that described the operation of these structures, have been unhelpful. In contrast, we propose a method of looking more systematically at the range of interlocking contexts in which socio-technical phenomena emerge and evolve. We introduce existing ideas about the significance of these different factors not as the finalisation of enquiry, as ANT’s critique of structuralist explanations presume, but as the beginning of enquiry. In this case, our work on procurement flagged the growing importance of a new kind of actor - industry analysts, with their role in providing access to community knowledge in procurement and their influence over the evolution of technology fields. Our multi-level framework, which shaped our research design, enabled us to tackle complex empirical phenomena across different locales, timeframes and scales. Without such an
analytical schema and the research design that it informed, these kinds of interactions might readily have gone unnoticed.
We have drawn upon Kaniadakis’s (2006) concept of the agora to highlight the extended socio-technical space in which developments unfold as well as the possibility this framework suggests of researchers taking different analytical slices, from different viewpoints and for differing analytical purposes, through this space. We thus emphasise the integrity of this socio-technical space: the agora is an extensive, seamless web of social (or rather, socio-technical) relations over time; there are no walls or gullies that allow what is ‘outside’ to be reliably fenced out/factored out of the analytic picture. The relations within the agora do not necessarily correspond to the relatively tightly knit arrays of actors that might tacitly be conveyed by established STS concepts such as community, arena or network (though parts of the agora may be constituted by these kinds of explicit and stronger linkages -including, for example, very strong contractual relationships between vendor and purchaser). Nor is there the sense that may be inferred by ‘network’ or ‘system’ concepts, of shared commitments or incentives. Elsewhere we have used the term ‘constellations’ in preference to other more established conceptual metaphors to convey the gaps as well as the relationships between groups of actors (Williams et al. 2005).
The agora, then, is a system characterised by differences in societal insertion, roles and incentives, knowledge, commitments and views. Here we draw attention not only to the heterogeneity of players but also to the intricate and heterogeneous pattern of linkages that exist between these players. Consider, for example, the varied kinds of linkage between packaged software vendor and its customers over different phases in the software lifecycle. The agora is, moreover, a markedly amorphous space, subject to imperfect alignments and competing pressures (and we have already noted the possible misalignment between business and technological circumstances, practices and cultures between developing and developed countries). Given this heterogeneity of linkages, we may need different analytical tools to capture the many different kinds of relation across the agora. This stands somewhat in contrast to prevalent STS concepts, which have tended to apply a uniform template (in terms, for example, of ‘system-building’ or ‘network-building’).
Differing viewpoints upon an agora may be drawn, depending upon the purposes and location of the analyst (or, for that matter, the actor) viewing it. Depending upon these viewpoints, different parts of the agora will appear central and peripheral and, to (perhaps over-)use our zoom lens analogy, the focal plane may vary temporally (between short-term and long-term analyses) and spatially (between local and larger-scale analyses).
The biography of a technology in this connection represents a particular ‘cut’ through the agora, foregrounding the complex sets of relationships surrounding a selected artefact or class of artefacts at particular historical moments, and addressed, depending upon the purposes of the analyst, at different levels of generality and timescales. Other analytical slices through
socio-technical space are possible for other purposes. In particular we would flag the concept of the Company Social Constitution (Clausen and Koch 1999), which pursues a rather similar analytical challenge to our own, but places at centre-stage the social (or more precisely socio-technical) relations surrounding the evolution of a particular firm.
Multiple timescales
These different framings in terms of level and focus of analysis are (partially) coupled with different timeframes of analysis. Social scientific enquiry encompasses different timeframes of action/interaction, ranging from those of interpersonal interaction, which may occupy split seconds, to the changes in the broad institutional level that may be measured up in periods of several decades. We take the argument about the need for multi-level, contexted analysis further in relation to temporal framings, to argue that multiple histories and timeframes intersect in any episode that we may wish to analyse. Hutchins (1995) reminds us that any action by an actor is, at the same time, the working through and completion of a specific job, the development of the actor who is conducting the task, the progression of the particular profession or community, and the development of the wider institutional practice (Hyysalo 2004).
Historical reconstructions
We argue for the importance of an historical perspective if we are to understand the evolution and implications of new technologies. However, our study flags some challenges that surround the goal of historical analysis. In this project, we were able to map out the dynamics of innovation at different stages in the biography of packaged software by conducting a set of contemporary studies of organisational solutions at different stages of maturity. This included the development of an early product that was evolving into a generic solution, and a mature product that, having been widely implemented in manufacturing, was being adapted for a new public service setting.
Unusually we were also able to return to the 1987-91 study of CAPM conducted by Fleck, Webster and Williams. Here we were able to gain particular insights by re-examining with the benefit of hindsight the findings of this study. Fieldwork for this study (which encompassed technology supply and implementation) was completed on the eve of the emergence of contemporary ERP and the widespread resort to packaged solutions. In 1990, however, as mentioned at the outset of the book, there was a strong consensus among virtually all academic experts and industrial commentators, and one which our own earlier studies reinforced (Webster and Williams 1993), that the way forward for this technology (known then as MRP II and CAPM) was through two interrelated developments: first, the more effective local adaptation of packaged solutions to the requirements of particular user
organisations, and second, that realisation that the future for technology supply would lie with vendors developing sector-specific (or ‘semi-generic’) offerings (Waterlow 1990). Interestingly, the extraordinary (and imminent) success of suppliers of standard ERP solutions was just not on the radar, even though SAP was then already at an advanced stage in developing its R/ 3 package (which was launched in 1992, and its widespread uptake was heralded as part of the ERP ‘revolution’). There was a vision of the future in 1990 but it was far removed from what subsequently transpired. This seems to us particularly significant.
First, we contrast the lack of effective foresight (then) with the extremely well-rehearsed historical accounts of the emergence of MRP II and CAPM. MRP II in 1990 had its own well-established history in MRP and earlier stock control systems. That retrospective history has been updated, and we find widely circulated accounts of the self-evident historical evolution of ERP. Today, MRP/ERP has an extremely well-rehearsed history. Writing (and re-writing) the history of a technology serves as part of projecting the future for technology, portrayed as a self-evident process of improvement. Czarniawska has drawn attention to this asymmetry of history, noting that, at the time, the significance of what is later seen to be crucial often goes unrecognised: ‘it is the ending that chooses its beginning, not the other way around’ (Czarniawska 2004: 774). She further discusses the ‘transformation of events into a story (history)’ (ibid.: 775), contrasting the carefully ordered retrospective accounts of historical narratives with the confusion of current observation in which the future significance of particular events and developments is obscure. In the processes of what she describes as the shift from chronological history - that is, accounts of raw contemporary records - to the kairotic history of organised narratives, accounts have been reorganised and the noise and uncertainties of everyday life have been removed.
Second, this observation makes us critical of a tendency within current discussions within the Sociology of Expectations, for instance, to portray technology as emerging as a simple product of expectation, as some kind of ‘self-fulfilling prophecy’ (see, for example, Brown and Michael [2003] or Rip [2006], discussed in Chapter 7). The 1990s debate that we introduced at the very outset of the book about what would happen ‘Beyond MRP II’ demonstrates that the actual path that was soon to become hegemonic was not anticipated by many of players most immediately involved, just on the verge of the ERP explosion. In this case, the prophecy did not materialise. Scholars in the Sociology of Expectations need to ask why some kinds of consensus or compelling vision come into being, and others do not (a question we are attempting to explore in current work - Pollock and Williams [2007]). Of course, one cannot presume stable trajectories and the continuation of existing sets of expectations (Fleck et al. 1990). As Jorgensen and Sorensen (1999) point out, even where apparently stable sets of beliefs are shared by those perceived as the relevant actors, one cannot rule out the
entry of other actors and factors into the arena. Moreover, the fulfilment of technological promise is not just a matter of belief - something that occurs solely in the ‘heads of actors’ (see MacKenzie [2006a]) - it is also crucially also about materiality. Our Social Learning framework perspective reminds us that the achievement of a new technology takes place in an extremely heterogeneous landscape, involving a diverse and unevenly malleable array of human and material elements. There is every opportunity for beliefs to be challenged and reworked in the arduous process of creating artefacts and making adequate linkages with organisational practices.
In our study we were also able to review competing predictions for the further development of ERP in the twenty-first century, where there has been a dialogue between transformatory visions (for example, Gartner’s year 2000 prediction of the death of ERP and its replacement by ERP II, based on supply-chain integration) and the more incremental evolution of ERP offerings. These kinds of consideration underpin our insistence on the need for STS as well as the Social Study of Information Systems to have a complex analytical register to begin to do justice to the complex intertwining of technical and social opportunities and constraints.
The process of technological innovation
Going beyond the technicallpolitical dichotomy
Another way in which we seek to promote a third wave in thinking in STS concerns its treatment of the relationship between ‘the technical’ and the social and political. We have argued that STS emerged from a critique of the boundary that the promoters and developers of particular options sought to construct around the technical - and the attendant separation between the technical and the political. Indeed, the critical project for STS was to examine who was trying to construct this boundary and for what purposes. This was a boundary that would privilege certain players (like technical specialists, vendors and promoters) and naturalise certain outcomes (MacKenzie 1990). One unhelpful consequence of this important endeavour has been a certain dichotomisation in many analyses between ‘the technical’ and ‘the political’. Some accounts tend to see politics as being in command and portray attempts at rational decision-making as some kind of mystification (see, for example, Feenberg 1995; Stearney 1996). While not wishing to reinstate conceptions about the neutrality of technical assessment criteria, we feel that this position fails to distinguish between different settings: between those cases in which politically contested or motivated assessments are more salient from others in which more systematic, technified assessment criteria are being applied. This can lead to an over-politicised account of technical decision-making. We suggest that it is now time for STS and the Social Study of Information Systems to come of age and go beyond its founding
concerns.
In this project, we have sought to give proper attention to the process of assessment of technologies (in the procurement of packaged software solutions) in a way that bypasses the dichotomy between political and rational-technical accounts. Inspired by recent work on the performativity of Economics, we have attempted a detailed analysis, in Chapter 6, of how assessment was achieved of various vendors and their products through the construction of ‘measures of comparability’ in the course of reaching a decision. Their deployment constituted a kind of ‘scaffold’ through which decision-making could take place.
The technology selection processes we explored took place on a complex terrain, involving third parties (the user organisation’s Joint Venture partner and the industry analyst Gartner) as well as would-be adopter and suppliers. We saw how the selection process was characterised by elements of ritual and drama as the specific adopting organisation sought to make public demonstration to their partner of the reasons for rejecting the latter’s preferred choice. (We also observed that European procurement legislation impinged upon the behaviour of the members of the selection team.) We also noted the complex forms of accountability and negotiation that surrounded Gartner’s assessment of the standing and prospects of different suppliers:
1	in Chapter 6 we saw how the implications of Gartner’s assessment (or rather its lack of knowledge of a small, new entrant to that software market) was negotiated between Gartner, the supplier and the adopter; and
2	in Chapter 7 we saw the complex alignments as the organisational user and its ERP supplier found common purpose in improving Gartner’s assessment of their solution as a means of ensuring that the supplier would increase its commitment to that sector.
Our analysis thus highlights the complex set of interests surrounding and conditioning these assessments encompassing intra-organisational politics (how internal organisational politics shaped the preferences of different groups within the adopter for particular supplier offerings) and these inter-organisational alignments. Political process and organisational culture are clearly present. In this respect, our study focuses on the other end of the spectrum compared to Callon’s (1998, 1999, 2007) and MacKenzie’s (2003, 2006a, 2006b) work on performativity and how framing processes serve to allow the operation of financial markets. Rather than try to reduce these features to either a simple technical or political process, we instead suggest that it is useful to be able to distinguish between different contexts of decision-making. For example, in some cases decisions are subject to more formalised and routinised assessment criteria than in others. Our concern here, linked to our multi-level analysis, is to be able to examine differences between particular locales of innovation and compare between one moment and another (as a technology field, for example, becomes more mature over time).
Commodification of networked reputation: the role of industry analysts
Comparing the implementation of ERP today with CAPM in the 1990s, we are struck by the increasingly sophisticated sources of specialised information available about this and other workplace technologies. The enterprise system (MRP, MRP II, ERP et seq.) arena is becoming ever more organised. This arena is comprised of suppliers, users, consultants and inter-organisational networks, which include sectoral networks, professional associations and latterly user-groups organised by vendors as well as industry analysts. In addition, it is the latter that constitutes a novel and increasingly influential category of player.
In earlier work we had highlighted the difficulties of potential adopters in assessing suppliers’ claims about the operation of enormously complex products like ERP packages as well as their potential fit to their organisation and their need, for example, to rely on proxy information sources such as measures of reputation (Brady et al. 1992). Our study draws attention to the commodified circulation of community knowledge by a novel kind of intermediary - industry analysts. Firms like Gartner are able to trade this knowledge extremely profitably (charging user organisations for access to their assessments based upon submissions by vendors and on freely provided experiences from their user community). Their assessments may be seen as constituting a new kind of privately provided public good, not subject to the strict controls of independent ‘scientific’ knowledge but having its own particular forms of accountability.
The constitution and evolution of technological fields
As we have seen, the constitution of technical fields helps suppliers, promoters and above all adopters manage the uncertainties surrounding comparison of performance within these heterogeneous domains. Our study draws attention to how industry analysts seek to establish boundaries around the field and generate assessments of the relative location of various suppliers (their current products and future prospects) within the product market for different user sectors. In addition, we noted the role of Gartner and other analysts in drawing up signposts about the state of the industry and its future development. However, it should be noted that the development of technological fields is not a space ‘owned’ by any particular group of practitioners, vendors, users or analysts.
Our analysis throws light on how ERP and its predecessors were shaped through a process of cumulative development and largely incremental evolution over four decades. This development was also marked by discontinuities - often occasioned by a change in name. These periodic name changes, once the field had acquired an identity (e.g. as MRP), seemed to be associated with a coincidence of a number of developments. These were:
modifications to technical functionality; changes in commercial visions, and associated concepts of best practice and how these may be achieved by implementing technologies; and, perhaps most importantly, shifts in underlying information architectures. We further saw how the progressively expanding scope of MRP/ERP brings it into contact with other competing constituencies. For example, today ERP suppliers find themselves in competition with vendors of Customer Relationship Management systems and of supply-chain integration solutions.
These observations bring us back to the more general issue for the analysis of technology and work raised in Chapter 2, regarding the need to pay attention to discourses of technology supply and commentary as a site where technology futures are worked out and promises articulated and validated. It is, we may add, an arena in which vendors, users, analysts, etc., are themselves developing and advancing their own accounts of the biographies of technology (which adds another level of complexity to all this). We were able in this study to explore the interaction between vendors, users and consultants who constitute the arena around a particular technological field.
How SAP conquered the world?
To return to the issue with which we began the book: today there is a widespread view that certain suppliers have conquered the world with their enterprise systems (Lozinsky 1998; Kumar and van Hillegersberg 2000; Jakovljevic 2001; Mabert et al. 2001). In our view, although not necessarily sharing the notion of technological change promoted by this particular language, we do think these systems provide one of the most important episodes and challenges to the Social Study of Information Systems. That is, how do we explain the widespread resort to package solutions, the rise of this new breed of supplier and the extension of the generic enterprise system? It is important to do so in a comprehensive manner - without either inflating or overlooking this achievement. It is this we have attempted to do in this book. In so doing, our analysis has called into question the simplistic account of the ‘victory’ of packaged supply. This story of success is a retrospective view. Take, for example, our research on the global giant SoftCo: if we go back to examine their early strategies, while the company had publicly articulated global claims its detailed practices convey a rather different picture. Its own materials emphasised the key sectors that it was targeting before moving on to others. This suggests that if a supplier like SoftCo has succeeded in conquering the world, it did so one sector at a time, carefully, in a process characterised by setbacks and ‘reversals’.
In this latter respect, for instance, we saw more recently how SoftCo decided to move into one particular new sector, an industry they thought to be as yet unexploited but potentially lucrative. We saw how, when the anticipated returns (the large take-up) did not immediately appear, they were cautious about sinking further resources into the sector. Entering a new
market requires suppliers to gain new knowledge and competences, which vendors such as SoftCo appeared to be highly competent in doing, as well as balancing certain competing requirements.4 It faced crucial decisions about which requirements to include within its software package, and extension of functionality had to be balanced against its established goal of maintaining and recycling a generic solution. In other words, ERP vendors have to decide just how much effort to invest in meeting the needs of particular market segments (and clearly, some target markets and adopters attract more attention than do others).
This caution did not go unnoticed. The supplier was criticised by industry analysts for what they saw as their lack of continued investment in this particular sector. Interestingly, while Gartner rate SoftCo highly in other industries they do not in this particular one. To repeat the words of one senior Gartner analyst, SoftCo ‘have the resources to be a global leader in [the particular sector] if they want to be, it is just that they have just never made the commitment’. What does Gartner’s intervention demonstrate? In our view, it relates to this wider issue concerning the nature of the modern enterprise solution and the extent to which it should be based on sectorally specific or generic components. On the one hand, SoftCo want to recycle their solutions in the most efficient way possible, only adapting them when absolutely necessary (as and when they meet sustained resistance). Gartner, on the other hand, chastise them for not taking into account the specificity of the market in which they are operating. Thus, this episode demonstrates a struggle, if you like, between competing visions of the nature of the field.
This struggle can be seen at other levels also. Our studies have shown that global vendors carefully manage their relationship with their customer base. They do not attempt to confront the diversity they find in new sectors head on. Rather, they segment the market in order to have a managed and manageable response to the range of needs in a potential user base. They build their offerings around carefully selected ‘pilot sites’ and use these to orient their offering to a heterogeneous potential market. However, and rather intriguingly, having captured the pilots is no guarantee of capturing the market to which these organisations belong. In the case of SoftCo, for instance, once their new ERP module had been designed there was the problem of whether it could be ‘disentangled’ (Thomas 1991) from its history (i.e. the sites on which it was based). There was a genuine fear among supplier managers that the module would become over-identified with those places and that, if this happened, it was unlikely to extend (at least easily) beyond them. Much effort, therefore, was expended by the vendor in actively managing the biography of this system (the history of relationships and sites implicated in its evolution). Could they convince others that the system that was currently working in the pilot sites would also work in their setting? One SoftCo executive, for instance, described his relief when they finally managed to sell the finished system beyond the pilot sites: ‘The project at [West Uni] is important to [SoftCo] as it is the first non-pilot to implement [Campus].’
However, paradoxically, if software packages are to be exported elsewhere then they also can never fully be ‘disentangled’ from the sites upon which they were built. They are continuously brought back to these earlier sites. In this sense, we see ‘entanglement’ as allied to but not the same as our notion of generification. While generification describes the process of making things the same, entanglement points to the ties that a piece of software has to its birthplace and to earlier communities. This continued entanglement occurs for a number of reasons, but most often when prospective adopters are in search of a ‘reference site’ - for instance, where a potential adopter can see with their ‘own eyes’ just how the system works (Chapter 6). The upshot is that global software packages are forever connected to - we might say reliant upon - the local sites upon which they are built. In other words, there remains a very close association between the package and its place(s) of birth.
Our studies thus call into question widely promulgated portrayals of the activities of package suppliers, which have achieved surprising currency even among Computer Science, Information Systems and critical social science accounts. These are typically ‘views from outside’, which tend to stress the ‘power of the vendor’ and its ‘non-responsiveness’ to customers. These do not correspond to the ‘view from inside’ that we have uncovered. In our research, we saw an extremely complex process of enrolment and engagement between supplier and users. We thus found complex sets of intra-organisational and inter-organisational alliances. Internal competition within the package vendor for development resources becomes bound up with external alliances with its users and user groups. We noted at least three key moments.
First, in terms of their decisions to move into (or reverse out of) new sectors: this is often the result of an internal process where various solution managers attempt to gather support and put together ‘business cases’ for entry into a territory or the production of new software. We witnessed cases where, having done initial research and sounded out ideas with key customers at user-group meetings, SoftCo staff subsequently decided not to pursue particular innovations. Second, we saw how the user organisations connected to one large supplier welcomed an improving assessment by Gartner of this partner vendor’s position in that product market, as it would bring opportunities for them to extract greater investment by the supplier in their current product. This heralded a process whereby the users sought to improve SoftCo’s showing in Gartner assessments in order to attract a bigger investment by the software supplier. The improving position on the Gartner Magic Quadrant, for instance, meant that internally the Solution Manager could present evidence to her board that investments in the sector were beginning to pay off. Third - and one of the unexpected findings from our research, perhaps - was the commitment of certain users in helping the vendor’s systems achieve success. At times, the effort of certain key users and pilots to look after and ensure the extension of these systems was extraordinary. This could involve attendance at user-group meetings, the actual organisation of
user-groups, feeding back ideas to the vendor, and also actively promoting the vendor’s products to other organisations in their sector. As well as acting as a reference site (for which they were paid only a small nominal fee), this also included instances of users actively marketing the systems to others.5 They undertook this effort and did this work because they recognised that the success of the module was also in their interests. None of the users, as numerous respondents remarked to us, wanted to end up with a product that was ‘old’ and ‘very specific’ to the pilots.
Overall, this book highlights the complex forms of bridging being constructed between vendor and user organisations. The state of affairs our studies have revealed is inordinately more complex than the existing literature within STS or Information Systems research suggests. This is, for example, in terms of the strategy and practices of package developers and of user organisations. Through this, we have drawn attention to the fine structure through which technology markets emerge and evolve, highlighting the growing role therein of industry analysts.
Extending our approach
With the Biography of Artefacts Framework we have developed the analytical tools to study the emergence and extension of a technology over time and within a broader setting. In this book, we have articulated the framework in an effort to characterise the particular dynamics of enterprise systems. However, we see no reason why this approach should not be used to study other artefacts or systems. It has immediate pertinence to analysing the emergence and evolution of ICT applications more generally (and we would see a number of other studies, notably Hyysalo [2006], as exemplifying this kind of analysis). Our observations about entanglement and extension would be particularly relevant to organisational technologies that are closely tied up with organisational practices and cultures. However, the framework could be readily applied elsewhere in the ICT domain, for instance to analyse different kinds of software such as ‘open-source’ systems where there are interesting tensions between, on the one hand, the community nature of this kind of software (i.e. the fact it is produced and freely distributed by individuals) and, on the other, the increasing and wider demand for its increased reliability and ease of use (its ‘commodification’) as it diffuses across organisations.
The Biography of Artefacts Framework could also be applied to very different kinds of technologies outside the ICT domain. There are, of course, other classes of technologies which are seemingly highly rooted to their place of production and thus, in theory, thought unable to move beyond them, but which despite this have achieved some form of mobility. In the construction industry, for instance, there is the example of houses or bridges that have always been custom-built at the places where they will be consumed, but with prefabrication this is no longer simply the case.6
To conclude, it can be seen that today the idea of the ‘mutual shaping’ (or co-construction) of technology and society has become widely adopted within STS analysis. We enthusiastically welcome this terminology as it signals clearly both the social shaping of technology and the materiality of technology. It focuses attention upon how technologies and their social outcomes emerge in a process in which ‘technical’ and ‘social’ elements are closely intertwined. Analyses of mutual shaping typically share our concerns to address the context and history of innovations. However, the metaphor of mutual shaping gives rather few cues as to the actual mechanisms of mutual shaping, or the spaces and occasions in which it occurs. The Biography of Artefacts Framework seeks to redress this gap. We hope this book will contribute some analytical concepts and a framework for addressing the multiple locales and timeframes in which mutual shaping occurs. In addition, we would like to open up discussion about research methodologies. We see this work as providing an instance of a complex methodology that seeks to exploit the respective strengths of contemporary ethnographic studies and longer-term and broader analysis. Socio-technical systems are becoming more extensive and more convoluted. STS and IS research needs to develop more sophisticated analytical schema that match the complexity and heterogeneity of the socio-technical phenomena it addresses.